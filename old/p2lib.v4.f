c 2d parallel PIC library for MPI communicationsc written by viktor k. decyk, uclac copyright 1995, regents of the university of californiac update: may 31, 2004c-----------------------------------------------------------------------      subroutine DCOMP2(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c this subroutine determines spatial boundaries for particlec decomposition, calculates number of grid points in each spatialc region, and the offset of these grid points from the global addressc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.c ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idps = number of partition boundariesc nblok = number of particle partitions.      implicit none      real edges      integer nyp, noff, ny, kstrt, nvp, idps, nblok      dimension edges(idps,nblok)      dimension nyp(nblok), noff(nblok)c local data      integer ks, kb, kr, l      real at1      ks = kstrt - 2      at1 = float(ny)/float(nvp)      do 10 l = 1, nblok      kb = l + ks      edges(1,l) = at1*float(kb)      noff(l) = edges(1,l) + .5      edges(2,l) = at1*float(kb + 1)      kr = edges(2,l) + .5      nyp(l) = kr - noff(l)   10 continue      return      endc-----------------------------------------------------------------------      subroutine DCOMP2L(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c this subroutine determines spatial boundaries for particlec decomposition, calculates number of grid points in each spatialc region, and the offset of these grid points from the global addressc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.c ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idps = number of partition boundariesc nblok = number of particle partitions.      dimension edges(idps,nblok)      dimension nyp(nblok), noff(nblok)      ks = kstrt - 2      at1 = float(ny)/float(nvp)      do 10 l = 1, nblok      kb = l + ks      edges(1,l) = at1*float(kb)      noff(l) = edges(1,l)      edges(2,l) = at1*float(kb + 1)      kr = edges(2,l)      nyp(l) = kr - noff(l)   10 continue      return      endc-----------------------------------------------------------------------      subroutine PCGUARD2(f,kstrt,nvp,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 30 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computersc     do 10 j = 1, nxvc     f(j,1,l) = f(j,kyp+1,kl)c     f(j,kyp+2,l) = f(j,2,kr)c     f(j,kyp+3,l) = f(j,ngc+1,krr)c  10 continuec this segment is used for mpi computers      call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+3,lgrp,msid,ierr)      call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+3,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+4,lgrp,msid,ie     1rr)      call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+6,lgrp,msi     1d,ierr)         call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kll-1,moff+6,lgrp,ierr)         call MPI_WAIT(msid,istatus,ierr)      endif   30 continue      return      endc-----------------------------------------------------------------------      subroutine PCGUARD2L(f,kstrt,nvp,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes one extra guard cell.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c linear interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer ks, moff, kl, kr, lc     integer j      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 20 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) then         kr = kr - nvp      endif      kl = l + ks      if (kl.lt.1) then         kl = kl + nvp      endifc this loop is used for shared memory computersc     do 10 j = 1, nxvc     f(j,kyp+1,l) = f(j,1,kr)c  10 continuec this segment is used for mpi computers      call MPI_IRECV(f(1,kyp+1,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)      call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)   20 continue      return      endc-----------------------------------------------------------------------      subroutine PAGUARD2(f,scr,kstrt,nvp,nxv,nypmx,kyp,kblok,ngds)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c scr(j,ngds,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nxv, nypmx, kyp, kblok, ngds      dimension f(nxv,nypmx,kblok), scr(nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)c special case for one processor      if (nvp.eq.1) then         do 20 l = 1, kblok         do 10 j = 1, nxv         f(j,2,l) = f(j,2,l) + f(j,kyp+2,l)         f(j,3,l) = f(j,3,l) + f(j,kyp+3,l)         f(j,kyp+1,l) = f(j,kyp+1,l) + f(j,1,l)   10    continue   20    continue         return      endif      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 50 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computersc     do 30 j = 1, nxvc     scr(j,1,l) = f(j,kyp+2,kl)c     scr(j,2,l) = f(j,kyp+3,kll)c     scr(j,3,l) = f(j,1,kr)c  30 continuec this segment is used for mpi computers      call MPI_IRECV(scr,ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+1,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scr(1,3,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)      call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(scr(1,2,l),ngc*nxv,mreal,kll-1,moff+5,lgrp,msid,     1ierr)         call MPI_SEND(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+5,lgrp,ierr     1)         call MPI_WAIT(msid,istatus,ierr)      endifc add up the guard cells      do 40 j = 1, nxv      f(j,2,l) = f(j,2,l) + scr(j,1,l)      f(j,ngc+1,l) = f(j,ngc+1,l) + scr(j,2,l)      f(j,kyp+1,l) = f(j,kyp+1,l) + scr(j,3,l)   40 continue   50 continue      return      endc-----------------------------------------------------------------------      subroutine PAGUARD2L(f,scr,kstrt,nvp,nxv,nypmx,kyp,kblok)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes one extra guard cell.c scr(j,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c linear interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok), scr(nxv,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer ks, moff, kl, kr, j, l      dimension istatus(lstat)c special case for one processor      if (nvp.eq.1) then         do 20 l = 1, kblok         do 10 j = 1, nxv         f(j,1,l) = f(j,1,l) + f(j,kyp+1,l)   10    continue   20    continue         return      endif      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 50 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) then         kr = kr - nvp      endif      kl = l + ks      if (kl.lt.1) then         kl = kl + nvp      endifc this segment is used for shared memory computersc     do 30 j = 1, nxvc     scr(j,l) = f(j,kyp+1,kl)c  30 continuec this segment is used for mpi computers      call MPI_IRECV(scr,nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+1,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)c add up the guard cells      do 40 j = 1, nxv      f(j,1,l) = f(j,1,l) + scr(j,l)   40 continue   50 continue      return      endc-----------------------------------------------------------------------      subroutine PDBLSIN2C(cu,cu2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2     1blok)c this subroutine creates a doubled vector array cu2 from a vector arrayc cu, so that various 2d sine/cosine transforms can be performed with ac 2d real to complex fft.  the x component is an odd function in y,c and y component is an odd function in x.c Asummes vector cu vanishes at end pointsc linear interpolation for distributed datac cu2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = second dimension of input array cu, must be >= nxc kyp = number of data values per block in yc kypd = third dimension of input array cu, must be >= kypc kyp2 = third dimension of output array cu2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real cu, cu2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension cu(2,nxv,kypd,kblok), cu2(2,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lsid, msid, nsid, ierr      integer i, j, k, l, nxs, nys, ny2, kyb, kyb2, ks, koff, moff      integer kk, ll, lm, k1, k2, joff      dimension istatus(lstat)      nxs = nx - 1      nys = ny - 1      kyb = ny/kyp      ny2 = ny + ny      kyb2 = ny2/kyp2      ks = kstrt - 2      moff = kypd + kybc copy to double array in x direction      do 80 l = 1, k2blok      koff = kyp2*(l + ks)      ll = koff/kyp + 1      koff = kyp*(l + ks)      lm = koff/kyp2 + 1c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, nys         do 10 j = 1, nxs         cu2(1,j+1,k+1,l) = cu(1,j+1,k+1,l)         cu2(2,j+1,k+1,l) = cu(2,j+1,k+1,l)         cu2(1,nx+j+1,k+1,l) = cu(1,nx-j+1,k+1,l)         cu2(2,nx+j+1,k+1,l) = -cu(2,nx-j+1,k+1,l)         cu2(1,j+1,ny+k+1,l) = -cu(1,j+1,ny-k+1,l)         cu2(2,j+1,ny+k+1,l) = cu(2,j+1,ny-k+1,l)         cu2(1,nx+j+1,ny+k+1,l) = -cu(1,nx-j+1,ny-k+1,l)         cu2(2,nx+j+1,ny+k+1,l) = -cu(2,nx-j+1,ny-k+1,l)   10    continue         cu2(1,1,k+1,l) = 0.         cu2(2,1,k+1,l) = 0.         cu2(1,nx+1,k+1,l) = 0.         cu2(2,nx+1,k+1,l) = 0.         cu2(1,1,k+ny+1,l) = 0.         cu2(2,1,k+ny+1,l) = 0.         cu2(1,nx+1,k+ny+1,l) = 0.         cu2(2,nx+1,k+ny+1,l) = 0.   20    continue         do 30 j = 1, nx         cu2(1,j,1,l) = 0.         cu2(2,j,1,l) = 0.         cu2(1,j+nx,1,l) = 0.         cu2(2,j+nx,1,l) = 0.         cu2(1,j,ny+1,l) = 0.         cu2(2,j,ny+1,l) = 0.         cu2(1,j+nx,ny+1,l) = 0.         cu2(2,j+nx,ny+1,l) = 0.   30    continue         return      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        do 50 k = 1, kypc        do 40 j = 1, nxc        do 35 i = 1, 2c        cu2(i,j,k,l) = cu(i,j,k,ll)c  35    continuec  40    continuec  50    continuec        if (kyp.lt.kyp2) thenc           do 70 k = 1, kypc           do 60 j = 1, nxc           do 55 i = 1, 2c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll+1)c  55       continuec  60       continuec  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         call MPI_IRECV(cu2(1,1,1,l),2*kyp*nxv,mreal,ll-1,moff+1,lgrp,ms     1id,ierr)         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),2*kyp*nxv,mreal,ll,moff+1,lg     1rp,nsid,ierr)         endif      endif      if (lm.le.(kyb2/2)) then         call MPI_SEND(cu(1,1,1,l),2*kyp*nxv,mreal,lm-1,moff+1,lgrp,ierr     1)      endifc wait for data and unpack it      if (ll.le.kyb) then         call MPI_WAIT(msid,istatus,ierr)         do 50 k = 2, kyp         k1 = kyp - k + 2         k2 = (k1 - 1)/2 + 1         joff = nxv*(k1 - 2*k2 + 1)         do 40 j = 1, nxv         do 35 i = 1, 2         cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)   35    continue   40    continue   50    continue         if (kyp.lt.kyp2) then            call MPI_WAIT(nsid,istatus,ierr)            do 70 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 60 j = 1, nxv            do 55 i = 1, 2            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   55       continue   60       continue   70       continue         endif      endif   80 continuec copy to double array in y direction      do 140 l = 1, k2blok      koff = kyp2*(l + ks)      ll = (ny2 - koff - 1)/kyp + 1      koff = kyp*(l + ks)      lm = (ny2 - koff - 1)/kyp2 + 1      koff = koff + kyp2*lm - ny2c this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((ll+1).le.kyb) thenc           do 90 j = 1, nxc           do 85 i = 1, 2c           cu2(i,j,1,l) = cu(i,j,1,ll+1)c  85       continuec  90       continuec        endifc        if (kyp.lt.kyp2) thenc           do 110 k = 1, kypc           do 100 j = 1, nxc           do 95 i = 1, 2c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll)c  95       continuec 100       continuec 110       continuec        endifc        if (kyp.gt.1) thenc           do 130 k = 2, kypc           do 120 j = 1, nxc           do 115 i = 1, 2c           cu2(i,j,k,l) = cu(i,j,k,ll-1)c 115       continuec 120       continuec 130       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_IRECV(cu2(1,1,1,l),2*nxv,mreal,ll,moff+2,lgrp,lsid,     1ierr)         endif         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),2*kyp*nxv,mreal,ll-1,moff+2,     1lgrp,msid,ierr)         endif         if (kyp.gt.1) then            call MPI_IRECV(cu2(1,1,2,l),2*(kyp-1)*nxv,mreal,ll-2,moff+2,     1lgrp,nsid,ierr)         endif      endif      if ((lm.gt.(kyb2/2)).and.(lm.le.kyb2)) then         if (koff.eq.0) then            if ((lm+1).le.kyb2) then               call MPI_SEND(cu(1,1,1,l),2*nxv,mreal,lm,moff+2,lgrp,ierr     1)            endif            if (kyp.gt.1) then               call MPI_SEND(cu(1,1,2,l),2*(kyp-1)*nxv,mreal,lm-1,moff+2     1,lgrp,ierr)            endif         else            call MPI_SEND(cu(1,1,1,l),2*kyp*nxv,mreal,lm-1,moff+2,lgrp,i     1err)         endif      endifc wait for data and unpack it      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_WAIT(lsid,istatus,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_WAIT(msid,istatus,ierr)            do 100 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 90 j = 1, nxv            do 85 i = 1, 2            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   85       continue   90       continue  100       continue         endif         if (kyp.gt.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 120 k = 3, kyp            k1 = kyp - k + 3            k2 = k1/2 + 1            joff = nxv*(k1 - 2*k2 + 2)            do 110 j = 1, nxv            do 105 i = 1, 2            cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)  105       continue  110       continue  120       continue         endif      endif  140 continuec create odd array      do 200 l = 1, k2blok      koff = kyp2*(l + ks)      do 190 k = 1, kyp2      kk = k + koff      if ((kk.eq.1).or.(kk.eq.(ny+1))) then         do 150 j = 1, nx         do 145 i = 1, 2         cu2(i,j,k,l) = 0.         cu2(i,j+nx,k,l) = 0.  145    continue  150    continue      else if (kk.le.ny) then         do 160 j = 1, nxs         cu2(1,nx+j+1,k,l) = cu2(1,nx-j+1,k,l)         cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)  160    continue         do 165 i = 1, 2         cu2(i,1,k,l) = 0.         cu2(i,nx+1,k,l) = 0.  165    continue      else if (kk.gt.(ny+1)) then         if (k.eq.1) then            do 170 j = 1, nxs            cu2(1,nx+j+1,k,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)  170       continue         else            do 180 j = 1, nxs            cu2(1,nx+j+1,kyp2-k+2,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,kyp2-k+2,l) = -cu2(2,nx-j+1,k,l)  180       continue         endif         do 185 i = 1, 2         cu2(i,1,k,l) = 0.         cu2(i,nx+1,k,l) = 0.  185    continue      endif  190 continue  200 continuec finish odd array      do 230 l = 1, k2blok      koff = kyp2*(l + ks)      do 220 k = 1, kyp2      kk = k + koff      if (kk.gt.(ny+1)) then         do 210 j = 1, nxs         cu2(1,nx-j+1,k,l) = cu2(1,nx+j+1,k,l)         cu2(2,nx-j+1,k,l) = -cu2(2,nx+j+1,k,l)  210    continue         cu2(1,nx+1,k,l) = cu2(1,nx+1,k,l)         cu2(2,nx+1,k,l) = -cu2(2,nx+1,k,l)      endif  220 continue  230 continue      return      endc-----------------------------------------------------------------------      subroutine PDBLSIN2D(q,q2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2bl     1ok)c this subroutine creates an odd array q2 from an array q, so thatc a 2d sine transform can be performed with a 2d real to complex fft.c linear interpolation for distributed datac q2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = first dimension of input array q, must be >= nxc kyp = number of data values per block in yc kypd = second dimension of input array q, must be >= kypc kyp2 = second dimension of output array q2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real q, q2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension q(nxv,kypd,kblok), q2(2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lsid, msid, nsid, ierr      integer j, k, l, nxs, nys, ny2, kyb, kyb2, ks, koff, moff      integer kk, ll, lm, k1, k2, joff      dimension istatus(lstat)      nxs = nx - 1      nys = ny - 1      kyb = ny/kyp      ny2 = ny + ny      kyb2 = ny2/kyp2      ks = kstrt - 2      moff = kypd + kybc copy to double array in x direction      do 80 l = 1, k2blok      koff = kyp2*(l + ks)      ll = koff/kyp + 1      koff = kyp*(l + ks)      lm = koff/kyp2 + 1c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, nys         do 10 j = 1, nxs         q2(j+1,k+1,l) = q(j+1,k+1,l)         q2(nx+j+1,k+1,l) = -q(nx-j+1,k+1,l)         q2(j+1,ny+k+1,l) = -q(j+1,ny-k+1,l)         q2(nx+j+1,ny+k+1,l) = q(nx-j+1,ny-k+1,l)   10    continue         q2(1,k+1,l) = 0.         q2(nx+1,k+1,l) = 0.         q2(1,k+ny+1,l) = 0.         q2(nx+1,k+ny+1,l) = 0.   20    continue         do 30 j = 1, nx         q2(j,1,l) = 0.         q2(j+nx,1,l) = 0.         q2(j,ny+1,l) = 0.         q2(j+nx,ny+1,l) = 0.   30    continue         return      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        do 50 k = 1, kypc        do 40 j = 1, nxc        q2(j,k,l) = q(j,k,ll)c  40    continuec  50    continuec        if (kyp.lt.kyp2) thenc           do 70 k = 1, kypc           do 60 j = 1, nxc           q2(j,k+kyp,l) = q(j,k,ll+1)c  60       continuec  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         call MPI_IRECV(q2(1,1,l),kyp*nxv,mreal,ll-1,moff+1,lgrp,msid,ie     1rr)         if (kyp.lt.kyp2) then            call MPI_IRECV(q2(1,kyp+1,l),kyp*nxv,mreal,ll,moff+1,lgrp,ns     1id,ierr)         endif      endif      if (lm.le.(kyb2/2)) then         call MPI_SEND(q(1,1,l),kyp*nxv,mreal,lm-1,moff+1,lgrp,ierr)      endifc wait for data and unpack it      if (ll.le.kyb) then         call MPI_WAIT(msid,istatus,ierr)         do 50 k = 2, kyp         k1 = kyp - k + 2         k2 = (k1 - 1)/2 + 1         joff = nxv*(k1 - 2*k2 + 1)         do 40 j = 1, nxv         q2(j,k1,l) = q2(j+joff,k2,l)   40    continue   50    continue         if (kyp.lt.kyp2) then            call MPI_WAIT(nsid,istatus,ierr)            do 70 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 60 j = 1, nxv            q2(j,k1+kyp,l) = q2(j+joff,k2+kyp,l)   60       continue   70       continue         endif      endif   80 continuec copy to double array in y direction      do 140 l = 1, k2blok      koff = kyp2*(l + ks)      ll = (ny2 - koff - 1)/kyp + 1      koff = kyp*(l + ks)      lm = (ny2 - koff - 1)/kyp2 + 1      koff = koff + kyp2*lm - ny2c this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((ll+1).le.kyb) thenc           do 90 j = 1, nxc           q2(j,1,l) = q(j,1,ll+1)c  90       continuec        endifc        if (kyp.lt.kyp2) thenc           do 110 k = 1, kypc           do 100 j = 1, nxc           q2(j,k+kyp,l) = q(j,k,ll)c 100       continuec 110       continuec        endifc        if (kyp.gt.1) thenc           do 130 k = 2, kypc           do 120 j = 1, nxc           q2(j,k,l) = q(j,k,ll-1)c 120       continuec 130       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_IRECV(q2(1,1,l),nxv,mreal,ll,moff+2,lgrp,lsid,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_IRECV(q2(1,kyp+1,l),kyp*nxv,mreal,ll-1,moff+2,lgrp,     1msid,ierr)         endif         if (kyp.gt.1) then            call MPI_IRECV(q2(1,2,l),(kyp-1)*nxv,mreal,ll-2,moff+2,lgrp,     1nsid,ierr)         endif      endif      if ((lm.gt.(kyb2/2)).and.(lm.le.kyb2)) then         if (koff.eq.0) then            if ((lm+1).le.kyb2) then               call MPI_SEND(q(1,1,l),nxv,mreal,lm,moff+2,lgrp,ierr)            endif            if (kyp.gt.1) then               call MPI_SEND(q(1,2,l),(kyp-1)*nxv,mreal,lm-1,moff+2,lgrp     1,ierr)            endif         else            call MPI_SEND(q(1,1,l),kyp*nxv,mreal,lm-1,moff+2,lgrp,ierr)         endif      endifc wait for data and unpack it      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_WAIT(lsid,istatus,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_WAIT(msid,istatus,ierr)            do 100 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 90 j = 1, nxv            q2(j,k1+kyp,l) = q2(j+joff,k2+kyp,l)   90       continue  100       continue         endif         if (kyp.gt.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 120 k = 3, kyp            k1 = kyp - k + 3            k2 = k1/2 + 1            joff = nxv*(k1 - 2*k2 + 2)            do 110 j = 1, nxv            q2(j,k1,l) = q2(j+joff,k2,l)  110       continue  120       continue         endif      endif  140 continuec create odd array      do 200 l = 1, k2blok      koff = kyp2*(l + ks)      do 190 k = 1, kyp2      kk = k + koff      if ((kk.eq.1).or.(kk.eq.(ny+1))) then         do 150 j = 1, nx         q2(j,k,l) = 0.         q2(j+nx,k,l) = 0.  150    continue      else if (kk.le.ny) then         do 160 j = 1, nxs         q2(nx+j+1,k,l) = -q2(nx-j+1,k,l)  160    continue         q2(1,k,l) = 0.         q2(nx+1,k,l) = 0.      else if (kk.gt.(ny+1)) then         if (k.eq.1) then            do 170 j = 1, nxs            q2(nx+j+1,k,l) = q2(nx-j+1,k,l)  170       continue         else            do 180 j = 1, nxs            q2(nx+j+1,kyp2-k+2,l) = q2(nx-j+1,k,l)  180       continue         endif         q2(1,k,l) = 0.         q2(nx+1,k,l) = 0.      endif  190 continue  200 continuec finish odd array      do 230 l = 1, k2blok      koff = kyp2*(l + ks)      do 220 k = 1, kyp2      kk = k + koff      if (kk.gt.(ny+1)) then         do 210 j = 1, nxs         q2(nx-j+1,k,l) = -q2(nx+j+1,k,l)  210    continue         q2(nx+1,k,l) = -q2(nx+1,k,l)      endif  220 continue  230 continue      return      endc-----------------------------------------------------------------------      subroutine PDBLSIN2B(cu,cu2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2     1blok)c this subroutine creates a doubled vector array cu2 from a vector arrayc cu, so that various 2d sine/cosine transforms can be performed with ac 2d real to complex fft.  the x component is an odd function in y,c y component is an odd function in x, and the z component is an oddc function in both x and y.  Asummes vector cu vanishes at end pointsc linear interpolation for distributed datac cu2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = first dimension of input array q, must be >= nxc kyp = number of data values per block in yc kypd = second dimension of input array q, must be >= kypc kyp2 = second dimension of output array q2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real cu, cu2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension cu(3,nxv,kypd,kblok), cu2(3,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lsid, msid, nsid, ierr      integer i, j, k, l, nxs, nys, ny2, kyb, kyb2, ks, koff, moff      integer kk, ll, lm, k1, k2, joff      dimension istatus(lstat)      nxs = nx - 1      nys = ny - 1      kyb = ny/kyp      ny2 = ny + ny      kyb2 = ny2/kyp2      ks = kstrt - 2      moff = kypd + kybc copy to double array in x direction      do 80 l = 1, k2blok      koff = kyp2*(l + ks)      ll = koff/kyp + 1      koff = kyp*(l + ks)      lm = koff/kyp2 + 1c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, nys         do 10 j = 1, nxs         cu2(1,j+1,k+1,l) = cu(1,j+1,k+1,l)         cu2(2,j+1,k+1,l) = cu(2,j+1,k+1,l)         cu2(3,j+1,k+1,l) = cu(3,j+1,k+1,l)         cu2(1,nx+j+1,k+1,l) = cu(1,nx-j+1,k+1,l)         cu2(2,nx+j+1,k+1,l) = -cu(2,nx-j+1,k+1,l)         cu2(3,nx+j+1,k+1,l) = -cu(3,nx-j+1,k+1,l)         cu2(1,j+1,ny+k+1,l) = -cu(1,j+1,ny-k+1,l)         cu2(2,j+1,ny+k+1,l) = cu(2,j+1,ny-k+1,l)         cu2(3,j+1,ny+k+1,l) = -cu(3,j+1,ny-k+1,l)         cu2(1,nx+j+1,ny+k+1,l) = -cu(1,nx-j+1,ny-k+1,l)         cu2(2,nx+j+1,ny+k+1,l) = -cu(2,nx-j+1,ny-k+1,l)         cu2(3,nx+j+1,ny+k+1,l) = cu(3,nx-j+1,ny-k+1,l)   10    continue         cu2(1,1,k+1,l) = 0.         cu2(2,1,k+1,l) = 0.         cu2(3,1,k+1,l) = 0.         cu2(1,nx+1,k+1,l) = 0.         cu2(2,nx+1,k+1,l) = 0.         cu2(3,nx+1,k+1,l) = 0.         cu2(1,1,k+ny+1,l) = 0.         cu2(2,1,k+ny+1,l) = 0.         cu2(3,1,k+ny+1,l) = 0.         cu2(1,nx+1,k+ny+1,l) = 0.         cu2(2,nx+1,k+ny+1,l) = 0.         cu2(3,nx+1,k+ny+1,l) = 0.   20    continue         do 30 j = 1, nx         cu2(1,j,1,l) = 0.         cu2(2,j,1,l) = 0.         cu2(3,j,1,l) = 0.         cu2(1,j+nx,1,l) = 0.         cu2(2,j+nx,1,l) = 0.         cu2(3,j+nx,1,l) = 0.         cu2(1,j,ny+1,l) = 0.         cu2(2,j,ny+1,l) = 0.         cu2(3,j,ny+1,l) = 0.         cu2(1,j+nx,ny+1,l) = 0.         cu2(2,j+nx,ny+1,l) = 0.         cu2(3,j+nx,ny+1,l) = 0.   30    continue         return      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        do 50 k = 1, kypc        do 40 j = 1, nxc        do 35 i = 1, 3c        cu2(i,j,k,l) = cu(i,j,k,ll)c  35    continuec  40    continuec  50    continuec        if (kyp.lt.kyp2) thenc           do 70 k = 1, kypc           do 60 j = 1, nxc           do 55 i = 1, 3c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll+1)c  55       continuec  60       continuec  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         call MPI_IRECV(cu2(1,1,1,l),3*kyp*nxv,mreal,ll-1,moff+1,lgrp,ms     1id,ierr)         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),3*kyp*nxv,mreal,ll,moff+1,lg     1rp,nsid,ierr)         endif      endif      if (lm.le.(kyb2/2)) then         call MPI_SEND(cu(1,1,1,l),3*kyp*nxv,mreal,lm-1,moff+1,lgrp,ierr     1)      endifc wait for data and unpack it      if (ll.le.kyb) then         call MPI_WAIT(msid,istatus,ierr)         do 50 k = 2, kyp         k1 = kyp - k + 2         k2 = (k1 - 1)/2 + 1         joff = nxv*(k1 - 2*k2 + 1)         do 40 j = 1, nxv         do 35 i = 1, 3         cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)   35    continue   40    continue   50    continue         if (kyp.lt.kyp2) then            call MPI_WAIT(nsid,istatus,ierr)            do 70 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 60 j = 1, nxv            do 55 i = 1, 3            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   55       continue   60       continue   70       continue         endif      endif   80 continuec copy to double array in y direction      do 140 l = 1, k2blok      koff = kyp2*(l + ks)      ll = (ny2 - koff - 1)/kyp + 1      koff = kyp*(l + ks)      lm = (ny2 - koff - 1)/kyp2 + 1      koff = koff + kyp2*lm - ny2c this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((ll+1).le.kyb) thenc           do 90 j = 1, nxc           do 85 i = 1, 3c           cu2(i,j,1,l) = cu(i,j,1,ll+1)c  85       continuec  90       continuec        endifc        if (kyp.lt.kyp2) thenc           do 110 k = 1, kypc           do 100 j = 1, nxc           do 95 i = 1, 3c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll)c  95       continuec 100       continuec 110       continuec        endifc        if (kyp.gt.1) thenc           do 130 k = 2, kypc           do 120 j = 1, nxc           do 115 i = 1, 3c           cu2(i,j,k,l) = cu(i,j,k,ll-1)c 115       continuec 120       continuec 130       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_IRECV(cu2(1,1,1,l),3*nxv,mreal,ll,moff+2,lgrp,lsid,     1ierr)         endif         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),3*kyp*nxv,mreal,ll-1,moff+2,     1lgrp,msid,ierr)         endif         if (kyp.gt.1) then            call MPI_IRECV(cu2(1,1,2,l),3*(kyp-1)*nxv,mreal,ll-2,moff+2,     1lgrp,nsid,ierr)         endif      endif      if ((lm.gt.(kyb2/2)).and.(lm.le.kyb2)) then         if (koff.eq.0) then            if ((lm+1).le.kyb2) then               call MPI_SEND(cu(1,1,1,l),3*nxv,mreal,lm,moff+2,lgrp,ierr     1)            endif            if (kyp.gt.1) then               call MPI_SEND(cu(1,1,2,l),3*(kyp-1)*nxv,mreal,lm-1,moff+2     1,lgrp,ierr)            endif         else            call MPI_SEND(cu(1,1,1,l),3*kyp*nxv,mreal,lm-1,moff+2,lgrp,i     1err)         endif      endifc wait for data and unpack it      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_WAIT(lsid,istatus,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_WAIT(msid,istatus,ierr)            do 100 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 90 j = 1, nxv            do 85 i = 1, 3            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   85       continue   90       continue  100       continue         endif         if (kyp.gt.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 120 k = 3, kyp            k1 = kyp - k + 3            k2 = k1/2 + 1            joff = nxv*(k1 - 2*k2 + 2)            do 110 j = 1, nxv            do 105 i = 1, 3            cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)  105       continue  110       continue  120       continue         endif      endif  140 continuec create odd array      do 200 l = 1, k2blok      koff = kyp2*(l + ks)      do 190 k = 1, kyp2      kk = k + koff      if ((kk.eq.1).or.(kk.eq.(ny+1))) then         do 150 j = 1, nx         do 145 i = 1, 3         cu2(i,j,k,l) = 0.         cu2(i,j+nx,k,l) = 0.  145    continue  150    continue      else if (kk.le.ny) then         do 160 j = 1, nxs         cu2(1,nx+j+1,k,l) = cu2(1,nx-j+1,k,l)         cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)         cu2(3,nx+j+1,k,l) = -cu2(3,nx-j+1,k,l)  160    continue         do 165 i = 1, 3         cu2(i,1,k,l) = 0.         cu2(i,nx+1,k,l) = 0.  165    continue      else if (kk.gt.(ny+1)) then         if (k.eq.1) then            do 170 j = 1, nxs            cu2(1,nx+j+1,k,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)            cu2(3,nx+j+1,k,l) = cu2(3,nx-j+1,k,l)  170       continue         else            do 180 j = 1, nxs            cu2(1,nx+j+1,kyp2-k+2,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,kyp2-k+2,l) = -cu2(2,nx-j+1,k,l)            cu2(3,nx+j+1,kyp2-k+2,l) = cu2(3,nx-j+1,k,l)  180       continue         endif         do 185 i = 1, 3         cu2(i,1,k,l) = 0.         cu2(i,nx+1,k,l) = 0.  185    continue      endif  190 continue  200 continuec finish odd array      do 230 l = 1, k2blok      koff = kyp2*(l + ks)      do 220 k = 1, kyp2      kk = k + koff      if (kk.gt.(ny+1)) then         do 210 j = 1, nxs         cu2(1,nx-j+1,k,l) = cu2(1,nx+j+1,k,l)         cu2(2,nx-j+1,k,l) = -cu2(2,nx+j+1,k,l)         cu2(3,nx-j+1,k,l) = -cu2(3,nx+j+1,k,l)  210    continue         cu2(1,nx+1,k,l) = cu2(1,nx+1,k,l)         cu2(2,nx+1,k,l) = -cu2(2,nx+1,k,l)         cu2(3,nx+1,k,l) = -cu2(3,nx+1,k,l)      endif  220 continue  230 continue      return      endc-----------------------------------------------------------------------      subroutine PHAFDBL2C(fxy,fxy2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,     1k2blok)c this subroutine copies data from a double array to regular arrayc with guard cells for vector field and linear interpolationc for distributed datac fxy array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = second dimension of output array fxy, must be >= nxc kyp = number of data values per block in yc kypd = third dimension of output array fxy, must be >= kyp+1c kyp2 = third dimension of output array fxy2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real fxy, fxy2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension fxy(2,nxv,kypd,kblok), fxy2(2,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer j, k, l, nx1, ny1, kyb, kyb2, kyp1, ks, joff, koff, moff      integer kk, ll, lm      dimension istatus(lstat)      nx1 = nx + 1      ny1 = ny + 1      kyb = ny/kyp      kyb2 = (ny + ny)/kyp2      kyp1 = kyp + 1      ks = kstrt - 2      moff = kypd + kyb      do 90 l = 1, k2blok      koff = kyp2*(l + ks)      lm = koff/kyp + 1      koff = kyp*(l + ks)      ll = koff/kyp2 + 1      koff = koff - kyp2*(ll - 1)c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, ny1         do 10 j = 1, nx1         fxy(1,j,k,l) = fxy2(1,j,k,l)         fxy(2,j,k,l) = fxy2(2,j,k,l)   10    continue   20    continue         go to 90      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((koff.eq.0).and.(kyp.lt.kyp2)) thenc           do 40 k = 1, kyp1c           do 30 j = 1, nx1c           fxy(1,j,k,l) = fxy2(1,j,k,ll)c           fxy(2,j,k,l) = fxy2(2,j,k,ll)c  30       continuec  40       continuec        elsec           do 60 k = 1, kypc           do 50 j = 1, nx1c           fxy(1,j,k,l) = fxy2(1,j,k+koff,ll)c           fxy(2,j,k,l) = fxy2(2,j,k+koff,ll)c  50       continuec  60       continuec           do 70 j = 1, nx1c           fxy(1,j,kyp+1,l) = fxy2(1,j,1,ll+1)c           fxy(2,j,kyp+1,l) = fxy2(2,j,1,ll+1)c  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_IRECV(fxy(1,1,1,l),2*nxv*kyp1,mreal,ll-1,moff+3,lgr     1p,msid,ierr)         else            call MPI_IRECV(fxy(1,1,1,l),2*nxv*kyp,mreal,ll-1,moff+3,lgrp     1,msid,ierr)            call MPI_IRECV(fxy(1,1,kyp+1,l),2*nxv,mreal,ll,moff+3,lgrp,n     1sid,ierr)         endif      endifc pack data and send it      if (lm.le.kyb) then         if (kyp.lt.kyp2) then            do 40 k = 2, kyp1            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 30 j = 1, nxv            fxy2(1,j+joff,kk,l) = fxy2(1,j,k,l)            fxy2(2,j+joff,kk,l) = fxy2(2,j,k,l)   30       continue   40       continue            call MPI_SEND(fxy2(1,1,1,l),2*nxv*kyp1,mreal,lm-1,moff+3,lgr     1p,ierr)            do 60 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 50 j = 1, nxv            fxy2(1,j+joff,kk+kyp,l) = fxy2(1,j,k+kyp,l)            fxy2(2,j+joff,kk+kyp,l) = fxy2(2,j,k+kyp,l)   50       continue   60       continue            call MPI_SEND(fxy2(1,1,kyp+1,l),2*nxv*kyp,mreal,lm,moff+3,lg     1rp,ierr)         else            do 80 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 70 j = 1, nxv            fxy2(1,j+joff,kk,l) = fxy2(1,j,k,l)            fxy2(2,j+joff,kk,l) = fxy2(2,j,k,l)   70       continue   80       continue            call MPI_SEND(fxy2(1,1,1,l),2*nxv*kyp,mreal,lm-1,moff+3,lgrp     1,ierr)         endif         if (lm.gt.1) then            call MPI_SEND(fxy2(1,1,1,l),2*nxv,mreal,lm-2,moff+3,lgrp,ier     1r)         endif      else if (lm.eq.(kyb+1)) then         call MPI_SEND(fxy2(1,1,1,l),2*nxv,mreal,lm-2,moff+3,lgrp,ierr)      endifc wait for data      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_WAIT(msid,istatus,ierr)         else            call MPI_WAIT(msid,istatus,ierr)            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   90 continue      return      endc-----------------------------------------------------------------------      subroutine PHAFDBL2D(q,q2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2bl     1ok)c this subroutine copies data from a double array to regular arrayc with guard cells for scalar field and linear interpolationc for distributed datac q2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = second dimension of output array fxy, must be >= nxc kyp = number of data values per block in yc kypd = third dimension of output array fxy, must be >= kyp+1c kyp2 = third dimension of output array fxy2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real q, q2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension q(nxv,kypd,kblok), q2(2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer j, k, l, nx1, ny1, kyb, kyb2, kyp1, ks, joff, koff, moff      integer  kk, ll, lm      dimension istatus(lstat)      nx1 = nx + 1      ny1 = ny + 1      kyb = ny/kyp      kyb2 = (ny + ny)/kyp2      kyp1 = kyp + 1      ks = kstrt - 2      moff = kypd + kyb      do 90 l = 1, k2blok      koff = kyp2*(l + ks)      lm = koff/kyp + 1      koff = kyp*(l + ks)      ll = koff/kyp2 + 1      koff = koff - kyp2*(ll - 1)c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, ny1         do 10 j = 1, nx1         q(j,k,l) = q2(j,k,l)   10    continue   20    continue         go to 90      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((koff.eq.0).and.(kyp.lt.kyp2)) thenc           do 40 k = 1, kyp1c           do 30 j = 1, nx1c           q(j,k,l) = q2(j,k,ll)c  30       continuec  40       continuec        elsec           do 60 k = 1, kypc           do 50 j = 1, nx1c           q(j,k,l) = q2(j,k+koff,ll)c  50       continuec  60       continuec           do 70 j = 1, nx1c           q(j,kyp+1,l) = q2(j,1,ll+1)c  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_IRECV(q(1,1,l),nxv*kyp1,mreal,ll-1,moff+4,lgrp,msid     1,ierr)         else            call MPI_IRECV(q(1,1,l),nxv*kyp,mreal,ll-1,moff+4,lgrp,msid,     1ierr)            call MPI_IRECV(q(1,kyp+1,l),nxv,mreal,ll,moff+4,lgrp,nsid,ie     1rr)         endif      endifc pack data and send it      if (lm.le.kyb) then         if (kyp.lt.kyp2) then            do 40 k = 2, kyp1            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 30 j = 1, nxv            q2(j+joff,kk,l) = q2(j,k,l)   30       continue   40       continue            call MPI_SEND(q2(1,1,l),nxv*kyp1,mreal,lm-1,moff+4,lgrp,ierr     1)            do 60 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 50 j = 1, nxv            q2(j+joff,kk+kyp,l) = q2(j,k+kyp,l)   50       continue   60       continue            call MPI_SEND(q2(1,kyp+1,l),nxv*kyp,mreal,lm,moff+4,lgrp,ier     1r)         else            do 80 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 70 j = 1, nxv            q2(j+joff,kk,l) = q2(j,k,l)   70       continue   80       continue            call MPI_SEND(q2(1,1,l),nxv*kyp,mreal,lm-1,moff+4,lgrp,ierr)         endif         if (lm.gt.1) then            call MPI_SEND(q2(1,1,l),nxv,mreal,lm-2,moff+4,lgrp,ierr)         endif      else if (lm.eq.(kyb+1)) then         call MPI_SEND(q2(1,1,l),nxv,mreal,lm-2,moff+4,lgrp,ierr)      endifc wait for data      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_WAIT(msid,istatus,ierr)         else            call MPI_WAIT(msid,istatus,ierr)            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   90 continue      return      endc-----------------------------------------------------------------------      subroutine PHAFDBL2B(fxy,fxy2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,     1k2blok)c this subroutine copies data from a double array to regular arrayc with guard cells for vector field and linear interpolationc for distributed datac fxy array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = second dimension of output array fxy, must be >= nxc kyp = number of data values per block in yc kypd = third dimension of output array fxy, must be >= kyp+1c kyp2 = third dimension of output array fxy2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real fxy, fxy2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension fxy(3,nxv,kypd,kblok), fxy2(3,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer j, k, l, nx1, ny1, kyb, kyb2, kyp1, ks, joff, koff, moff      integer kk, ll, lm      dimension istatus(lstat)      nx1 = nx + 1      ny1 = ny + 1      kyb = ny/kyp      kyb2 = (ny + ny)/kyp2      kyp1 = kyp + 1      ks = kstrt - 2      moff = kypd + kyb      do 90 l = 1, k2blok      koff = kyp2*(l + ks)      lm = koff/kyp + 1      koff = kyp*(l + ks)      ll = koff/kyp2 + 1      koff = koff - kyp2*(ll - 1)c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, ny1         do 10 j = 1, nx1         fxy(1,j,k,l) = fxy2(1,j,k,l)         fxy(2,j,k,l) = fxy2(2,j,k,l)         fxy(3,j,k,l) = fxy2(3,j,k,l)   10    continue   20    continue         go to 90      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((koff.eq.0).and.(kyp.lt.kyp2)) thenc           do 40 k = 1, kyp1c           do 30 j = 1, nx1c           fxy(1,j,k,l) = fxy2(1,j,k,ll)c           fxy(2,j,k,l) = fxy2(2,j,k,ll)c           fxy(3,j,k,l) = fxy2(3,j,k,ll)c  30       continuec  40       continuec        elsec           do 60 k = 1, kypc           do 50 j = 1, nx1c           fxy(1,j,k,l) = fxy2(1,j,k+koff,ll)c           fxy(2,j,k,l) = fxy2(2,j,k+koff,ll)c           fxy(3,j,k,l) = fxy2(3,j,k+koff,ll)c  50       continuec  60       continuec           do 70 j = 1, nx1c           fxy(1,j,kyp+1,l) = fxy2(1,j,1,ll+1)c           fxy(2,j,kyp+1,l) = fxy2(2,j,1,ll+1)c           fxy(3,j,kyp+1,l) = fxy2(3,j,1,ll+1)c  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_IRECV(fxy(1,1,1,l),3*nxv*kyp1,mreal,ll-1,moff+3,lgr     1p,msid,ierr)         else            call MPI_IRECV(fxy(1,1,1,l),3*nxv*kyp,mreal,ll-1,moff+3,lgrp     1,msid,ierr)            call MPI_IRECV(fxy(1,1,kyp+1,l),3*nxv,mreal,ll,moff+3,lgrp,n     1sid,ierr)         endif      endifc pack data and send it      if (lm.le.kyb) then         if (kyp.lt.kyp2) then            do 40 k = 2, kyp1            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 30 j = 1, nxv            fxy2(1,j+joff,kk,l) = fxy2(1,j,k,l)            fxy2(2,j+joff,kk,l) = fxy2(2,j,k,l)            fxy2(3,j+joff,kk,l) = fxy2(3,j,k,l)   30       continue   40       continue            call MPI_SEND(fxy2(1,1,1,l),3*nxv*kyp1,mreal,lm-1,moff+3,lgr     1p,ierr)            do 60 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 50 j = 1, nxv            fxy2(1,j+joff,kk+kyp,l) = fxy2(1,j,k+kyp,l)            fxy2(2,j+joff,kk+kyp,l) = fxy2(2,j,k+kyp,l)            fxy2(3,j+joff,kk+kyp,l) = fxy2(3,j,k+kyp,l)   50       continue   60       continue            call MPI_SEND(fxy2(1,1,kyp+1,l),3*nxv*kyp,mreal,lm,moff+3,lg     1rp,ierr)         else            do 80 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 70 j = 1, nxv            fxy2(1,j+joff,kk,l) = fxy2(1,j,k,l)            fxy2(2,j+joff,kk,l) = fxy2(2,j,k,l)            fxy2(3,j+joff,kk,l) = fxy2(3,j,k,l)   70       continue   80       continue            call MPI_SEND(fxy2(1,1,1,l),3*nxv*kyp,mreal,lm-1,moff+3,lgrp     1,ierr)         endif         if (lm.gt.1) then            call MPI_SEND(fxy2(1,1,1,l),3*nxv,mreal,lm-2,moff+3,lgrp,ier     1r)         endif      else if (lm.eq.(kyb+1)) then         call MPI_SEND(fxy2(1,1,1,l),3*nxv,mreal,lm-2,moff+3,lgrp,ierr)      endifc wait for data      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_WAIT(msid,istatus,ierr)         else            call MPI_WAIT(msid,istatus,ierr)            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   90 continue      return      endc-----------------------------------------------------------------------      subroutine PLCGUARD2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.  for vector datac the field is replicated so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(2,nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer nx2, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx2 = nx + 2      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 70 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nxvc        f(1,j,1,l) = f(1,j,kyp+1,kl)c        f(2,j,1,l) = f(2,j,kyp+1,kl)c  10    continuec     elsec        do 20 j = 1, nxvc        f(1,j,1,l) = f(1,j,3,l)c        f(2,j,1,l) = f(2,j,3,l)c  20    continuec     endifc     if (kr.le.nvp) thenc        do 30 j = 1, nxvc        f(1,j,kyp+2,l) = f(1,j,2,kr)c        f(2,j,kyp+2,l) = f(2,j,2,kr)c        f(1,j,kyp+3,l) = f(1,j,ngc+1,krr)c        f(2,j,kyp+3,l) = f(2,j,ngc+1,krr)c  30    continuec     elsec        do 40 j = 2, nx2c        f(1,j,kyp+3,l) = 2.*f(1,j,kyp+2,l) - f(1,j,kyp+1,l)c        f(2,j,kyp+3,l) = 2.*f(2,j,kyp+2,l) - f(2,j,kyp+1,l)c  40    continuec     endifc     if (kyp.eq.1) thenc        if ((kl.eq.0).and.(kr.le.nvp)) thenc           do 50 j = 1, nxvc           f(1,j,1,l) = f(1,j,2,kr)c           f(2,j,1,l) = f(2,j,2,kr)c  50       continuec        endifc        if (kr.eq.nvp) thenc           do 60 j = 1, nxvc           f(1,j,kyp+3,l) = f(1,j,kyp+2,kr)c           f(2,j,kyp+3,l) = f(2,j,kyp+2,kr)c  60       continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kl-1,moff+3,lgrp,msid,ier     1r)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+1,l),2*nxv,mreal,kr-1,moff+3,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nxv         f(1,j,1,l) = f(1,j,3,l)         f(2,j,1,l) = f(2,j,3,l)   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(f(1,1,kyp+2,l),2*ngc*nxv,mreal,kr-1,moff+4,lgrp,     1msid,ierr)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,2,l),2*ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 2, nx2         f(1,j,kyp+3,l) = 2.*f(1,j,kyp+2,l) - f(1,j,kyp+1,l)         f(2,j,kyp+3,l) = 2.*f(2,j,kyp+2,l) - f(2,j,kyp+1,l)   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (krr.le.nvp) then            call MPI_IRECV(f(1,1,kyp+3,l),2*nxv,mreal,krr-1,moff+6,lgrp,     1msid,ierr)         else if (kr.le.nvp) then            call MPI_IRECV(f(1,1,kyp+3,l),2*nxv,mreal,kr-1,moff+6,lgrp,m     1sid,ierr)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kr-1,moff+6,lgrp,nsid,     1ierr)         endif         if (kll.ge.1) then            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,kll-1,moff+6,lgrp,ierr)         else if (kl.eq.1) then            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,kl-1,moff+6,lgrp,ierr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_SEND(f(1,1,kyp+2,l),2*nxv,mreal,kl-1,moff+6,lgrp,ie     1rr)         endif         if (kr.le.nvp) then            call MPI_WAIT(msid,istatus,ierr)         endif         if (kl.eq.0) then            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   70 continuec fix left edge      do 100 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 90 j = 2, nx2         f(1,j,1,l) = 2.*f(1,j,2,l) - f(1,j,1,l)         f(2,j,1,l) = 2.*f(2,j,2,l) - f(2,j,1,l)   90    continue      endif  100 continue      return      endc-----------------------------------------------------------------------      subroutine PLDGUARD2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.c the field is replicated so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer nx2, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx2 = nx + 2      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 70 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nxvc        f(j,1,l) = f(j,kyp+1,kl)c  10    continuec     elsec        do 20 j = 1, nxvc        f(j,1,l) = f(j,3,l)c  20    continuec     endifc     if (kr.le.nvp) thenc        do 30 j = 1, nxvc        f(j,kyp+2,l) = f(j,2,kr)c        f(j,kyp+3,l) = f(j,ngc+1,krr)c  30    continuec     elsec        do 40 j = 2, nx2c        f(j,kyp+3,l) = 2.*f(j,kyp+2,l) - f(j,kyp+1,l)c  40    continuec     endifc     if (kyp.eq.1) thenc        if ((kl.eq.0).and.(kr.le.nvp)) thenc           do 50 j = 1, nxvc           f(j,1,l) = f(j,2,kr)c  50       continuec        endifc        if (kr.eq.nvp) thenc           do 60 j = 1, nxvc           f(j,kyp+3,l) = f(j,kyp+2,kr)c  60       continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+3,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+3,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nxv         f(j,1,l) = f(j,3,l)   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+4,lgrp,msid     1,ierr)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 2, nx2         f(j,kyp+3,l) = 2.*f(j,kyp+2,l) - f(j,kyp+1,l)   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (krr.le.nvp) then            call MPI_IRECV(f(1,kyp+3,l),nxv,mreal,krr-1,moff+6,lgrp,msid     1,ierr)         else if (kr.le.nvp) then            call MPI_IRECV(f(1,kyp+3,l),nxv,mreal,kr-1,moff+6,lgrp,msid,     1ierr)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kr-1,moff+6,lgrp,nsid,ierr     1)         endif         if (kll.ge.1) then            call MPI_SEND(f(1,2,l),nxv,mreal,kll-1,moff+6,lgrp,ierr)         else if (kl.eq.1) then            call MPI_SEND(f(1,2,l),nxv,mreal,kl-1,moff+6,lgrp,ierr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_SEND(f(1,kyp+2,l),nxv,mreal,kl-1,moff+6,lgrp,ierr)         endif         if (kr.le.nvp) then            call MPI_WAIT(msid,istatus,ierr)         endif         if (kl.eq.0) then            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   70 continuec fix left edge      do 100 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 90 j = 2, nx2         f(j,1,l) = 2.*f(j,2,l) - f(j,1,l)   90    continue      endif  100 continue      return      endc-----------------------------------------------------------------------      subroutine PLBGUARD2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.  for vector datac the field is replicated so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(3,nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer nx2, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx2 = nx + 2      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 70 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nxvc        f(1,j,1,l) = f(1,j,kyp+1,kl)c        f(2,j,1,l) = f(2,j,kyp+1,kl)c        f(3,j,1,l) = f(3,j,kyp+1,kl)c  10    continuec     elsec        do 20 j = 1, nxvc        f(1,j,1,l) = f(1,j,3,l)c        f(2,j,1,l) = f(2,j,3,l)c        f(3,j,1,l) = f(3,j,3,l)c  20    continuec     endifc     if (kr.le.nvp) thenc        do 30 j = 1, nxvc        f(1,j,kyp+2,l) = f(1,j,2,kr)c        f(2,j,kyp+2,l) = f(2,j,2,kr)c        f(3,j,kyp+2,l) = f(3,j,2,kr)c        f(1,j,kyp+3,l) = f(1,j,ngc+1,krr)c        f(2,j,kyp+3,l) = f(2,j,ngc+1,krr)c        f(3,j,kyp+3,l) = f(3,j,ngc+1,krr)c  30    continuec     elsec        do 40 j = 2, nx2c        f(1,j,kyp+3,l) = 2.*f(1,j,kyp+2,l) - f(1,j,kyp+1,l)c        f(2,j,kyp+3,l) = 2.*f(2,j,kyp+2,l) - f(2,j,kyp+1,l)c        f(3,j,kyp+3,l) = 2.*f(3,j,kyp+2,l) - f(3,j,kyp+1,l)c  40    continuec     endifc     if (kyp.eq.1) thenc        if ((kl.eq.0).and.(kr.le.nvp)) thenc           do 50 j = 1, nxvc           f(1,j,1,l) = f(1,j,2,kr)c           f(2,j,1,l) = f(2,j,2,kr)c           f(3,j,1,l) = f(3,j,2,kr)c  50       continuec        endifc        if (kr.eq.nvp) thenc           do 60 j = 1, nxvc           f(1,j,kyp+3,l) = f(1,j,kyp+2,kr)c           f(2,j,kyp+3,l) = f(2,j,kyp+2,kr)c           f(3,j,kyp+3,l) = f(3,j,kyp+2,kr)c  60       continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kl-1,moff+3,lgrp,msid,ier     1r)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+1,l),3*nxv,mreal,kr-1,moff+3,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nxv         f(1,j,1,l) = f(1,j,3,l)         f(2,j,1,l) = f(2,j,3,l)         f(3,j,1,l) = f(3,j,3,l)   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(f(1,1,kyp+2,l),3*ngc*nxv,mreal,kr-1,moff+4,lgrp,     1msid,ierr)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,2,l),3*ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 2, nx2         f(1,j,kyp+3,l) = 2.*f(1,j,kyp+2,l) - f(1,j,kyp+1,l)         f(2,j,kyp+3,l) = 2.*f(2,j,kyp+2,l) - f(2,j,kyp+1,l)         f(3,j,kyp+3,l) = 2.*f(3,j,kyp+2,l) - f(3,j,kyp+1,l)   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (krr.le.nvp) then            call MPI_IRECV(f(1,1,kyp+3,l),3*nxv,mreal,krr-1,moff+6,lgrp,     1msid,ierr)         else if (kr.le.nvp) then            call MPI_IRECV(f(1,1,kyp+3,l),3*nxv,mreal,kr-1,moff+6,lgrp,m     1sid,ierr)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kr-1,moff+6,lgrp,nsid,     1ierr)         endif         if (kll.ge.1) then            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,kll-1,moff+6,lgrp,ierr)         else if (kl.eq.1) then            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,kl-1,moff+6,lgrp,ierr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_SEND(f(1,1,kyp+2,l),3*nxv,mreal,kl-1,moff+6,lgrp,ie     1rr)         endif         if (kr.le.nvp) then            call MPI_WAIT(msid,istatus,ierr)         endif         if (kl.eq.0) then            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   70 continuec fix left edge      do 100 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 90 j = 2, nx2         f(1,j,1,l) = 2.*f(1,j,2,l) - f(1,j,1,l)         f(2,j,1,l) = 2.*f(2,j,2,l) - f(2,j,1,l)         f(3,j,1,l) = 2.*f(3,j,2,l) - f(3,j,1,l)   90    continue      endif  100 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARD2(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok,ngds)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(3,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c scr(j,ngds,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok, ngds      dimension f(3,nxv,nypmx,kblok), scr(3,nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l, m      dimension istatus(lstat)      nx3 = nx + 3      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 170 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 20 j = 1, nx3c        do 10 m = 1, 3c        scr(m,j,1,l) = f(m,j,kyp+2,kl)c        scr(m,j,2,l) = f(m,j,kyp+3,kll)c  10    continuec  20    continuec     elsec        do 40 j = 1, nx3c        do 30 m = 1, 3c        scr(m,j,1,l) = 2.*f(m,j,1,l)c        scr(m,j,2,l) = -f(m,j,1,l)c  30    continuec  40    continuec     endifc     if (kr.le.nvp) thenc        do 60 j = 1, nx3c        do 50 m = 1, 3c        scr(m,j,3,l) = f(m,j,1,kr)c  50    continuec  60    continuec     elsec        do 80 j = 1, nx3c        do 70 m = 1, 3c        scr(m,j,3,l) = -f(m,j,kyp+3,l)c        f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + 2.*f(m,j,kyp+3,l)c        f(m,j,kyp+3,l) = 0.c  70    continuec  80    continuec     endifc     if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 100 j = 1, nx3c           do 90 m = 1, 3c           scr(m,j,1,l) = f(m,j,kyp+2,kl)c           scr(m,j,2,l) = -f(m,j,1,kl)c  90       continuec 100       continuec        else if (kl.eq.0) thenc           do 120 j = 1, nx3c           do 110 m = 1, 3c           scr(m,j,2,l) = 0.c 110       continuec 120       continuec        endifc last point is special with only one gridc        if ((kl.eq.(nvp-1)).and.(kl.ge.1)) thenc           do 140 j = 1, nx3c           do 130 m = 1, 3c           f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + f(m,j,kyp+3,kl)c 130    continuec 140    continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(scr,3*ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+2,l),3*ngc*nxv,mreal,kr-1,moff+1,lgrp,i     1err)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nx3         do 10 m = 1, 3         scr(m,j,1,l) = 2.*f(m,j,1,l)         scr(m,j,2,l) = -f(m,j,1,l)   10    continue   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(scr(1,1,3,l),3*nxv,mreal,kr-1,moff+2,lgrp,msid,i     1err)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,1,l),3*nxv,mreal,kl-1,moff+2,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 1, nx3         do 30 m = 1, 3         scr(m,j,3,l) = -f(m,j,kyp+3,l)         f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + 2.*f(m,j,kyp+3,l)         f(m,j,kyp+3,l) = 0.   30    continue   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (kll.ge.1) then            call MPI_IRECV(scr(1,1,2,l),3*nxv,mreal,kll-1,moff+5,lgrp,ms     1id,ierr)         else if (kl.eq.1) then            call MPI_IRECV(scr(1,1,2,l),3*nxv,mreal,kl-1,moff+5,lgrp,msi     1d,ierr)         endif         if (krr.le.nvp) then            call MPI_SEND(f(1,1,kyp+3,l),3*nxv,mreal,krr-1,moff+5,lgrp,i     1err)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_SEND(f(1,1,1,l),3*nxv,mreal,kr-1,moff+5,lgrp,ierr)         endif         if (kl.ge.1) then            call MPI_WAIT(msid,istatus,ierr)            if (kl.eq.1) then               do 60 j = 1, nx3               do 50 m = 1, 3               scr(m,j,2,l) = -scr(m,j,2,l)   50          continue   60          continue            endif         else            do 80 j = 1, nx3            do 70 m = 1, 3            scr(m,j,2,l) = 0.   70       continue   80       continue         endifc last point is special with only one grid         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_IRECV(f(1,1,kyp+3,l),3*nxv,mreal,kl-1,moff+6,lgrp,m     1sid,ierr)         endif         if (kr.eq.nvp) then            call MPI_SEND(f(1,1,kyp+3,l),3*nxv,mreal,kr-1,moff+6,lgrp,ie     1rr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_WAIT(msid,istatus,ierr)            do 140 j = 1, nx3            do 130 m = 1, 3            f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + f(m,j,kyp+3,l)            f(m,j,kyp+3,l) = 0.  130       continue  140       continue         endif      endifc add up the guard cells      do 160 j = 1, nx3      do 150 m = 1, 3      f(m,j,2,l) = f(m,j,2,l) + scr(m,j,1,l)      f(m,j,ngc+1,l) = f(m,j,ngc+1,l) + scr(m,j,2,l)      f(m,j,kyp+1,l) = f(m,j,kyp+1,l) + scr(m,j,3,l)  150 continue  160 continue  170 continuec zero out the left edge      do 200 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 190 j = 1, nx3         do 180 m = 1, 3         f(m,j,1,l) = 0.  180    continue  190    continue      endif  200 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARD22(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok,ngds     1)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(2,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c scr(j,ngds,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok, ngds      dimension f(2,nxv,nypmx,kblok), scr(2,nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l, m      dimension istatus(lstat)      nx3 = nx + 3      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 170 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nx3c        do 10 m = 1, 2c        scr(m,j,1,l) = f(m,j,kyp+2,kl)c        scr(m,j,2,l) = f(m,j,kyp+3,kll)c  10    continuec  20    continuec     elsec        do 40 j = 1, nx3c        do 30 m = 1, 2c        scr(m,j,1,l) = 2.*f(m,j,1,l)c        scr(m,j,2,l) = -f(m,j,1,l)c  30    continuec  40    continuec     endifc     if (kr.le.nvp) thenc        do 60 j = 1, nx3c        do 50 m = 1, 2c        scr(m,j,3,l) = f(m,j,1,kr)c  50    continuec  60    continuec     elsec        do 80 j = 1, nx3c        do 70 m = 1, 2c        scr(m,j,3,l) = -f(m,j,kyp+3,l)c        f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + 2.*f(m,j,kyp+3,l)c        f(m,j,kyp+3,l) = 0.c  70    continuec  80    continuec     endifc     if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 100 j = 1, nx3c           do 90 m = 1, 2c           scr(m,j,1,l) = f(m,j,kyp+2,kl)c           scr(m,j,2,l) = -f(m,j,1,kl)c  90       continuec 100       continuec        else if (kl.eq.0) thenc           do 120 j = 1, nx3c           do 110 m = 1, 2c           scr(m,j,2,l) = 0.c 110       continuec 120       continuec        endifc last point is special with only one gridc        if ((kl.eq.(nvp-1)).and.(kl.ge.1)) thenc           do 140 j = 1, nx3c           do 130 m = 1, 2c           f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + f(m,j,kyp+3,kl)c 130    continuec 140    continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(scr,2*ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+2,l),2*ngc*nxv,mreal,kr-1,moff+1,lgrp,i     1err)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nx3         do 10 m = 1, 2         scr(m,j,1,l) = 2.*f(m,j,1,l)         scr(m,j,2,l) = -f(m,j,1,l)   10    continue   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(scr(1,1,3,l),2*nxv,mreal,kr-1,moff+2,lgrp,msid,i     1err)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,1,l),2*nxv,mreal,kl-1,moff+2,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 1, nx3         do 30 m = 1, 2         scr(m,j,3,l) = -f(m,j,kyp+3,l)         f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + 2.*f(m,j,kyp+3,l)         f(m,j,kyp+3,l) = 0.   30    continue   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (kll.ge.1) then            call MPI_IRECV(scr(1,1,2,l),2*nxv,mreal,kll-1,moff+5,lgrp,ms     1id,ierr)         else if (kl.eq.1) then            call MPI_IRECV(scr(1,1,2,l),2*nxv,mreal,kl-1,moff+5,lgrp,msi     1d,ierr)         endif         if (krr.le.nvp) then            call MPI_SEND(f(1,1,kyp+3,l),2*nxv,mreal,krr-1,moff+5,lgrp,i     1err)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_SEND(f(1,1,1,l),2*nxv,mreal,kr-1,moff+5,lgrp,ierr)         endif         if (kl.ge.1) then            call MPI_WAIT(msid,istatus,ierr)            if (kl.eq.1) then               do 60 j = 1, nx3               do 50 m = 1, 2               scr(m,j,2,l) = -scr(m,j,2,l)   50          continue   60          continue            endif         else            do 80 j = 1, nx3            do 70 m = 1, 2            scr(m,j,2,l) = 0.   70       continue   80       continue         endifc last point is special with only one grid         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_IRECV(f(1,1,kyp+3,l),2*nxv,mreal,kl-1,moff+6,lgrp,m     1sid,ierr)         endif         if (kr.eq.nvp) then            call MPI_SEND(f(1,1,kyp+3,l),2*nxv,mreal,kr-1,moff+6,lgrp,ie     1rr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_WAIT(msid,istatus,ierr)            do 140 j = 1, nx3            do 130 m = 1, 2            f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + f(m,j,kyp+3,l)            f(m,j,kyp+3,l) = 0.  130       continue  140       continue         endif      endifc add up the guard cells      do 160 j = 1, nx3      do 150 m = 1, 2      f(m,j,2,l) = f(m,j,2,l) + scr(m,j,1,l)      f(m,j,ngc+1,l) = f(m,j,ngc+1,l) + scr(m,j,2,l)      f(m,j,kyp+1,l) = f(m,j,kyp+1,l) + scr(m,j,3,l)  150 continue  160 continue  170 continuec zero out the left edge      do 200 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 190 j = 1, nx3         do 180 m = 1, 2         f(m,j,1,l) = 0.  180    continue  190    continue      endif  200 continue      return      endc-----------------------------------------------------------------------      subroutine PLAGUARD2(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok,ngds)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c scr(j,ngds,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok, ngds      dimension f(nxv,nypmx,kblok), scr(nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx3 = nx + 3      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 90 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nx3c        scr(j,1,l) = f(j,kyp+2,kl)c        scr(j,2,l) = f(j,kyp+3,kll)c  10    continuec     elsec        do 20 j = 1, nx3c        scr(j,1,l) = 2.*f(j,1,l)c        scr(j,2,l) = -f(j,1,l)c  20    continuec     endifc     if (kr.le.nvp) thenc        do 30 j = 1, nx3c        scr(j,3,l) = f(j,1,kr)c  30    continuec     elsec        do 40 j = 1, nx3c        scr(j,3,l) = -f(j,kyp+3,l)c        f(j,kyp+2,l) = f(j,kyp+2,l) + 2.*f(j,kyp+3,l)c        f(j,kyp+3,l) = 0.c  40    continuec     endifc     if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 50 j = 1, nx3c           scr(j,1,l) = f(j,kyp+2,kl)c           scr(j,2,l) = -f(j,1,kl)c  50       continuec        else if (kl.eq.0) thenc           do 60 j = 1, nx3c           scr(j,2,l) = 0.c  60       continuec        endifc last point is special with only one gridc        if ((kl.eq.(nvp-1)).and.(kl.ge.1)) thenc           do 70 j = 1, nx3c           f(j,kyp+2,l) = f(j,kyp+2,l) + f(j,kyp+3,kl)c  70    continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(scr,ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+1,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 10 j = 1, nx3         scr(j,1,l) = 2.*f(j,1,l)         scr(j,2,l) = -f(j,1,l)   10    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(scr(1,3,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nx3         scr(j,3,l) = -f(j,kyp+3,l)         f(j,kyp+2,l) = f(j,kyp+2,l) + 2.*f(j,kyp+3,l)         f(j,kyp+3,l) = 0.   20    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (kll.ge.1) then            call MPI_IRECV(scr(1,2,l),nxv,mreal,kll-1,moff+5,lgrp,msid,i     1err)         else if (kl.eq.1) then            call MPI_IRECV(scr(1,2,l),nxv,mreal,kl-1,moff+5,lgrp,msid,ie     1rr)         endif         if (krr.le.nvp) then            call MPI_SEND(f(1,kyp+3,l),nxv,mreal,krr-1,moff+5,lgrp,ierr)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_SEND(f(1,1,l),nxv,mreal,kr-1,moff+5,lgrp,ierr)         endif         if (kl.ge.1) then            call MPI_WAIT(msid,istatus,ierr)            if (kl.eq.1) then               do 30 j = 1, nx3               scr(j,2,l) = -scr(j,2,l)   30          continue            endif         else            do 40 j = 1, nx3            scr(j,2,l) = 0.   40       continue         endifc last point is special with only one grid         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_IRECV(f(1,kyp+3,l),nxv,mreal,kl-1,moff+6,lgrp,msid,     1ierr)         endif         if (kr.eq.nvp) then            call MPI_SEND(f(1,kyp+3,l),nxv,mreal,kr-1,moff+6,lgrp,ierr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_WAIT(msid,istatus,ierr)            do 70 j = 1, nx3            f(j,kyp+2,l) = f(j,kyp+2,l) + f(j,kyp+3,l)            f(j,kyp+3,l) = 0.   70       continue         endif      endifc add up the guard cells      do 80 j = 1, nx3      f(j,2,l) = f(j,2,l) + scr(j,1,l)      f(j,ngc+1,l) = f(j,ngc+1,l) + scr(j,2,l)      f(j,kyp+1,l) = f(j,kyp+1,l) + scr(j,3,l)   80 continue   90 continuec zero out the left edge      do 110 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 100 j = 1, nx3         f(j,1,l) = 0.  100    continue      endif  110 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARDS2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine corrects current density data for particle boundaryc conditions which keep particles one grid away from the edgesc the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(3,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(3,nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer ks, moff, kr, krr, kl, kll, j, l, m      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 210 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = klc special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1      endifc fix edges if all points are on the same processor      if (kl.eq.0) then         if (kyp.gt.2) then            do 20 j = 2, nx            do 10 m = 1, 3            f(m,j+1,3,l) = f(m,j+1,3,l) + 2.*f(m,j+1,2,l)            f(m,j+1,4,l) = f(m,j+1,4,l) - f(m,j+1,2,l)            f(m,j+1,2,l) = 0.   10       continue   20       continue         else if (kyp.eq.2) then            do 40 j = 2, nx            do 30 m = 1, 3            f(m,j+1,3,l) = f(m,j+1,3,l) + 2.*f(m,j+1,2,l)   30       continue   40       continue         endif      endif      if (kr.eq.(nvp+1)) then         if (kyp.gt.1) then            do 60 j = 2, nx            do 50 m = 1, 3            f(m,j+1,kyp,l) = f(m,j+1,kyp,l) - f(m,j+1,kyp+2,l)            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) + 2.*f(m,j+1,kyp+2,l)            f(m,j+1,kyp+2,l) = 0.   50       continue   60       continue         else if (kyp.eq.1) then            do 80 j = 2, nx            do 70 m = 1, 3            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) + 2.*f(m,j+1,kyp+2,l)   70       continue   80       continue         endif      endifc this segment is used for shared memory computersc     if (kyp.eq.2) thenc        if (kl.eq.1) thenc           do 120 j = 2, nxc           do 110 m = 1, 3c           f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,2,kl)c 110       continuec 120       continuec        endifc     else if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 140 j = 2, nxc           do 130 m = 1, 3c           f(m,j+1,2,l) = f(m,j+1,2,l) + 2.*f(m,j+1,2,kl)c 130       continuec 140       continuec        endifc        if (kll.eq.1) thenc           do 160 j = 2, nxc           do 150 m = 1, 3c           f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,2,kll)c 150       continuec 160       continuec        endifc        if (kr.eq.nvp) thenc           do 180 j = 2, nxc           do 170 m = 1, 3c           f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) - f(m,j+1,kyp+2,kr)c 170       continuec 180       continuec        endifc     endifc this segment is used for mpi computers      if (kyp.eq.2) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kl-1,moff+1,lgrp,msid,     1ierr)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,kr-1,moff+1,lgrp,ierr)            do 100 j = 2, nx            do 90 m = 1, 3            f(m,j+1,2,l) = 0.   90       continue  100       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 120 j = 2, nx            do 110 m = 1, 3            f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  110       continue  120       continue         endif      else if (kyp.eq.1) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kl-1,moff+1,lgrp,msid,     1ierr)         endif         if (kll.eq.1) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kll-1,moff+1,lgrp,nsid     1,ierr)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,kr-1,moff+1,lgrp,ierr)            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,krr-1,moff+1,lgrp,ierr)            do 140 j = 2, nx            do 130 m = 1, 3            f(m,j+1,2,l) = 0.  130       continue  140       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 160 j = 2, nx            do 150 m = 1, 3            f(m,j+1,2,l) = f(m,j+1,2,l) + 2.*f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  150       continue  160       continue         endif         if (kll.eq.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 180 j = 2, nx            do 170 m = 1, 3            f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  170       continue  180       continue         endif         if (kr.eq.nvp) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kr-1,moff+2,lgrp,msid,     1ierr)         endif         if (kr.eq.(nvp+1)) then            call MPI_SEND(f(1,1,kyp+2,l),3*nxv,mreal,kl-1,moff+2,lgrp,ie     1rr)         endif         if (kr.eq.nvp) then            call MPI_WAIT(msid,istatus,ierr)            do 200 j = 2, nx            do 190 m = 1, 3            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  190       continue  200       continue         endif      endif  210 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARDS22(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine corrects current density data for particle boundaryc conditions which keep particles one grid away from the edgesc the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(2,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(2,nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer ks, moff, kr, krr, kl, kll, j, l, m      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 210 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = klc special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1      endifc fix edges if all points are on the same processor      if (kl.eq.0) then         if (kyp.gt.2) then            do 20 j = 2, nx            do 10 m = 1, 2            f(m,j+1,3,l) = f(m,j+1,3,l) + 2.*f(m,j+1,2,l)            f(m,j+1,4,l) = f(m,j+1,4,l) - f(m,j+1,2,l)            f(m,j+1,2,l) = 0.   10       continue   20       continue         else if (kyp.eq.2) then            do 40 j = 2, nx            do 30 m = 1, 2            f(m,j+1,3,l) = f(m,j+1,3,l) + 2.*f(m,j+1,2,l)   30       continue   40       continue         endif      endif      if (kr.eq.(nvp+1)) then         if (kyp.gt.1) then            do 60 j = 2, nx            do 50 m = 1, 2            f(m,j+1,kyp,l) = f(m,j+1,kyp,l) - f(m,j+1,kyp+2,l)            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) + 2.*f(m,j+1,kyp+2,l)            f(m,j+1,kyp+2,l) = 0.   50       continue   60       continue         else if (kyp.eq.1) then            do 80 j = 2, nx            do 70 m = 1, 2            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) + 2.*f(m,j+1,kyp+2,l)   70       continue   80       continue         endif      endifc this segment is used for shared memory computersc     if (kyp.eq.2) thenc        if (kl.eq.1) thenc           do 120 j = 2, nxc           do 110 m = 1, 2c           f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,2,kl)c 110       continuec 120       continuec        endifc     else if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 140 j = 2, nxc           do 130 m = 1, 2c           f(m,j+1,2,l) = f(m,j+1,2,l) + 2.*f(m,j+1,2,kl)c 130       continuec 140       continuec        endifc        if (kll.eq.1) thenc           do 160 j = 2, nxc           do 150 m = 1, 2c           f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,2,kll)c 150       continuec 160       continuec        endifc        if (kr.eq.nvp) thenc           do 180 j = 2, nxc           do 170 m = 1, 2c           f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) - f(m,j+1,kyp+2,kr)c 170       continuec 180       continuec        endifc     endifc this segment is used for mpi computers      if (kyp.eq.2) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kl-1,moff+1,lgrp,msid,     1ierr)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,kr-1,moff+1,lgrp,ierr)            do 100 j = 2, nx            do 90 m = 1, 2            f(m,j+1,2,l) = 0.   90       continue  100       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 120 j = 2, nx            do 110 m = 1, 2            f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  110       continue  120       continue         endif      else if (kyp.eq.1) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kl-1,moff+1,lgrp,msid,     1ierr)         endif         if (kll.eq.1) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kll-1,moff+1,lgrp,nsid     1,ierr)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,kr-1,moff+1,lgrp,ierr)            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,krr-1,moff+1,lgrp,ierr)            do 140 j = 2, nx            do 130 m = 1, 2            f(m,j+1,2,l) = 0.  130       continue  140       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 160 j = 2, nx            do 150 m = 1, 2            f(m,j+1,2,l) = f(m,j+1,2,l) + 2.*f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  150       continue  160       continue         endif         if (kll.eq.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 180 j = 2, nx            do 170 m = 1, 2            f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  170       continue  180       continue         endif         if (kr.eq.nvp) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kr-1,moff+2,lgrp,msid,     1ierr)         endif         if (kr.eq.(nvp+1)) then            call MPI_SEND(f(1,1,kyp+2,l),2*nxv,mreal,kl-1,moff+2,lgrp,ie     1rr)         endif         if (kr.eq.nvp) then            call MPI_WAIT(msid,istatus,ierr)            do 200 j = 2, nx            do 190 m = 1, 2            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  190       continue  200       continue         endif      endif  210 continue      return      endc-----------------------------------------------------------------------      subroutine PLAGUARDS2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine corrects the charge density data for particle boundaryc conditions which keep particles one grid away from the edgesc the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer ks, moff, kr, krr, kl, kll, j, l      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 110 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = klc special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1      endifc fix edges if all points are on the same processor      if (kl.eq.0) then         if (kyp.gt.2) then            do 10 j = 2, nx            f(j+1,3,l) = f(j+1,3,l) + 2.*f(j+1,2,l)            f(j+1,4,l) = f(j+1,4,l) - f(j+1,2,l)            f(j+1,2,l) = 0.   10       continue         else if (kyp.eq.2) then            do 20 j = 2, nx            f(j+1,3,l) = f(j+1,3,l) + 2.*f(j+1,2,l)   20       continue         endif      endif      if (kr.eq.(nvp+1)) then         if (kyp.gt.1) then            do 30 j = 2, nx            f(j+1,kyp,l) = f(j+1,kyp,l) - f(j+1,kyp+2,l)            f(j+1,kyp+1,l) = f(j+1,kyp+1,l) + 2.*f(j+1,kyp+2,l)            f(j+1,kyp+2,l) = 0.   30       continue         else if (kyp.eq.1) then            do 40 j = 2, nx            f(j+1,kyp+1,l) = f(j+1,kyp+1,l) + 2.*f(j+1,kyp+2,l)   40       continue         endif      endifc this segment is used for shared memory computersc     if (kyp.eq.2) thenc        if (kl.eq.1) thenc           do 60 j = 2, nxc           f(j+1,2,l) = f(j+1,2,l) - f(j+1,2,kl)c  60       continuec        endifc     else if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 70 j = 2, nxc           f(j+1,2,l) = f(j+1,2,l) + 2.*f(j+1,2,kl)c  70       continuec        endifc        if (kll.eq.1) thenc           do 80 j = 2, nxc           f(j+1,2,l) = f(j+1,2,l) - f(j+1,2,kll)c  80       continuec        endifc        if (kr.eq.nvp) thenc           do 90 j = 2, nxc           f(j+1,kyp+1,l) = f(j+1,kyp+1,l) - f(j+1,kyp+2,kr)c  90       continuec        endifc     endifc this segment is used for mpi computers      if (kyp.eq.2) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+1,lgrp,msid,ierr     1)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,2,l),nxv,mreal,kr-1,moff+1,lgrp,ierr)            do 50 j = 2, nx            f(j+1,2,l) = 0.   50       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 60 j = 2, nx            f(j+1,2,l) = f(j+1,2,l) - f(j+1,1,l)            f(j+1,1,l) = 0.   60       continue         endif      else if (kyp.eq.1) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+1,lgrp,msid,ierr     1)         endif         if (kll.eq.1) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kll-1,moff+1,lgrp,nsid,ier     1r)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,2,l),nxv,mreal,kr-1,moff+1,lgrp,ierr)            call MPI_SEND(f(1,2,l),nxv,mreal,krr-1,moff+1,lgrp,ierr)            do 70 j = 2, nx            f(j+1,2,l) = 0.   70       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 80 j = 2, nx            f(j+1,2,l) = f(j+1,2,l) + 2.*f(j+1,1,l)            f(j+1,1,l) = 0.   80       continue         endif         if (kll.eq.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 90 j = 2, nx            f(j+1,2,l) = f(j+1,2,l) - f(j+1,1,l)            f(j+1,1,l) = 0.   90       continue         endif         if (kr.eq.nvp) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr     1)         endif         if (kr.eq.(nvp+1)) then            call MPI_SEND(f(1,kyp+2,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)         endif         if (kr.eq.nvp) then            call MPI_WAIT(msid,istatus,ierr)            do 100 j = 2, nx            f(j+1,kyp+1,l) = f(j+1,kyp+1,l) - f(j+1,1,l)            f(j+1,1,l) = 0.  100       continue         endif      endif  110 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARD2L(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c no copying is done at the boundary edges.c f(3,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes one extra guard cell.c scr(3,j,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c linear interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(3,nxv,nypmx,kblok), scr(3,nxv,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx1, ks, moff, kl, kr, j, l, m      dimension istatus(lstat)      nx1 = nx + 1      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 80 l = 1, kblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nx1c        do 10 m = 1, 3c        scr(m,j,l) = f(m,j,kyp+1,kl)c  10    continuec  20    continuec     elsec        do 40 j = 1, nx1c        do 30 m = 1, 3c        scr(m,j,l) = 0.c  30    continuec  40    continuec     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(scr,3*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+1,l),3*nxv,mreal,kr-1,moff+1,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 1, nx1         do 30 m = 1, 3         scr(m,j,l) = 0.   30    continue   40    continue      endifc add up the guard cells      do 60 j = 1, nx1      do 50 m = 1, 3      f(m,j,1,l) = f(m,j,1,l) + scr(m,j,l)   50 continue   60 continue   80 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARD22L(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c no copying is done at the boundary edges.c f(2,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes one extra guard cell.c scr(2,j,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c linear interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(2,nxv,nypmx,kblok), scr(2,nxv,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx1, ks, moff, kl, kr, j, l, m      dimension istatus(lstat)      nx1 = nx + 1      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 80 l = 1, kblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nx1c        do 10 m = 1, 2c        scr(m,j,l) = f(m,j,kyp+1,kl)c  10    continuec  20    continuec     elsec        do 40 j = 1, nx1c        do 30 m = 1, 2c        scr(m,j,l) = 0.c  30    continuec  40    continuec     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(scr,2*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+1,l),2*nxv,mreal,kr-1,moff+1,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 1, nx1         do 30 m = 1, 2         scr(m,j,l) = 0.   30    continue   40    continue      endifc add up the guard cells      do 60 j = 1, nx1      do 50 m = 1, 2      f(m,j,1,l) = f(m,j,1,l) + scr(m,j,l)   50 continue   60 continue   80 continue      return      endc-----------------------------------------------------------------------      subroutine PLAGUARD2L(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c no copying is done at the boundary edges.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes one extra guard cell.c scr(j,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c linear interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok), scr(nxv,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx1, ks, moff, kl, kr, j, l      dimension istatus(lstat)      nx1 = nx + 1      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 40 l = 1, kblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nx1c        scr(j,l) = f(j,kyp+1,kl)c  10    continuec     elsec        do 20 j = 1, nx1c        scr(j,l) = 0.c  20    continuec     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(scr,nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+1,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nx1         scr(j,l) = 0.   20    continue      endifc add up the guard cells      do 30 j = 1, nx1      f(j,1,l) = f(j,1,l) + scr(j,l)   30 continue   40 continue      return      endc-----------------------------------------------------------------------      subroutine PMOVE2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc ierr = (0,1) = (no,yes) error condition exists      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer i, nbsize, nter, itermax      integer msid, istatus      real any, yt      dimension msid(4), istatus(lstat)      dimension ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0      itermax = 2000c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 50 l = 1, nblok      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 30 j = 1, npp(l)      yt = part(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            do 23 i = 1, idimp            sbufr(i,jsr(1,l),l) = part(i,j,l)   23       continue            sbufr(2,jsr(1,l),l) = yt            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endifc particles going down      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            do 27 i = 1, idimp            sbufl(i,jsl(1,l),l) = part(i,j,l)   27       continue            sbufl(2,jsl(1,l),l) = yt            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endif      endif   30 continue   40 jss(1,l) = jsl(1,l) + jsr(1,l)   50 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computersc     jsl(2,l) = jsr(1,kl)c     do 110 j = 1, jsl(2,l)c     do 105 i = 1, idimpc     rbufl(i,j,l) = sbufr(i,j,kl)c 105 continuec 110 continuec     jsr(2,l) = jsl(1,kr)c     do 120 j = 1, jsr(2,l)c     do 115 i = 1, idimpc     rbufr(i,j,l) = sbufl(i,j,kr)c 115 continuec 120 continuec this segment is used for mpi computersc post receive      call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)      call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particles      call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)     1,ierr)      call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),i     1err)c wait for particles to arrive      call MPI_WAIT(msid(1),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsl(2,l) = nps/idimp      call MPI_WAIT(msid(2),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (2,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (2,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sent      call MPI_WAIT(msid(3),istatus,ierr)      call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         rbufr(2,j,l) = yt         do 163 i = 1, idimp         sbufl(i,jsl(1,l),l) = rbufr(i,j,l)  163    continuec particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         rbufr(2,j,l) = yt         do 165 i = 1, idimp         sbufr(i,jsr(1,l),l) = rbufr(i,j,l)  165    continuec particles staying here      else         jss(2,l) = jss(2,l) + 1         do 167 i = 1, idimp         rbufr(i,jss(2,l),l) = rbufr(i,j,l)  167    continue      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            rbufl(2,j,l) = yt            do 173 i = 1, idimp            sbufr(i,jsr(1,l),l) = rbufl(i,j,l)  173       continue             else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            rbufl(2,j,l) = yt            do 175 i = 1, idimp            sbufl(i,jsl(1,l),l) = rbufl(i,j,l)  175       continue         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         do 177 i = 1, idimp         rbufl(i,jss(2,l),l) = rbufl(i,j,l)  177    continue      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      do 225 i = 1, idimp      part(i,ihole(j,l),l) = rbufl(i,j,l)  225 continue  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         do 233 i = 1, idimp         part(i,ihole(j+jsl(2,l),l),l) = rbufr(i,j,l)  233    continue      elsec no more holesc distribute remaining particles from below into bottom         do 237 i = 1, idimp         part(i,j+npp(l),l) = rbufl(i,j+jss(1,l),l)  237    continue      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            do 243 i = 1, idimp            part(i,ihole(j2,l),l) = part(i,j1,l)  243       continue         endif      elsec no more holesc distribute remaining particles from above into bottom         do 247 i = 1, idimp         part(i,j+npp(l),l) = rbufr(i,j+jss(1,l),l)  247    continue      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (2,*) 'Info: particles being passed further = ', ibflg(2)         if (ibflg(3).gt.0) ibflg(3) = 1         if (iter.lt.itermax) go to 100         ierr = -iter/2         write (2,*) 'Iteration overflow, iter = ', ierr         go to 280      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after move      nps = 0      do 270 l = 1, nblok      nps = nps + npp(l)  270 continue      ibflg(2) = nps      ibflg(1) = npr      call PISUM(ibflg,iwork,2,1)      if (ibflg(1).ne.ibflg(2)) then         write (2,*) 'particle number error, old/new=',ibflg(1),ibflg(2)         ierr = 1      endifc information  280 if (nter.gt.0) then         write (2,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine PXMOV2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ier     2r)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc maskp = scratch array for particle addressesc ierr = (0,1) = (no,yes) error condition existsc optimized for vector processor      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, maskp, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok), maskp(npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer i, nbsize, nter, itermax      integer msid, istatus      real any, yt      dimension msid(4), istatus(lstat)      dimension ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0      itermax = 2000c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 80 l = 1, nblok      jss(1,l) = 0      jss(2,l) = 0c find mask function for particles out of bounds      do 30 j = 1, npp(l)      yt = part(2,j,l)      if ((yt.ge.edges(2,l)).or.(yt.lt.edges(1,l))) then         jss(1,l) = jss(1,l) + 1         maskp(j,l) = 1      else         maskp(j,l) = 0      endif   30 continuec set flag if hole buffer would overflow      if (jss(1,l).gt.ntmax) then         jss(1,l) = ntmax         jss(2,l) = 1      endifc accumulate location of holes      do 40 j = 2, npp(l)      maskp(j,l) = maskp(j,l) + maskp(j-1,l)   40 continuec store addresses of particles out of bounds      do 50 j = 2, npp(l)      if ((maskp(j,l).gt.maskp(j-1,l)).and.(maskp(j,l).le.ntmax)) then         ihole(maskp(j,l),l) = j      endif   50 continue      if (maskp(1,l).gt.0) ihole(1,l) = 1      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0c load particle buffers      do 60 j = 1, jss(1,l)      yt = part(2,ihole(j,l),l)c particles going up      if (yt.ge.edges(2,l)) then         if ((kb+1).eq.nvp) yt = yt - any         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            do 53 i = 1, idimp            sbufr(i,jsr(1,l),l) = part(i,ihole(j,l),l)   53       continue            sbufr(2,jsr(1,l),l) = yt            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endifc particles going down      else         if (kb.eq.0) yt = yt + any         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            do 57 i = 1, idimp            sbufl(i,jsl(1,l),l) = part(i,ihole(j,l),l)   57       continue            sbufl(2,jsl(1,l),l) = yt            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endif      endif   60 continue   70 jss(1,l) = jsl(1,l) + jsr(1,l)   80 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computersc     jsl(2,l) = jsr(1,kl)c     do 110 j = 1, jsl(2,l)c     do 105 i = 1, idimpc     rbufl(i,j,l) = sbufr(i,j,kl)c 105 continuec 110 continuec     jsr(2,l) = jsl(1,kr)c     do 120 j = 1, jsr(2,l)c     do 115 i = 1, idimpc     rbufr(i,j,l) = sbufl(i,j,kr)c 115 continuec 120 continuec this segment is used for mpi computersc post receive      call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)      call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particles      call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)     1,ierr)      call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),i     1err)c wait for particles to arrive      call MPI_WAIT(msid(1),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsl(2,l) = nps/idimp      call MPI_WAIT(msid(2),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (2,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (2,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sent      call MPI_WAIT(msid(3),istatus,ierr)      call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         rbufr(2,j,l) = yt         do 163 i = 1, idimp         sbufl(i,jsl(1,l),l) = rbufr(i,j,l)  163    continuec particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         rbufr(2,j,l) = yt         do 165 i = 1, idimp         sbufr(i,jsr(1,l),l) = rbufr(i,j,l)  165    continuec particles staying here      else         jss(2,l) = jss(2,l) + 1         do 167 i = 1, idimp         rbufr(i,jss(2,l),l) = rbufr(i,j,l)  167    continue      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            rbufl(2,j,l) = yt            do 173 i = 1, idimp            sbufr(i,jsr(1,l),l) = rbufl(i,j,l)  173       continue         else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            rbufl(2,j,l) = yt            do 175 i = 1, idimp            sbufl(i,jsl(1,l),l) = rbufl(i,j,l)  175       continue         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         do 177 i = 1, idimp         rbufl(i,jss(2,l),l) = rbufl(i,j,l)  177    continue      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      do 225 i = 1, idimp      part(i,ihole(j,l),l) = rbufl(i,j,l)  225 continue  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         do 233 i = 1, idimp         part(i,ihole(j+jsl(2,l),l),l) = rbufr(i,j,l)  233    continue      elsec no more holesc distribute remaining particles from below into bottom         do 237 i = 1, idimp         part(i,j+npp(l),l) = rbufl(i,j+jss(1,l),l)  237    continue      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            do 243 i = 1, idimp            part(i,ihole(j2,l),l) = part(i,j1,l)  243       continue         endif      elsec no more holesc distribute remaining particles from above into bottom         do 247 i = 1, idimp         part(i,j+npp(l),l) = rbufr(i,j+jss(1,l),l)  247    continue      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (2,*) 'Info: particles being passed further = ', ibflg(2)         if (ibflg(3).gt.0) ibflg(3) = 1         if (iter.lt.itermax) go to 100         ierr = -iter/2         write (2,*) 'Iteration overflow, iter = ', ierr         go to 280      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after move      nps = 0      do 270 l = 1, nblok      nps = nps + npp(l)  270 continue      ibflg(2) = nps      ibflg(1) = npr      call PISUM(ibflg,iwork,2,1)      if (ibflg(1).ne.ibflg(2)) then         write (2,*) 'particle number error, old/new=',ibflg(1),ibflg(2)         ierr = 1      endifc information  280 if (nter.gt.0) then         write (2,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine PFMOVE2(f,g,noff,nyp,noffd,nypd,jsr,jsl,isign,kyp,kstrt     1,nvp,nxv,nypmx,nblok,idps,mter,ierr)c this subroutine moves fields into appropriate spatial regions,c between non-uniform and uniform partitionsc f(j,k,l) = real data for grid j,k in field partition l.c the grid is non-uniform and includes three extra guard cells.c g(j,k,l) = scratch data for grid j,k in field partition l.c noff(l) = lowermost global gridpoint in field partition lc nyp(l) = number of primary gridpoints in field partition lc noffd(l)/nypd(l) = scratch arrays for field partition lc jsl(idps,l) = number of particles going down in field partition lc jsr(idps,l) = number of particles going up in field partition lc isign = -1, move from non-uniform to uniform fieldsc isign = 1, move from uniform to non-uniform fieldsc kyp = number of complex grids in each uniform field partition.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c nblok = number of field partitions.c idps = number of partition boundariesc mter = number of shifts requiredc if mter = 0, then number of shifts is determined and returnedc ierr = (0,1) = (no,yes) error condition exists      implicit none      real f, g      integer noff, nyp, noffd, nypd, jsr, jsl      integer isign, kyp, kstrt, nvp, nxv, nypmx, nblok, idps, mter      integer ierr      dimension f(nxv,nypmx,nblok), g(nxv,nypmx,nblok)      dimension noff(nblok), nyp(nblok), noffd(nblok), nypd(nblok)      dimension jsl(idps,nblok), jsr(idps,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer j, k, l      integer nbsize, ks, iter, npr, nps, nter, koff, kl, kr, kk      integer msid, istatus      integer ibflg, iwork      dimension istatus(lstat)      dimension ibflg(2), iwork(2)      if (isign.eq.0) return      ks = kstrt - 2      nbsize = nxv*nypmx      iter = 2      ierr = 0c move from non-uniform to uniform fields   10 if (isign.lt.0) thenc copy non-uniform partition parameters         if (iter.eq.2) then            do 20 l = 1, nblok            noffd(l) = noff(l)            nypd(l) = nyp(l)   20       continue         endifc determine number of outgoing grids         do 40 l = 1, nblok         kl = kyp*(l + ks)         kr = kl + kyp         jsl(1,l) = 0         jsr(1,l) = 0         do 30 k = 1, nypd(l)         kk = k + noffd(l)c fields going up         if (kk.gt.kr) then            jsr(1,l) = jsr(1,l) + 1c fields going down         else if (kk.le.kl) then            jsl(1,l) = jsl(1,l) + 1         endif   30    continue   40    continuec move from uniform to non-uniform fields      else if (isign.gt.0) thenc set uniform partition parameters         if (iter.eq.2) then            do 50 l = 1, nblok            koff = kyp*(l + ks)            noffd(l) = koff            nypd(l) = kyp   50       continue         endifc determine number of outgoing grids         do 70 l = 1, nblok         kl = noff(l)         kr = kl + nyp(l)         jsl(1,l) = 0         jsr(1,l) = 0         do 60 k = 1, nypd(l)         kk = k + noffd(l)c fields going up         if (kk.gt.kr) then            jsr(1,l) = jsr(1,l) + 1c fields going down         else if (kk.le.kl) then            jsl(1,l) = jsl(1,l) + 1         endif   60    continue   70    continuec exit without processing      else         return      endifc copy fields      iter = iter + 2      npr = 0      nter = 0c get fields from below      do 100 l = 1, nblok      kr = l + ks + 2      kl = l + ks      jsl(2,l) = 0      jsr(2,l) = 0c this segment is used for shared memory computersc     if (kl.gt.0) thenc        jsl(2,l) = jsr(1,kl)c        do 90 k = 1, jsl(2,l)c        do 80 j = 1, nxvc        g(j,k,l) = f(j,k+nypd(kl)-jsr(1,kl),kl)c  80    continuec  90    continuec     endifc this segment is used for mpi computersc post receive from left      if (kl.gt.0) then         call MPI_IRECV(g,nbsize,mreal,kl-1,iter-1,lgrp,msid,ierr)      endifc send fields to right      if (kr.le.nvp) then         call MPI_SEND(f(1,nypd(l)-jsr(1,l)+1,l),nxv*jsr(1,l),mreal,kr-1     1,iter-1,lgrp,ierr)      endifc wait for fields to arrive      if (kl.gt.0) then         call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nps,ierr)         jsl(2,l) = nps/nxv      endif  100 continuec adjust field      do 170 l = 1, nblokc adjust field size      nypd(l) = nypd(l) - jsr(1,l)c do not allow move to overflow field array      jsr(1,l) = max0((nypd(l)+jsl(2,l)-nypmx),0)      nypd(l) = nypd(l) - jsr(1,l)      if (jsr(1,l).gt.0) then         npr = max0(npr,jsr(1,l))c save whatever is possible into end of g         kk = min0(jsr(1,l),nypmx-jsl(2,l))         do 120 k = 1, kk         do 110 j = 1, nxv         g(j,nypmx-kk+k,l) = f(j,nypd(l)+k,l)  110    continue  120    continue      endifc shift data which is staying, if necessary      if ((nypd(l).gt.0).and.(jsl(2,l).gt.0)) then         do 140 k = 1, nypd(l)         kk = nypd(l) - k + 1         do 130 j = 1, nxv         f(j,kk+jsl(2,l),l) = f(j,kk,l)  130    continue  140    continue      endifc insert data coming from left      do 160 k = 1, jsl(2,l)      do 150 j = 1, nxv      f(j,k,l) = g(j,k,l)  150 continue  160 continuec adjust field size and offset      nypd(l) = nypd(l) + jsl(2,l)      noffd(l) = noffd(l) - jsl(2,l)  170 continuec get fields from above      do 200 l = 1, nblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if (kr.le.nvp) thenc        jsr(2,l) = jsl(1,kr)c        do 190 k = 1, jsr(2,l)c        do 180 j = 1, nxvc        g(j,k,l) =  f(j,k,kr)c 180    continuec 190    continuec     endifc this segment is used for mpi computersc post receive from right      if (kr.le.nvp) then         call MPI_IRECV(g,nbsize,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send fields to left      if (kl.gt.0) then         call MPI_SEND(f(1,1,l),nxv*jsl(1,l),mreal,kl-1,iter,lgrp,ierr)      endifc wait for fields to arrive      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nps,ierr)         jsr(2,l) = nps/nxv      endif  200 continuec adjust field      do 260 l = 1, nblokc adjust field size      nypd(l) = nypd(l) - jsl(1,l)      noffd(l) = noffd(l) + jsl(1,l)c shift data which is staying, if necessary      if ((nypd(l).gt.0).and.(jsl(1,l).gt.0)) then         do 220 k = 1, nypd(l)         do 210 j = 1, nxv         f(j,k,l) = f(j,k+jsl(1,l),l)  210    continue  220    continue      endifc do not allow move to overflow field array      jsl(1,l) = max0((nypd(l)+jsr(2,l)-nypmx),0)      if (jsl(1,l).gt.0) then         npr = max0(npr,jsl(1,l))         jsr(2,l) = jsr(2,l) - jsl(1,l)c do not process if prior error      else if (jsr(1,l).gt.0) then         go to 250      endifc insert data coming from right      do 240 k = 1, jsr(2,l)      do 230 j = 1, nxv      f(j,k+nypd(l),l) = g(j,k,l)  230 continue  240 continuec adjust field size and offset      nypd(l) = nypd(l) + jsr(2,l)c check if new partition is uniform  250 if (isign.lt.0) then         nter = nter + abs(nypd(l)-kyp) + abs(noffd(l)-kyp*(l+ks))c check if new partition is non-uniform      else if (isign.gt.0) then         nter = nter + abs(nypd(l)-nyp(l)) + abs(noffd(l)-noff(l))      endif  260 continuec calculate number of iterations      nps = iter/2 - 1      if (nps.le.mter) thenc process errors         if (npr.ne.0) then            ierr = npr            write (2,*) 'local field overflow error, ierr = ', ierr            return         endif         if (nps.lt.mter) go to 10         return      endifc process errors      ibflg(1) = npr      ibflg(2) = nter      call PIMAX(ibflg,iwork,2,1)c field overflow error      if (ibflg(1).ne.0) then         ierr = ibflg(1)         write (2,*) 'global field overflow error, ierr = ', ierr         return      endifc check if any fields have to be passed further      if (ibflg(2).gt.0) then         write (2,*) 'Info: fields being passed further = ', ibflg(2)         go to 10      endif      mter = nps      return      endc-----------------------------------------------------------------------      subroutine PTPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jb     1lok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(nxv,kypd,kblok), g(nyv,kxpd,jblok)      dimension s(kxp,kyp,kblok), t(kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb      integer jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff,j,l) = f(j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,l),kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,msid,     1ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(j,k,l) = f(j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,l),kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,ierr     1)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(k+koff,j,l) = t(j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine P2TPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(2,nxv,kypd,kblok), g(2,nyv,kxpd,jblok)      dimension s(2,kxp,kyp,kblok), t(2,kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb      integer jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1,k+koff,j,l) = f(1,j+joff,k,i)c     g(2,k+koff,j,l) = f(2,j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,1,l),2*kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,m     1sid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(1,j,k,l) = f(1,j+joff,k,l)         s(2,j,k,l) = f(2,j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,1,l),2*kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,     1ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(1,k+koff,j,l) = t(1,j,k,l)         g(2,k+koff,j,l) = t(2,j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine P3TPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:3,k+kyp*(m-1),j,l) = f(1:3,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(3,nxv,kypd,kblok), g(3,nyv,kxpd,jblok)      dimension s(3,kxp,kyp,kblok), t(3,kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb      integer jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1,k+koff,j,l) = f(1,j+joff,k,i)c     g(2,k+koff,j,l) = f(2,j+joff,k,i)c     g(3,k+koff,j,l) = f(3,j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,1,l),3*kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,m     1sid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(1,j,k,l) = f(1,j+joff,k,l)         s(2,j,k,l) = f(2,j+joff,k,l)         s(3,j,k,l) = f(3,j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,1,l),3*kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,     1ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(1,k+koff,j,l) = t(1,j,k,l)         g(2,k+koff,j,l) = t(2,j,k,l)         g(3,k+koff,j,l) = t(3,j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine PTPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblok     1,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(nxv*kypd*kblok), g(nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff+nyv*(j-1+kxpd*(l-1))) = f(j+joff+nxv*(k-1+kypd*(i-1)))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, kxp         g(j+kxp*(k+is)) = f(j+joff+nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continuec exchange data      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+kxp*kyp*(ir+koff)),kxp*kyp,mcplx,ir-1,ir+kxy     1m+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+kxp*kyp*(is+ioff)),kxp*kyp,mcplx,is-1,l+ks+kx     1ym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(k+koff+nyv*(j+joff)) = f(j+kxp*(k+ir))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      subroutine P2TPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(2*nxv*kypd*kblok), g(2*nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks) - 1c     do 30 i = 1, kybc     koff = kyp*(i - 1) - 1c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(1+2*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(2+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(2+2*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = 2*kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, 2*kxp         g(j+2*kxp*(k+is)) = f(j+joff+2*nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continue      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+2*kxp*kyp*(ir+koff)),2*kxp*kyp,mcplx,ir-1,ir     1+kxym+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+2*kxp*kyp*(is+ioff)),2*kxp*kyp,mcplx,is-1,l+k     1s+kxym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(2*(k+koff+nyv*(j+joff))-1) = f(2*(j+kxp*(k+ir))-1)         g(2*(k+koff+nyv*(j+joff))) = f(2*(j+kxp*(k+ir)))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      subroutine P3TPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:3,k+kyp*(m-1),j,l) = f(1:3,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(3*nxv*kypd*kblok), g(3*nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks) - 1c     do 30 i = 1, kybc     koff = kyp*(i - 1) - 1c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1+3*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(1+3*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(2+3*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(2+3*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(3+3*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(3+3*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = 3*kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, 3*kxp         g(j+3*kxp*(k+is)) = f(j+joff+3*nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continue      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+3*kxp*kyp*(ir+koff)),3*kxp*kyp,mcplx,ir-1,ir     1+kxym+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+3*kxp*kyp*(is+ioff)),3*kxp*kyp,mcplx,is-1,l+k     1s+kxym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(3*(k+koff+nyv*(j+joff))-2) = f(3*(j+kxp*(k+ir))-2)         g(3*(k+koff+nyv*(j+joff))-1) = f(3*(j+kxp*(k+ir))-1)         g(3*(k+koff+nyv*(j+joff))) = f(3*(j+kxp*(k+ir)))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      subroutine PWRITE2(f,nx,kyp,nxv,iunit,nrec,name)c this subroutine collects distributed real data f and writes to a filec f = input data to be written, modified on node 0c nx/kyp = length of data f in x/y on each processor to writec nxv = first dimension of data array f, must be >= nxc iunit = fortran unit numberc nrec = current record number for write (if negative, open file withc recl=-nren)c name = file name (used only if nren < 0)c input: f, nx, kyp, nxv, iunit, nrec, fnamec output: nrec      implicit none      integer nx, kyp, nxv, iunit, nrec      real f      character*(*) name      dimension f(nxv,kyp)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc lworld = MPI_COMM_WORLD communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lrec, nvp, idproc, np, ioff, id, nrec0, i, j, k      integer ierr      dimension istatus(lstat)c this segment is used for shared memory computersc     if (nrec.lt.0) thenc        lrec = -nrecc        open(unit=iunit,file=name,form='unformatted',access='direct',rec    1cl=lrec,status='unknown')c        nrec = 1c     endifc     write (unit=iunit,rec=nrec) ((f(j,k),j=1,nx),k=1,kyp)c     nrec = nrec + 1c this segment is used for mpi computersc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lworld,idproc,ierr)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lworld,nvp,ierr)c node 0 receives messages from other nodes      if (idproc.eq.0) then         if (nrec.lt.0) then            lrec = -nrec            open(unit=iunit,file=name,form='unformatted',access='direct'     1,recl=lrec,status='unknown')            nrec = 1         endifc no special diagnostic node         if (nvp.eq.nproc) then            np = nvp            ioff = 1c special diagnostic node present         else            np = nvp - nproc            ioff = 0            id = 1            call MPI_RECV(f,nxv*kyp,mreal,id,99,lworld,istatus,ierr)         endifc first write data for node 0         nrec0 = nrec         write (unit=iunit,rec=nrec) ((f(j,k),j=1,nx),k=1,kyp)         nrec = nrec + 1c then write data from remaining nodes         do 10 i = 2, np            id = i - ioff            call MPI_RECV(f,nxv*kyp,mreal,id,99,lworld,istatus,ierr)            write (unit=iunit,rec=nrec) ((f(j,k),j=1,nx),k=1,kyp)            nrec = nrec + 1   10    continuec read data back for node 0         read (unit=iunit,rec=nrec0) ((f(j,k),j=1,nx),k=1,kyp)c other nodes send data to node 0      elseif (idproc.le.(nproc+1)) then         call MPI_SEND(f,nxv*kyp,mreal,0,99,lworld,ierr)      endif      return      endc-----------------------------------------------------------------------      subroutine PHEAD2(iunit,nx,ny,nvp,fname)c this subroutine writes header file for diagnosticsc iunit = fortran unit numberc nx = size of first dimensionc ny = size of second dimensionc nvp = size of third dimensionc name = file namec input: all      implicit none      integer iunit, nx, ny, nvp      character*(*) fnamec common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc lworld = MPI_COMM_WORLD communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer idproc, ierrc this segment is used for shared memory computersc     idproc = 0c this segment is used for mpi computersc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lworld,idproc,ierr)      if (idproc.ne.0) return      open(unit=iunit,file=fname//'head',form='formatted',status='unknow     1n')      write (iunit,*) nx, ny, nvp, fname      close(unit=iunit)      return      end