c 2d parallel PIC library for MPI communicationsc written by viktor k. decyk, uclac copyright 1995, regents of the university of californiac update: september 9, 2000c-----------------------------------------------------------------------      subroutine PPINIT(idproc,nvp)c this subroutine initializes parallel processingc input: nvp, output: idprocc idproc = processor idc nvp = number of real or virtual processors requested      implicit none      integer idproc, nvpc get definition of MPI constantsc     include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for realsc mint = default datatype for integersc mcplx = default datatype for complex type      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local datac     integer ierror, ndprec      save /pparms/c ndprec = (0,1) = (no,yes) use (normal,autodouble) precision      data ndprec /0/c this segment is used for shared memory computers      nproc = nvp      idproc = 0c this segment is used for mpi computersc     if (MPI_STATUS_SIZE.gt.lstat) thenc        write (2,*) ' status size too small, actual/required = ', lstatc    1, MPI_STATUS_SIZEc        stopc     endifc initialize the MPI execution environmentc     call MPI_INIT(ierror)c     if (ierror.ne.0) stopc     lgrp = MPI_COMM_WORLDc determine the rank of the calling process in the communicatorc     call MPI_COMM_RANK(lgrp,idproc,ierror)c determine the size of the group associated with a communicatorc     call MPI_COMM_SIZE(lgrp,nproc,ierror)c set default datatypesc        mint = MPI_INTEGERc single precisionc     if (ndprec.eq.0) thenc        mreal = MPI_REALc        mcplx = MPI_COMPLEXc double precisionc     elsec        mreal = MPI_DOUBLE_PRECISIONc        mcplx = MPI_DOUBLE_COMPLEXc     endifc requested number of processors not obtained      if (nproc.ne.nvp) then         write (2,*) ' processor number error: nvp, nproc=', nvp, nproc         call PPEXIT         stop      endif      return      endc-----------------------------------------------------------------------      subroutine PPEXITc this subroutine terminates parallel processing      implicit nonec common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplxc lgrp = current communicator      common /pparms/ nproc, lgrp, mreal, mint, mcplxc     integer ierrorc synchronize processesc     call MPI_BARRIER(lgrp,ierror)c terminate MPI execution environmentc     call MPI_FINALIZE(ierror)      return      endc-----------------------------------------------------------------------      subroutine DCOMP2(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c this subroutine determines spatial boundaries for particlec decomposition, calculates number of grid points in each spatialc region, and the offset of these grid points from the global addressc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.c ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idps = number of partition boundariesc nblok = number of particle partitions.      implicit none      real edges      integer nyp, noff, ny, kstrt, nvp, idps, nblok      dimension edges(idps,nblok)      dimension nyp(nblok), noff(nblok)c local data      integer ks, kb, kr, l      real at1      ks = kstrt - 2      at1 = float(ny)/float(nvp)      do 10 l = 1, nblok      kb = l + ks      edges(1,l) = at1*float(kb)      noff(l) = edges(1,l) + .5      edges(2,l) = at1*float(kb + 1)      kr = edges(2,l) + .5      nyp(l) = kr - noff(l)   10 continue      return      endc-----------------------------------------------------------------------      subroutine DCOMP2L(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c this subroutine determines spatial boundaries for particlec decomposition, calculates number of grid points in each spatialc region, and the offset of these grid points from the global addressc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.c ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idps = number of partition boundariesc nblok = number of particle partitions.      dimension edges(idps,nblok)      dimension nyp(nblok), noff(nblok)      ks = kstrt - 2      at1 = float(ny)/float(nvp)      do 10 l = 1, nblok      kb = l + ks      edges(1,l) = at1*float(kb)      noff(l) = edges(1,l)      edges(2,l) = at1*float(kb + 1)      kr = edges(2,l)      nyp(l) = kr - noff(l)   10 continue      return      endc-----------------------------------------------------------------------      subroutine PCGUARD2(f,kstrt,nvp,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local datac     integer istatus, msid, ierr      integer ks, moff, kr, krr, kl, kll, ngc, j, lc     dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 30 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computers      do 10 j = 1, nxv      f(j,1,l) = f(j,kyp+1,kl)      f(j,kyp+2,l) = f(j,2,kr)      f(j,kyp+3,l) = f(j,ngc+1,krr)   10 continuec this segment is used for mpi computersc     call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+3,lgrp,msid,ierr)c     call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+3,lgrp,ierr)c     call MPI_WAIT(msid,istatus,ierr)c     call MPI_IRECV(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+4,lgrp,msid,iec    1rr)c     call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)c     call MPI_WAIT(msid,istatus,ierr)c     if (kyp.eq.1) thenc        call MPI_IRECV(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+6,lgrp,msic    1d,ierr)c        call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kll-1,moff+6,lgrp,ierr)c        call MPI_WAIT(msid,istatus,ierr)c     endif   30 continue      return      endc-----------------------------------------------------------------------      subroutine PCGUARD2L(f,kstrt,nvp,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes one extra guard cell.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c linear interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local datac     integer istatus, msid, ierr      integer ks, moff, kl, kr, l      integer jc     dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 20 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) then         kr = kr - nvp      endif      kl = l + ks      if (kl.lt.1) then         kl = kl + nvp      endifc this loop is used for shared memory computers      do 10 j = 1, nxv      f(j,kyp+1,l) = f(j,1,kr)   10 continuec this segment is used for mpi computersc     call MPI_IRECV(f(1,kyp+1,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)c     call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)c     call MPI_WAIT(msid,istatus,ierr)   20 continue      return      endc-----------------------------------------------------------------------      subroutine PAGUARD2(f,scr,kstrt,nvp,nxv,nypmx,kyp,kblok,ngds)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c scr(j,idps,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nxv, nypmx, kyp, kblok, ngds      dimension f(nxv,nypmx,kblok), scr(nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local datac     integer istatus, msid, ierr      integer ks, moff, kr, krr, kl, kll, ngc, j, lc     dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 40 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computers      do 10 j = 1, nxv      scr(j,1,l) = f(j,kyp+2,kl)      scr(j,2,l) = f(j,kyp+3,kll)      scr(j,3,l) = f(j,1,kr)   10 continuec this segment is used for mpi computersc     call MPI_IRECV(scr,ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)c     call MPI_SEND(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+1,lgrp,ierr)c     call MPI_WAIT(msid,istatus,ierr)c     call MPI_IRECV(scr(1,3,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)c     call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)c     call MPI_WAIT(msid,istatus,ierr)c     if (kyp.eq.1) thenc        call MPI_IRECV(scr(1,2,l),ngc*nxv,mreal,kll-1,moff+5,lgrp,msid,c    1ierr)c        call MPI_SEND(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+5,lgrp,ierrc    1)c        call MPI_WAIT(msid,istatus,ierr)c     endifc add up the guard cells      do 30 j = 1, nxv      f(j,2,l) = f(j,2,l) + scr(j,1,l)      f(j,ngc+1,l) = f(j,ngc+1,l) + scr(j,2,l)      f(j,kyp+1,l) = f(j,kyp+1,l) + scr(j,3,l)   30 continue   40 continue      return      endc-----------------------------------------------------------------------      subroutine PAGUARD2L(f,scr,kstrt,nvp,nxv,nypmx,kyp,kblok)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes one extra guard cell.c scr(j,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c linear interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok), scr(nxv,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local datac     integer istatus, msid, ierr      integer ks, moff, kl, kr, j, lc     dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 30 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) then         kr = kr - nvp      endif      kl = l + ks      if (kl.lt.1) then         kl = kl + nvp      endifc this segment is used for shared memory computers      do 10 j = 1, nxv      scr(j,l) = f(j,kyp+1,kl)   10 continuec this segment is used for mpi computersc     call MPI_IRECV(scr,nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)c     call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+1,lgrp,ierr)c     call MPI_WAIT(msid,istatus,ierr)      do 20 j = 1, nxv      f(j,1,l) = f(j,1,l) + scr(j,l)   20 continue   30 continue      return      endc-----------------------------------------------------------------------      subroutine PMOVE2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc ierr = (0,1) = (no,yes) error condition exists      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer nbsize, nterc     integer msid, istatus      real any, ytc     dimension msid(4), istatus(lstat)      dimension ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 50 l = 1, nblok      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 30 j = 1, npp(l)      yt = part(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = part(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,j,l)            sbufr(4,jsr(1,l),l) = part(4,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endifc particles going down      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = part(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,j,l)            sbufl(4,jsl(1,l),l) = part(4,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endif      endif   30 continue   40 jss(1,l) = jsl(1,l) + jsr(1,l)   50 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computers      jsl(2,l) = jsr(1,kl)      do 110 j = 1, jsl(2,l)      rbufl(1,j,l) = sbufr(1,j,kl)      rbufl(2,j,l) = sbufr(2,j,kl)      rbufl(3,j,l) = sbufr(3,j,kl)      rbufl(4,j,l) = sbufr(4,j,kl)  110 continue      jsr(2,l) = jsl(1,kr)      do 120 j = 1, jsr(2,l)      rbufr(1,j,l) = sbufl(1,j,kr)      rbufr(2,j,l) = sbufl(2,j,kr)      rbufr(3,j,l) = sbufl(3,j,kr)      rbufr(4,j,l) = sbufl(4,j,kr)  120 continuec this segment is used for mpi computersc post receivec     call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)c     call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particlesc     call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)c    1,ierr)c     call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),ic    1err)c wait for particles to arrivec     call MPI_WAIT(msid(1),istatus,ierr)c     call MPI_GET_COUNT(istatus,mreal,nps,ierr)c     jsl(2,l) = nps/idimpc     call MPI_WAIT(msid(2),istatus,ierr)c     call MPI_GET_COUNT(istatus,mreal,nps,ierr)c     jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (2,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (2,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sentc     call MPI_WAIT(msid(3),istatus,ierr)c     call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)c particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufr(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufr(3,j,l)         sbufr(4,jsr(1,l),l) = rbufr(4,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = rbufl(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = rbufl(3,j,l)            sbufr(4,jsr(1,l),l) = rbufl(4,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = rbufl(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = rbufl(3,j,l)            sbufl(4,jsl(1,l),l) = rbufl(4,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (2,*) 'Info: particles being passed further = ', ibflg(2)         go to 100      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after move      nps = 0      do 270 l = 1, nblok      nps = nps + npp(l)  270 continue      ibflg(2) = nps      ibflg(1) = npr      call PISUM(ibflg,iwork,2,1)      if (ibflg(1).ne.ibflg(2)) then         write (2,*) 'particle number error, old/new=',ibflg(1),ibflg(2)         ierr = 1      endifc information      if (nter.gt.0) then         write (2,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine PXMOV2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ier     2r)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc maskp = scratch array for particle addressesc ierr = (0,1) = (no,yes) error condition existsc optimized for vector processor      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, maskp, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok), maskp(npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer nbsize, nterc     integer msid, istatus      real any, ytc     dimension msid(4), istatus(lstat)      dimension ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 80 l = 1, nblok      jss(1,l) = 0      jss(2,l) = 0c find mask function for particles out of bounds      do 30 j = 1, npp(l)      yt = part(2,j,l)      if ((yt.ge.edges(2,l)).or.(yt.lt.edges(1,l))) then         jss(1,l) = jss(1,l) + 1         maskp(j,l) = 1      else         maskp(j,l) = 0      endif   30 continuec set flag if hole buffer would overflow      if (jss(1,l).gt.ntmax) then         jss(1,l) = ntmax         jss(2,l) = 1      endifc accumulate location of holes      do 40 j = 2, npp(l)      maskp(j,l) = maskp(j,l) + maskp(j-1,l)   40 continuec store addresses of particles out of bounds      do 50 j = 2, npp(l)      if ((maskp(j,l).gt.maskp(j-1,l)).and.(maskp(j,l).le.ntmax)) then         ihole(maskp(j,l),l) = j      endif   50 continue      if (maskp(1,l).gt.0) ihole(1,l) = 1      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0c load particle buffers      do 60 j = 1, jss(1,l)      yt = part(2,ihole(j,l),l)c particles going up      if (yt.ge.edges(2,l)) then         if ((kb+1).eq.nvp) yt = yt - any         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            sbufr(1,jsr(1,l),l) = part(1,ihole(j,l),l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,ihole(j,l),l)            sbufr(4,jsr(1,l),l) = part(4,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endifc particles going down      else         if (kb.eq.0) yt = yt + any         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            sbufl(1,jsl(1,l),l) = part(1,ihole(j,l),l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,ihole(j,l),l)            sbufl(4,jsl(1,l),l) = part(4,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endif      endif   60 continue   70 jss(1,l) = jsl(1,l) + jsr(1,l)   80 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computers      jsl(2,l) = jsr(1,kl)      do 110 j = 1, jsl(2,l)      rbufl(1,j,l) = sbufr(1,j,kl)      rbufl(2,j,l) = sbufr(2,j,kl)      rbufl(3,j,l) = sbufr(3,j,kl)      rbufl(4,j,l) = sbufr(4,j,kl)  110 continue      jsr(2,l) = jsl(1,kr)      do 120 j = 1, jsr(2,l)      rbufr(1,j,l) = sbufl(1,j,kr)      rbufr(2,j,l) = sbufl(2,j,kr)      rbufr(3,j,l) = sbufl(3,j,kr)      rbufr(4,j,l) = sbufl(4,j,kr)  120 continuec this segment is used for mpi computersc post receivec     call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)c     call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particlesc     call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)c    1,ierr)c     call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),ic    1err)c wait for particles to arrivec     call MPI_WAIT(msid(1),istatus,ierr)c     call MPI_GET_COUNT(istatus,mreal,nps,ierr)c     jsl(2,l) = nps/idimpc     call MPI_WAIT(msid(2),istatus,ierr)c     call MPI_GET_COUNT(istatus,mreal,nps,ierr)c     jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (2,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (2,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sentc     call MPI_WAIT(msid(3),istatus,ierr)c     call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)c particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufr(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufr(3,j,l)         sbufr(4,jsr(1,l),l) = rbufr(4,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = rbufl(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = rbufl(3,j,l)            sbufr(4,jsr(1,l),l) = rbufl(4,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = rbufl(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = rbufl(3,j,l)            sbufl(4,jsl(1,l),l) = rbufl(4,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (2,*) 'Info: particles being passed further = ', ibflg(2)         go to 100      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after move      nps = 0      do 270 l = 1, nblok      nps = nps + npp(l)  270 continue      ibflg(2) = nps      ibflg(1) = npr      call PISUM(ibflg,iwork,2,1)      if (ibflg(1).ne.ibflg(2)) then         write (2,*) 'particle number error, old/new=',ibflg(1),ibflg(2)         ierr = 1      endifc information      if (nter.gt.0) then         write (2,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine PTPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jb     1lok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(nxv,kypd,kblok), g(nyv,kxpd,jblok)      dimension s(kxp,kyp,kblok), t(kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kybc     integer jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, jc     integer ir0, is0, ii, ir, is, ierr, msid, istatusc     dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computers      if (kstrt.gt.nx) return      do 40 l = 1, jblok      joff = kxp*(l + ks)      do 30 i = 1, kyb      koff = kyp*(i - 1)      do 20 k = 1, kyp      do 10 j = 1, kxp      g(k+koff,j,l) = f(j+joff,k,i)   10 continue   20 continue   30 continue   40 continuec this segment is used for mpi computersc     jkblok = max0(jblok,kblok)c     kxym = min0(kxb,kyb)c     mtr = kyb/kxymc     ntr = kxb/kxymc     mntr = max0(mtr,ntr)c     do 70 l = 1, jkblokc     do 60 i = 1, kxymc     ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1c     is0 = ir0c     do 50 ii = 1, mntrc post receivec     if ((kstrt.le.nx).and.(ii.le.mtr)) thenc        ir = ir0 + kxym*(ii - 1)c        call MPI_IRECV(t(1,1,l),kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,msid,c    1ierr)c     endifc send datac     if ((kstrt.le.ny).and.(ii.le.ntr)) thenc        is = is0 + kxym*(ii - 1)c        joff = kxp*(is - 1)c        do 20 k = 1, kypc        do 10 j = 1, kxpc        s(j,k,l) = f(j+joff,k,l)c  10    continuec  20    continuec        call MPI_SEND(s(1,1,l),kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,ierrc    1)c     endifc receive datac     if ((kstrt.le.nx).and.(ii.le.mtr)) thenc        koff = kyp*(ir - 1)c        call MPI_WAIT(msid,istatus,ierr)c        do 40 k = 1, kypc        do 30 j = 1, kxpc        g(k+koff,j,l) = t(j,k,l)c  30    continuec  40    continuec     endifc  50 continuec  60 continuec  70 continue      return      endc-----------------------------------------------------------------------      subroutine P2TPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(2,nxv,kypd,kblok), g(2,nyv,kxpd,jblok)      dimension s(2,kxp,kyp,kblok), t(2,kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kybc     integer jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, jc     integer ir0, is0, ii, ir, is, ierr, msid, istatusc     dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computers      if (kstrt.gt.nx) return      do 40 l = 1, jblok      joff = kxp*(l + ks)      do 30 i = 1, kyb      koff = kyp*(i - 1)      do 20 k = 1, kyp      do 10 j = 1, kxp      g(1,k+koff,j,l) = f(1,j+joff,k,i)      g(2,k+koff,j,l) = f(2,j+joff,k,i)   10 continue   20 continue   30 continue   40 continuec this segment is used for mpi computersc     jkblok = max0(jblok,kblok)c     kxym = min0(kxb,kyb)c     mtr = kyb/kxymc     ntr = kxb/kxymc     mntr = max0(mtr,ntr)c     do 70 l = 1, jkblokc     do 60 i = 1, kxymc     ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1c     is0 = ir0c     do 50 ii = 1, mntrc post receivec     if ((kstrt.le.nx).and.(ii.le.mtr)) thenc        ir = ir0 + kxym*(ii - 1)c        call MPI_IRECV(t(1,1,1,l),2*kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,mc    1sid,ierr)c     endifc send datac     if ((kstrt.le.ny).and.(ii.le.ntr)) thenc        is = is0 + kxym*(ii - 1)c        joff = kxp*(is - 1)c        do 20 k = 1, kypc        do 10 j = 1, kxpc        s(1,j,k,l) = f(1,j+joff,k,l)c        s(2,j,k,l) = f(2,j+joff,k,l)c  10    continuec  20    continuec        call MPI_SEND(s(1,1,1,l),2*kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,c    1ierr)c     endifc receive datac     if ((kstrt.le.nx).and.(ii.le.mtr)) thenc        koff = kyp*(ir - 1)c        call MPI_WAIT(msid,istatus,ierr)c        do 40 k = 1, kypc        do 30 j = 1, kxpc        g(1,k+koff,j,l) = t(1,j,k,l)c        g(2,k+koff,j,l) = t(2,j,k,l)c  30    continuec  40    continuec     endifc  50 continuec  60 continuec  70 continue      return      endc-----------------------------------------------------------------------      subroutine PTPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblok     1,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(nxv*kypd*kblok), g(nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kyb, l, i, joff, koff, k, jc     integer jkblok, kxym, mtr, ntr, mntr, msidc     integer ir0, is0, ii, ir, is, ioff, ierr, istatusc     dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computers      if (kstrt.gt.nx) return      do 40 l = 1, jblok      joff = kxp*(l + ks)      do 30 i = 1, kyb      koff = kyp*(i - 1)      do 20 k = 1, kyp      do 10 j = 1, kxp      g(k+koff+nyv*(j-1+kxpd*(l-1))) = f(j+joff+nxv*(k-1+kypd*(i-1)))   10 continue   20 continue   30 continue   40 continuec this segment is used for mpi computersc     jkblok = max0(jblok,kblok)c     kxym = min0(kxb,kyb)c     mtr = kyb/kxymc     ntr = kxb/kxymc     mntr = max0(mtr,ntr)c transpose local datac     do 50 l = 1, jkblokc     ioff = kxb*(l - 1) - 1c     koff = kypd*(l - 1) - 1c     do 40 i = 1, kxymc     is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1c     do 30 ii = 1, ntrc     if (kstrt.le.ny) thenc        is = is0 + kxym*(ii - 1)c        joff = kxp*(is - 1)c        is = kyp*(is + ioff) - 1c        do 20 k = 1, kypc        do 10 j = 1, kxpc        g(j+kxp*(k+is)) = f(j+joff+nxv*(k+koff))c  10    continuec  20    continuec     endifc  30 continuec  40 continuec  50 continuec exchange datac     do 80 l = 1, jkblokc     ioff = kxb*(l - 1) - 1c     koff = kyb*(l - 1) - 1c     do 70 i = 1, kxymc     ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1c     is0 = ir0c     do 60 ii = 1, mntrc post receivec     if ((kstrt.le.nx).and.(ii.le.mtr)) thenc        ir = ir0 + kxym*(ii - 1)c        call MPI_IRECV(f(1+kxp*kyp*(ir+koff)),kxp*kyp,mcplx,ir-1,ir+kxyc    1m+1,lgrp,msid,ierr)c     endifc send datac     if ((kstrt.le.ny).and.(ii.le.ntr)) thenc        is = is0 + kxym*(ii - 1)c        call MPI_SEND(g(1+kxp*kyp*(is+ioff)),kxp*kyp,mcplx,is-1,l+ks+kxc    1ym+2,lgrp,ierr)c     endifc receive datac     if ((kstrt.le.nx).and.(ii.le.mtr)) thenc        call MPI_WAIT(msid,istatus,ierr)c     endifc  60 continuec  70 continuec  80 continuec transpose local datac     do 130 l = 1, jkblokc     ioff = kyb*(l - 1) - 1c     joff = kxpd*(l - 1) - 1c     do 120 i = 1, kxymc     ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1c     do 110 ii = 1, mtrc     if (kstrt.le.nx) thenc        ir = ir0 + kxym*(ii - 1)c        koff = kyp*(ir - 1)c        ir = kyp*(ir + ioff) - 1c        do 100 k = 1, kypc        do 90 j = 1, kxpc        g(k+koff+nyv*(j+joff)) = f(j+kxp*(k+ir))c  90    continuec 100    continuec     endifc 110 continuec 120 continuec 130 continue      return      endc-----------------------------------------------------------------------      subroutine P2TPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc msid, mrid = scratch arrays for identifying asynchronous messagesc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kxb/kyb = number of processors in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(2*nxv*kypd*kblok), g(2*nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kyb, l, i, joff, koff, k, jc     integer jkblok, kxym, mtr, ntr, mntr, msidc     integer ir0, is0, ii, ir, is, ioff, ierr, istatusc     dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computers      if (kstrt.gt.nx) return      do 40 l = 1, jblok      joff = kxp*(l + ks) - 1      do 30 i = 1, kyb      koff = kyp*(i - 1) - 1      do 20 k = 1, kyp      do 10 j = 1, kxp      g(1+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(1+2*(j+joff+nxv*(k-1+kypd     1*(i-1))))      g(2+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(2+2*(j+joff+nxv*(k-1+kypd     1*(i-1))))   10 continue   20 continue   30 continue   40 continuec this segment is used for mpi computersc     jkblok = max0(jblok,kblok)c     kxym = min0(kxb,kyb)c     mtr = kyb/kxymc     ntr = kxb/kxymc     mntr = max0(mtr,ntr)c transpose local datac     do 50 l = 1, jkblokc     ioff = kxb*(l - 1) - 1c     koff = kypd*(l - 1) - 1c     do 40 i = 1, kxymc     is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1c     do 30 ii = 1, ntrc     if (kstrt.le.ny) thenc        is = is0 + kxym*(ii - 1)c        joff = 2*kxp*(is - 1)c        is = kyp*(is + ioff) - 1c        do 20 k = 1, kypc        do 10 j = 1, 2*kxpc        g(j+2*kxp*(k+is)) = f(j+joff+2*nxv*(k+koff))c  10    continuec  20    continuec     endifc  30 continuec  40 continuec  50 continuec     do 80 l = 1, jkblokc     ioff = kxb*(l - 1) - 1c     koff = kyb*(l - 1) - 1c     do 70 i = 1, kxymc     ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1c     is0 = ir0c     do 60 ii = 1, mntrc post receivec     if ((kstrt.le.nx).and.(ii.le.mtr)) thenc        ir = ir0 + kxym*(ii - 1)c        call MPI_IRECV(f(1+2*kxp*kyp*(ir+koff)),2*kxp*kyp,mcplx,ir-1,irc    1+kxym+1,lgrp,msid,ierr)c     endifc send datac     if ((kstrt.le.ny).and.(ii.le.ntr)) thenc        is = is0 + kxym*(ii - 1)c        call MPI_SEND(g(1+2*kxp*kyp*(is+ioff)),2*kxp*kyp,mcplx,is-1,l+kc    1s+kxym+2,lgrp,ierr)c     endifc receive datac     if ((kstrt.le.nx).and.(ii.le.mtr)) thenc        call MPI_WAIT(msid,istatus,ierr)c     endifc  60 continuec  70 continuec  80 continuec transpose local datac     do 130 l = 1, jkblokc     ioff = kyb*(l - 1) - 1c     joff = kxpd*(l - 1) - 1c     do 120 i = 1, kxymc     ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1c     do 110 ii = 1, mtrc     if (kstrt.le.nx) thenc        ir = ir0 + kxym*(ii - 1)c        koff = kyp*(ir - 1)c        ir = kyp*(ir + ioff) - 1c        do 100 k = 1, kypc        do 90 j = 1, kxpc        g(2*(k+koff+nyv*(j+joff))-1) = f(2*(j+kxp*(k+ir))-1)c        g(2*(k+koff+nyv*(j+joff))) = f(2*(j+kxp*(k+ir)))c  90    continuec 100    continuec     endifc 110 continuec 120 continuec 130 continue      return      endc-----------------------------------------------------------------------      subroutine PTIMERA(icntrl,time,dtime)c this subroutine performs parallel wall clock timingc input: icntrl, dtimec icntrl = (-1,0,1) = (initialize,ignore,read) clockc clock should be initialized before it is read!c time = maximum/minimum elapsed time in secondsc dtime = current timec written for mpi      implicit none      integer icntrl      real time      double precision dtime      dimension time(2)c get definition of MPI constantsc     include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplxc lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local datac     integer ierrc     real nclockc     double precision jclockc initialize clockc     if (icntrl.eq.(-1)) thenc        call MPI_BARRIER(lgrp,ierr)c        dtime = MPI_WTIME()c read clock and write time difference from last clock initializationc     else if (icntrl.eq.1) thenc        jclock = dtimec        dtime = MPI_WTIME()c        nclock = real(dtime - jclock)c        call MPI_ALLREDUCE(nclock,time(2),1,mreal,MPI_MIN,lgrp,ierr)c        call MPI_ALLREDUCE(nclock,time(1),1,mreal,MPI_MAX,lgrp,ierr)c     endif      return      endc-----------------------------------------------------------------------      subroutine PSUM(f,g,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c f(j,k) = sum over k of f(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local datac     integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, jc     dimension istatus(lstat)c find processor idc this line is used for shared memory computers      idproc = 0c this line is used for mpi computersc     call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computers      do 20 j = 1, nxp      if (lb.eq.0) then         g(j,k) = f(j,kb+kxs)      else         g(j,k) = f(j,kb-kxs)      endif   20 continuec this segment is used for mpi computersc     if (lb.eq.0) thenc        call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)c        call MPI_SEND(f,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)c     elsec        call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)c        call MPI_SEND(f,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)c     endifc     call MPI_WAIT(msid,istatus,ierr)   30 continuec perform sumc     do 50 k = 1, nblokc     do 40 j = 1, nxpc     f(j,k) = f(j,k) + g(j,k)c  40 continuec  50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PISUM(if,ig,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c if(j,k) = sum over k of if(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c if = input and output integer datac ig = scratch integer arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      integer if, ig, nxp, nblok      dimension if(nxp,nblok), ig(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mint = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local datac     integer istatus, ierr, nsid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, jc     dimension istatus(lstat)c find processor idc this line is used for shared memory computers      idproc = 0c this line is used for mpi computersc     call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computers      do 20 j = 1, nxp      if (lb.eq.0) then         ig(j,k) = if(j,kb+kxs)      else         ig(j,k) = if(j,kb-kxs)      endif   20 continuec this segment is used for mpi computersc     if (lb.eq.0) thenc        call MPI_ISEND(if,nxp,mint,kb+kxs-1,l+nxp,lgrp,nsid,ierr)c        call MPI_RECV(ig,nxp,mint,kb+kxs-1,l+nxp,lgrp,istatus,ierr)c     elsec        call MPI_ISEND(if,nxp,mint,kb-kxs-1,l+nxp,lgrp,nsid,ierr)c        call MPI_RECV(ig,nxp,mint,kb-kxs-1,l+nxp,lgrp,istatus,ierr)c     endifc     call MPI_WAIT(nsid,istatus,ierr)   30 continuec perform sumc     do 50 k = 1, nblokc     do 40 j = 1, nxpc     if(j,k) = if(j,k) + ig(j,k)c  40 continuec  50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      end