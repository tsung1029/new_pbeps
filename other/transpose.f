      program ptransposec indnvp = exponent which determines number of virtual processorsc mshare = (0,1) = (no,yes) architecture is shared memory      parameter( indx =   7, indy =   8, indnvp =   1, mshare =   0)      parameter(nx=2**indx,ny=2**indy)      parameter(nxh=nx/2)      parameter(nvp=2**indnvp,kyp=(ny-1)/nvp+1,kxp=(nxh-1)/nvp+1)      parameter(kblok=1+mshare*(ny/kyp-1),jblok=1+mshare*(nxh/kxp-1))      parameter(nxv=nx+2,nyv=ny+2,nxvh=nxv/2)      double precision ranorm      complex sbuff, bbuff      dimension g(nxv,ny), f(nxv,kyp,kblok), t(2*nyv,kxp,jblok)      dimension sbuff(kxp,kyp,kblok), bbuff(kxp,kyp,jblok)      open(unit=6,file='trans.output',form='formatted',status='unknown')c initialize for parallel processing      call ppinit(idproc,nvp)      kstrt = idproc + 1      ks = kstrt - 2c create test data, save global result in g      do 30 k = 1, ny      kk = (k - 1)/kyp      k1 = k - kyp*kk      do 20 j = 1, nx      g(j,k) = ranorm(d)      do 10 l = 1, kblok      if (kk.eq.(l+ks)) f(j,k1,l) = g(j,k)   10 continue   20 continue   30 continue      call timera(-1,'total   ',time)      call ptpose(f,t,sbuff,bbuff,nxh,ny,kstrt,nxvh,nyv,kxp,kyp,jblok,kb     1lok)      call ptpose(t,f,bbuff,sbuff,ny,nxh,kstrt,nyv,nxvh,kyp,kxp,kblok,jb     1lok)      call timera(1,'total   ',time)      sum1 = 0.      epsmax = 0.      if (kstrt.gt.ny) go to 80      do 70 l = 1, kblok      koff = kyp*(l + ks)      do 60 k = 1, kyp      k1 = k + koff      do 50 j = 1, nx      eps = abs(f(j,k,l) - g(j,k1))      if (eps.gt.epsmax) then         write (6,*) j,k1,f(j,k,l)         write (6,*) j,k1,g(j,k1)         epsmax = eps      endif      sum1 = sum1 + eps   50 continue   60 continue   70 continue   80 continue      call psum (sum1,at1,1,1)      write (6,*) 'global error=',sum1      call ppexit      stop      endc-----------------------------------------------------------------------      subroutine ppinit(idproc,nvp)c this subroutine initializes parallel processingc input: nvp, output: idprocc idproc = processor idc nvp = number of real or virtual processors requested      implicit none      integer idproc, nvpc get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for realsc mint = default datatype for integersc mcplx = default datatype for complex type      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ierror, ndprec      save /pparms/c ndprec = (0,1) = (no,yes) use (normal,autodouble) precision      data ndprec /0/c this segment is used for shared memory computersc     nproc = nvpc     idproc = 0c this segment is used for mpi computers      if (MPI_STATUS_SIZE.gt.lstat) then         write (2,*) ' status size too small, actual/required = ', lstat     1, MPI_STATUS_SIZE         stop      endifc initialize the MPI execution environment      call MPI_INIT(ierror)      if (ierror.ne.0) stop      lgrp = MPI_COMM_WORLDc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lgrp,idproc,ierror)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lgrp,nproc,ierror)c set default datatypes         mint = MPI_INTEGERc single precision      if (ndprec.eq.0) then         mreal = MPI_REAL         mcplx = MPI_COMPLEXc double precision      else         mreal = MPI_DOUBLE_PRECISION         mcplx = MPI_DOUBLE_COMPLEX      endifc requested number of processors not obtained      if (nproc.ne.nvp) then         write (2,*) ' processor number error: nvp, nproc=', nvp, nproc         call ppexit         stop      endif      return      endc-----------------------------------------------------------------------      subroutine ppexitc this subroutine terminates parallel processing      implicit nonec common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplxc lgrp = current communicator      common /pparms/ nproc, lgrp, mreal, mint, mcplx      integer ierrorc synchronize processes      call MPI_BARRIER(lgrp,ierror)c terminate MPI execution environment      call MPI_FINALIZE(ierror)      return      endc-----------------------------------------------------------------------      subroutine ptpose(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,jblok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, jblok, kblok      complex f, g, s, t      dimension f(nxv,kyp,kblok), g(nyv,kxp,jblok)      dimension s(kxp,kyp,kblok), t(kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kyb, jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff,j,l) = f(j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,l),kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,msid,     1ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(j,k,l) = f(j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,l),kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,ierr     1)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(k+koff,j,l) = t(j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      function ranorm(d)c this program calculates a random number y from a gaussian distributionc with zero mean and unit variance, according to the method ofc mueller and box:c    y(k) = (-2*ln(x(k)))**1/2*sin(2*pi*x(k+1))c    y(k+1) = (-2*ln(x(k)))**1/2*cos(2*pi*x(k+1)),c where x is a random number uniformly distributed on (0,1).c written for the ibm by viktor k. decyk, ucla      integer r1,r2,r4,r5      double precision ranorm,h1l,h1u,h2l,r0,r3,asc,bsc,temp      save iflg,r1,r2,r4,r5,h1l,h1u,h2l,r0      data r1,r2,r4,r5 /885098780,1824280461,1396483093,55318673/      data h1l,h1u,h2l /65531.0d0,32767.0d0,65525.0d0/      data iflg,r0 /0,0.0d0/      if (iflg.eq.0) go to 10      ranorm = r0      r0 = 0.0d0      iflg = 0      return   10 isc = 65536      asc = dfloat(isc)      bsc = asc*asc      i1 = r1 - (r1/isc)*isc      r3 = h1l*dfloat(r1) + asc*h1u*dfloat(i1)      i1 = r3/bsc      r3 = r3 - dfloat(i1)*bsc      bsc = 0.5d0*bsc      i1 = r2/isc      isc = r2 - i1*isc      r0 = h1l*dfloat(r2) + asc*h1u*dfloat(isc)      asc = 1.0d0/bsc      isc = r0*asc      r2 = r0 - dfloat(isc)*bsc      r3 = r3 + (dfloat(isc) + 2.0d0*h1u*dfloat(i1))      isc = r3*asc      r1 = r3 - dfloat(isc)*bsc      temp = dsqrt(-2.0d0*dlog((dfloat(r1) + dfloat(r2)*asc)*asc))      isc = 65536      asc = dfloat(isc)      bsc = asc*asc      i1 = r4 - (r4/isc)*isc      r3 = h2l*dfloat(r4) + asc*h1u*dfloat(i1)      i1 = r3/bsc      r3 = r3 - dfloat(i1)*bsc      bsc = 0.5d0*bsc      i1 = r5/isc      isc = r5 - i1*isc      r0 = h2l*dfloat(r5) + asc*h1u*dfloat(isc)      asc = 1.0d0/bsc      isc = r0*asc      r5 = r0 - dfloat(isc)*bsc      r3 = r3 + (dfloat(isc) + 2.0d0*h1u*dfloat(i1))      isc = r3*asc      r4 = r3 - dfloat(isc)*bsc      r0 = 6.28318530717959d0*((dfloat(r4) + dfloat(r5)*asc)*asc)      ranorm = temp*dsin(r0)      r0 = temp*dcos(r0)      iflg = 1      return      endc-----------------------------------------------------------------------      subroutine timera(icntrl,chr,time)c this subroutine performs timingc input: icntrl, chrc icntrl = (-1,0,1) = (initialize,ignore,read) clockc clock should be initialized before it is read!c chr = character variable for labeling timingsc time = elapsed time in secondsc written for mpi      implicit none      integer icntrl      character*8 chr      real timec get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplxc lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer idproc, ierr      real nclock, mclock      double precision jclock      save jclock   91 format (1x,a8,1x,'max/min real time = ',e14.7,1x,e14.7,1x,'sec')      data jclock /0.0d0/      if (icntrl.eq.0) return      if (icntrl.eq.1) go to 10c initialize clock      call MPI_BARRIER(lgrp,ierr)      jclock = MPI_WTIME()      returnc read clock and write time difference from last clock initialization   10 nclock = real(MPI_WTIME() - jclock)      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MIN,lgrp,ierr)      mclock = time      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MAX,lgrp,ierr)      call MPI_COMM_RANK(lgrp,idproc,ierr)      if (idproc.eq.0) write (6,91) chr, time, mclock      return      endc-----------------------------------------------------------------------      subroutine psum (f,g,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c f(j,k) = sum over k of f(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer istatus      integer idproc, ierr, kstrt, ks, l, kxs, k, kb, lb, msid, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = f(j,kb+kxs)c     elsec        g(j,k) = f(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      f(j,k) = f(j,k) + g(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      end