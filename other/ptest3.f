c-----------------------------------------------------------------------      program ptest3      implicit none      integer indx, indy, indnvp, mshare, idps, nx, ny, nvp, kyp, kblok      integer nblok, nxv, nyv, nypm, nypmxc indnvp = exponent which determines number of virtual processorsc mshare = (0,1) = (no,yes) architecture is shared memoryc     parameter( indx =   3, indy =   4, indnvp =   2, mshare =   0)c     parameter( indx =   3, indy =   3, indnvp =   2, mshare =   0)      parameter( indx =   3, indy =   4, indnvp =   3, mshare =   0)      parameter(idps=2)      parameter(nx=2**indx,ny=2**indy)      parameter(nvp=2**indnvp,kyp=(ny-1)/nvp+1)      parameter(kblok=1+mshare*(ny/kyp-1),nblok=1+mshare*(nvp-1))      parameter(nxv=nx+2,nyv=ny+2,nypmx=ny+3)      parameter(nypm=9)      integer idproc, kstrt, ks, j, k, l, kk, k1, koff, isign, nter      integer ierr      integer nyp, noff, nyps, noffs, nypd, noffd, jsl, jsr, npp      integer noffg, nypg      integer inorder, npav, nypmin, nypmax      real epsmax, eps      real f, g, h, t      double precision ranorm      dimension f(nxv,nypmx,kblok), g(nxv,nypmx,kblok), h(nxv,nyv)      dimension t(nxv,nypmx,kblok)      dimension nyp(nblok), noff(nblok)      dimension nyps(nblok), noffs(nblok), nypd(nblok), noffd(nblok)      dimension jsl(idps,nblok), jsr(idps,nblok)      dimension nypg(nvp), noffg(nvp), npp(nblok)c      real edges, eg, es, et, edg, eds      integer npic      dimension edges(idps,nblok)      dimension eg(idps,nblok), es(idps,nblok), et(3*idps,nblok)      dimension edg(nypm,nblok), eds(nypm,nblok)      dimension npic(nypm,nblok)cc     data noffg /0,2,9,10/, nypg /2,7,1,6/c     data noffg /0,2,8,10/, nypg /2,6,2,6/c     data noffg /0,5,6,7/, nypg /5,1,1,1/c     data noffg /0,1,2,3/, nypg /1,1,1,5/      data noffg /0,9,10,11,12,13,14,15/, nypg /9,1,1,1,1,1,1,1/c     data noffg /0,1,2,3,4,5,6,7/, nypg /1,1,1,1,1,1,1,9/cc initialize for parallel processing      call PPINIT(idproc,nvp)      kstrt = idproc + 1      ks = kstrt - 2c prepare partition      do 5 l = 1, kblok      noff(l) = noffg((l+ks)+1)      nyp(l) = nypg((l+ks)+1)    5 continuec create test data for uniform partition      do 30 k = 1, ny      kk = (k - 1)/kyp      k1 = k - kyp*kk      do 20 j = 1, nx      h(j,k) = ranorm()      do 10 l = 1, kblok      if (kk.eq.(l+ks)) g(j,k1,l) = h(j,k)   10 continue   20 continue   30 continuec create data test for non-uniform partition      do 60 l = 1, kblok      do 50 k = 1, nyp(l)      kk = k + noff(l)      npic(k,l) = 2      do 40 j = 1, nx      f(j,k,l) = h(j,kk)   40 continue   50 continue   60 continuec create test data  for scan      npav = 0      inorder = 2      do 170 l = 1, nvp      npav = npav + 2*nypg(l)  170 continue      npav = npav/nvp      write (71,*) 'npav = ', npav      do 180 l = 1, kblok      npp(l) = 2*nyp(l)  180 continuec move field datac     call PSCAN(edges,eg,es,idps,nblok)c     call REPART2(edges,eg,es,et,npp,noff,nyp,npav,nypmin,nypmax,kstrt,c    1nvp,nblok,idps,inorder)c-----------------------------------------------------------------------      call REPARTD2(edges,edg,eds,eg,es,et,npic,noff,nyp,npav,nypmin,nyp     1max,kstrt,nvp,nblok,idps,nypm,inorder)      write (71,*) 'nypmin,nypmax=',nypmin,nypmaxc check data  for scan      do 190 l = 1, kblok      write (71,*) 'after edges=', edges(1,l), edges(2,l)  190 continue      if (kstrt.ge.0) then         call PPEXIT         stop      endifcc-----------------------------------------------------------------------      write (71,*) 'calling first PFMOVE2'      isign = -1      nter = 0      call PFMOVE2(f,t,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsl,isign,kyp,     1kstrt,nvp,nxv,nypmx,nblok,idps,nter,ierr)      write (71,*) 'nter,ierr=',nter,ierr      write (71,*) 'noffs,nyps=',noffs(1),nyps(1)      write (71,*) 'noffd,nypd=',noffd(1),nypd(1)      epsmax = 0.      do 90 l = 1, kblok      koff = kyp*(l + ks)      do 80 k = 1, kyp      kk = k + koff      do 70 j = 1, nx      eps = abs(f(j,k,l) - g(j,k,l))      if (eps.gt.epsmax) then         write (71,*) j,k,f(j,k,l),g(j,k,l),eps         epsmax = eps      endif   70 continue   80 continue   90 continue      write (71,*) 'local epsmax=',epsmax      call PSUM(epsmax,eps,1,1)      write (71,*) 'global epsmax=',epsmaxcc-----------------------------------------------------------------------c     write (71,*) 'calling second PFMOVE2'c     isign = 1c     nter = 0c     call PFMOVE2(g,t,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsl,isign,kyp,c    1kstrt,nvp,nxv,nypmx,nblok,idps,nter,ierr)c     write (71,*) 'nter,ierr=',nter,ierrc     write (71,*) 'noffd,nypd=',noffd(1),nypd(1)c     epsmax = 0.c     do 120 l = 1, kblokc     do 110 k = 1, nyp(l)c     kk = k + noff(l)c     do 100 j = 1, nxc     eps = abs(g(j,k,l) - f(j,k,l))c     if (eps.gt.epsmax) thenc        write (71,*) j,k,g(j,k,l),f(j,k,l),epsc        epsmax = epsc     endifc 100 continuec 110 continuec 120 continuec     write (71,*) 'local epsmax=',epsmaxc     call PSUM(epsmax,eps,1,1)c     write (71,*) 'global epsmax=',epsmaxc      call PPEXIT      stop      endc-----------------------------------------------------------------------      function ranorm()c this program calculates a random number y from a gaussian distributionc with zero mean and unit variance, according to the method ofc mueller and box:c    y(k) = (-2*ln(x(k)))**1/2*sin(2*pi*x(k+1))c    y(k+1) = (-2*ln(x(k)))**1/2*cos(2*pi*x(k+1)),c where x is a random number uniformly distributed on (0,1).c written for the ibm by viktor k. decyk, ucla      integer r1,r2,r4,r5      double precision ranorm,h1l,h1u,h2l,r0,r3,asc,bsc,temp      save iflg,r1,r2,r4,r5,h1l,h1u,h2l,r0      data r1,r2,r4,r5 /885098780,1824280461,1396483093,55318673/      data h1l,h1u,h2l /65531.0d0,32767.0d0,65525.0d0/      data iflg,r0 /0,0.0d0/      if (iflg.eq.0) go to 10      ranorm = r0      r0 = 0.0d0      iflg = 0      return   10 isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r1 - (r1/isc)*isc      r3 = h1l*dble(r1) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r2/isc      isc = r2 - i1*isc      r0 = h1l*dble(r2) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r2 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r1 = r3 - dble(isc)*bsc      temp = dsqrt(-2.0d0*dlog((dble(r1) + dble(r2)*asc)*asc))      isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r4 - (r4/isc)*isc      r3 = h2l*dble(r4) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r5/isc      isc = r5 - i1*isc      r0 = h2l*dble(r5) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r5 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r4 = r3 - dble(isc)*bsc      r0 = 6.28318530717959d0*((dble(r4) + dble(r5)*asc)*asc)      ranorm = temp*dsin(r0)      r0 = temp*dcos(r0)      iflg = 1      return      endc-----------------------------------------------------------------------      subroutine PPINIT(idproc,nvp)c this subroutine initializes parallel processingc input: nvp, output: idprocc idproc = processor idc nvp = number of real or virtual processors requested      implicit none      integer idproc, nvpc get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for realsc mint = default datatype for integersc mcplx = default datatype for complex type      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ierror, ndprec      save /PPARMS/c ndprec = (0,1) = (no,yes) use (normal,autodouble) precision      data ndprec /1/c this segment is used for shared memory computersc     nproc = nvpc     idproc = 0c this segment is used for mpi computers      if (MPI_STATUS_SIZE.gt.lstat) then         write (2,*) ' status size too small, actual/required = ', lstat     1, MPI_STATUS_SIZE         stop      endifc initialize the MPI execution environment      call MPI_INIT(ierror)      if (ierror.ne.0) stop      lgrp = MPI_COMM_WORLDc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lgrp,idproc,ierror)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lgrp,nproc,ierror)c set default datatypes         mint = MPI_INTEGERc single precision      if (ndprec.eq.0) then         mreal = MPI_REAL         mcplx = MPI_COMPLEXc double precision      else         mreal = MPI_DOUBLE_PRECISION         mcplx = MPI_DOUBLE_COMPLEX      endifc requested number of processors not obtained      if (nproc.ne.nvp) then         write (2,*) ' processor number error: nvp, nproc=', nvp, nproc         call PPEXIT         stop      endif      return      endc-----------------------------------------------------------------------      subroutine PPEXITc this subroutine terminates parallel processing      implicit nonec common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc lgrp = current communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworld      integer ierrorc synchronize processes      call MPI_BARRIER(lgrp,ierror)c terminate MPI execution environment      call MPI_FINALIZE(ierror)      return      endc-----------------------------------------------------------------------      subroutine PSUM(f,g,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c f(j,k) = sum over k of f(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = f(j,kb+kxs)c     elsec        g(j,k) = f(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      f(j,k) = f(j,k) + g(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PIMAX(if,ig,nxp,nblok)c this subroutine finds parallel maximum for each element of a vectorc that is, if(j,k) = maximum as a function of k of if(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c if = input and output integer datac ig = scratch integer arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      integer if, ig      integer nxp, nblok      dimension if(nxp,nblok), ig(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        ig(j,k) = if(j,kb+kxs)c     elsec        ig(j,k) = if(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(ig,nxp,mint,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(if,nxp,mint,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(ig,nxp,mint,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(if,nxp,mint,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec find maximum      do 50 k = 1, nblok      do 40 j = 1, nxp      if(j,k) = max0(if(j,k),ig(j,k))   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PFMOVE2(f,g,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsl,isig     1n,kyp,kstrt,nvp,nxv,nypmx,nblok,idps,mter,ierr)c this subroutine moves fields into appropriate spatial regions,c between non-uniform and uniform partitionsc f(j,k,l) = real data for grid j,k in field partition l.c the grid is non-uniform and includes extra guard cells.c g(j,k,l) = scratch data for grid j,k in field partition l.c noff(l) = lowermost global gridpoint in field partition lc nyp(l) = number of primary gridpoints in field partition lc noffs(l)/nyps(l) = source or scratch arrays for field partition lc noffd(l)/nypd(l) = source scratch arrays for field partition lc jsl(idps,l) = number of particles going down in field partition lc jsr(idps,l) = number of particles going up in field partition lc isign = -1, move from non-uniform (noff/nyp) to uniform (kyp) fieldsc isign = 1, move from uniform (kyp) to non-uniform (noff/nyp) fieldsc if isign = 0, the noffs/nyps contains the source partition, noffd/nypdc    contains the destination partition, and noff/nyp, kyp are not used.c    the source partition noffs/nyps is modified.c kyp = number of complex grids in each uniform field partition.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c nblok = number of field partitions.c idps = number of partition boundariesc mter = number of shifts requiredc if mter = 0, then number of shifts is determined and returnedc ierr = (0,1) = (no,yes) error condition exists      implicit none      real f, g      integer noff, nyp, noffs, nyps, noffd, nypd, jsr, jsl      integer isign, kyp, kstrt, nvp, nxv, nypmx, nblok, idps, mter      integer ierr      dimension f(nxv,nypmx,nblok), g(nxv,nypmx,nblok)      dimension noff(nblok), nyp(nblok)      dimension noffs(nblok), nyps(nblok), noffd(nblok), nypd(nblok)      dimension jsl(idps,nblok), jsr(idps,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer j, k, l      integer nbsize, ks, iter, npr, nps, nter, koff, kl, kr, kk      integer msid, istatus      integer ibflg, iwork      dimension istatus(lstat)      dimension ibflg(2), iwork(2)c exit if certain flags are set      if ((isign.eq.0).or.(mter.lt.0)) return      ks = kstrt - 2      nbsize = nxv*nypmx      iter = 2      ierr = 0c move from non-uniform to uniform fields      if (isign.lt.0) thenc copy non-uniform partition parameters         do 10 l = 1, nblok         noffs(l) = noff(l)         nyps(l) = nyp(l)         koff = kyp*(l + ks)         noffd(l) = koff         nypd(l) = kyp   10    continuec move from uniform to non-uniform fields      else if (isign.gt.0) thenc set uniform partition parameters         do 20 l = 1, nblok         koff = kyp*(l + ks)         noffs(l) = koff         nyps(l) = kyp         noffd(l) = noff(l)         nypd(l) = nyp(l)   20    continuec exit without processing      else         return      endifc determine number of outgoing grids   30 do 50 l = 1, nblok      kl = noffd(l)      kr = kl + nypd(l)      jsl(1,l) = 0      jsr(1,l) = 0      do 40 k = 1, nyps(l)      kk = k + noffs(l)c fields going up      if (kk.gt.kr) then         jsr(1,l) = jsr(1,l) + 1c fields going down      else if (kk.le.kl) then         jsl(1,l) = jsl(1,l) + 1      endif   40 continue   50 continuec copy fields      iter = iter + 2      npr = 0      nter = 0c get fields from below      do 80 l = 1, nblok      kr = l + ks + 2      kl = l + ks      jsl(2,l) = 0      jsr(2,l) = 0c this segment is used for shared memory computersc     if (noffs(l).gt.noffd(l)) then     c        jsl(2,l) = jsr(1,kl)c        do 70 k = 1, jsl(2,l)c        do 60 j = 1, nxvc        g(j,k,l) = f(j,k+nyps(kl)-jsr(1,kl),kl)c  60    continuec  70    continuec     endifc this segment is used for mpi computersc post receive from left      if (noffs(l).gt.noffd(l)) then               call MPI_IRECV(g,nbsize,mreal,kl-1,iter-1,lgrp,msid,ierr)      endifc send fields to right      if (jsr(1,l).gt.0) then         call MPI_SEND(f(1,nyps(l)-jsr(1,l)+1,l),nxv*jsr(1,l),mreal,kr-1     1,iter-1,lgrp,ierr)      endifc wait for fields to arrive      if (noffs(l).gt.noffd(l)) then          call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nps,ierr)         jsl(2,l) = nps/nxv      endif   80 continuec adjust field      do 150 l = 1, nblokc adjust field size      nyps(l) = nyps(l) - jsr(1,l)c do not allow move to overflow field array      jsr(1,l) = max0((nyps(l)+jsl(2,l)-nypmx),0)      nyps(l) = nyps(l) - jsr(1,l)      if (jsr(1,l).gt.0) then         npr = max0(npr,jsr(1,l))c save whatever is possible into end of g         kk = min0(jsr(1,l),nypmx-jsl(2,l))         do 100 k = 1, kk         do  90 j = 1, nxv         g(j,nypmx-kk+k,l) = f(j,nyps(l)+k,l)   90    continue  100    continue      endifc shift data which is staying, if necessary      if ((nyps(l).gt.0).and.(jsl(2,l).gt.0)) then         do 120 k = 1, nyps(l)         kk = nyps(l) - k + 1         do 110 j = 1, nxv         f(j,kk+jsl(2,l),l) = f(j,kk,l)  110    continue  120    continue      endifc insert data coming from left      do 140 k = 1, jsl(2,l)      do 130 j = 1, nxv      f(j,k,l) = g(j,k,l)  130 continue  140 continuec adjust field size and offset      nyps(l) = nyps(l) + jsl(2,l)      noffs(l) = noffs(l) - jsl(2,l)  150 continuec get fields from above      do 180 l = 1, nblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if ((noffs(l)+nyps(l)).lt.(noffd(l)+nypd(l))) then c        jsr(2,l) = jsl(1,kr)c        do 170 k = 1, jsr(2,l)c        do 160 j = 1, nxvc        g(j,k,l) =  f(j,k,kr)c 160    continuec 170    continuec     endifc this segment is used for mpi computersc post receive from right      if ((noffs(l)+nyps(l)).lt.(noffd(l)+nypd(l))) then              call MPI_IRECV(g,nbsize,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send fields to left      if (jsl(1,l).gt.0) then         call MPI_SEND(f(1,1,l),nxv*jsl(1,l),mreal,kl-1,iter,lgrp,ierr)      endifc wait for fields to arrive      if ((noffs(l)+nyps(l)).lt.(noffd(l)+nypd(l))) then           call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nps,ierr)         jsr(2,l) = nps/nxv      endif  180 continuec adjust field      do 240 l = 1, nblokc adjust field size      nyps(l) = nyps(l) - jsl(1,l)      noffs(l) = noffs(l) + jsl(1,l)c shift data which is staying, if necessary      if ((nyps(l).gt.0).and.(jsl(1,l).gt.0)) then         do 200 k = 1, nyps(l)         do 190 j = 1, nxv         f(j,k,l) = f(j,k+jsl(1,l),l)  190    continue  200    continue      endifc do not allow move to overflow field array      jsl(1,l) = max0((nyps(l)+jsr(2,l)-nypmx),0)      if (jsl(1,l).gt.0) then         npr = max0(npr,jsl(1,l))         jsr(2,l) = jsr(2,l) - jsl(1,l)c do not process if prior error      else if (jsr(1,l).gt.0) then         go to 230      endifc insert data coming from right      do 220 k = 1, jsr(2,l)      do 210 j = 1, nxv      f(j,k+nyps(l),l) = g(j,k,l)  210 continue  220 continuec adjust field size and offset      nyps(l) = nyps(l) + jsr(2,l)c check if new partition is uniform  230 nter = nter + abs(nyps(l)-nypd(l)) + abs(noffs(l)-noffd(l))  240 continuec calculate number of iterations      nps = iter/2 - 1      if (nps.le.mter) thenc process errors         if (npr.ne.0) then            ierr = npr            write (2,*) 'local field overflow error, ierr = ', ierr            return         endif         if (nps.lt.mter) go to 30         return      endifc process errors      ibflg(1) = npr      ibflg(2) = nter      call PIMAX(ibflg,iwork,2,1)c field overflow error      if (ibflg(1).ne.0) then         ierr = ibflg(1)         write (2,*) 'global field overflow error, ierr = ', ierr         return      endifc check if any fields have to be passed further      if (ibflg(2).gt.0) then         write (2,*) 'Info: fields being passed further = ', ibflg(2)         go to 30      endif      mter = nps      return      endc-----------------------------------------------------------------------      subroutine PSCAN(f,g,s,nxp,nblok)c this subroutine performs a parallel prefix reduction of a vector,c that is: f(j,k) = sum over k of f(j,k), where the sum is over k valuesc less than idproc.c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g, s = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g, s      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok), s(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c initialize global sum      do 20 k = 1, nblok      do 10 j = 1, nxp      s(j,k) = f(j,k)   10 continue   20 continuec main iteration loop   30 if (kxs.ge.nproc) go to 90c shift data      do 60 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 40 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = s(j,kb+kxs)c     elsec        g(j,k) = s(j,kb-kxs)c     endifc  40 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(s,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(s,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)c perform prefix scan      if (lb.ne.0) then         do 50 j = 1, nxp         f(j,k) = f(j,k) + g(j,k)   50    continue      endif   60 continuec perform sum      do 80 k = 1, nblok      do 70 j = 1, nxp      s(j,k) = s(j,k) + g(j,k)   70 continue   80 continue      l = l + 1      kxs = kxs + kxs      go to 30   90 return      endc-----------------------------------------------------------------------      subroutine REPART2(edges,eg,es,et3,npp,noff,nyp,npav,nypmin,nypmax     1,kstrt,nvp,nblok,idps,inorder)c this subroutines finds new partitions boundaries (edges, noff, nyp)c from old partition information (npp, nyp).c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc eg/es/et3 = scratch arraysc npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition lc nyp(l) = number of primary gridpoints in particle partition lc npav = average number of particles per partition desiredc nypmin = minimum value of nyp in new partitionc nypmax = maximum value of nyp plus guard cells in new partitionc kstrt = starting data block numberc nvp = number of real or virtual processorsc nblok = number of field partitions.c idps = number of partition boundariesc inorder = (1,2) (linear,quadratic) interpolation used      implicit none      real edges, eg, es, et3      integer npp, noff, nyp      integer npav, nypmin, nypmax, kstrt, nvp, nblok, idps, inorder      dimension edges(idps,nblok)      dimension eg(idps,nblok), es(idps,nblok), et3(3*idps,nblok)      dimension npp(nblok), noff(nblok), nyp(nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, iter, nter, kl, kr, l, ierr      real anpav, apav, anpl, anpr      integer msid, istatus      integer ibflg, iwork      dimension istatus(lstat)      dimension ibflg(2), iwork(2)c exit if flag is set      ks = kstrt - 2      iter = 2      anpav = real(npav)c copy number of particles and grid in current partition      do 10 l = 1, nblok      edges(1,l) = npp(l)      edges(2,l) = nyp(l)      et3(1,l) = edges(1,l)      et3(2,l) = edges(2,l)   10 continuec perform running sum      call PSCAN(edges,eg,es,idps,nblok)      do 20 l = 1, nblok      es(1,l) = et3(1,l)      es(2,l) = et3(2,l)      et3(1,l) = edges(1,l)      et3(2,l) = edges(2,l)      et3(3,l) = et3(1,l)      et3(4,l) = et3(2,l)      et3(5,l) = es(1,l)      et3(6,l) = es(2,l)      edges(2,l) = 1.0   20 continuec move partitions   30 iter = iter + 2c get partition from left      do 40 l = 1, nblok      kr = l + ks + 2      kl = l + ksc apav = desired number of particles on processor to left      apav = real(kl)*anpavc anpl = deficit of particles on processor to left      anpl = apav - et3(1,l) + es(1,l)c anpr = excess of particles on current processor      anpr = et3(1,l) - apav - anpavc this segment is used for shared memory computersc     if (anpl.lt.0.) thenc        eg(1,l) = es(1,kl)c        eg(2,l) = es(2,kl)c     endifc this segment is used for mpi computersc post receive from left      if (anpl.lt.0.) then         call MPI_IRECV(eg,idps,mreal,kl-1,iter-1,lgrp,msid,ierr)      endifc send partition to right      if (anpr.gt.0.) then         call MPI_SEND(es,idps,mreal,kr-1,iter-1,lgrp,ierr)      endifc wait for partition to arrive      if (anpl.lt.0.) call MPI_WAIT(msid,istatus,ierr)   40 continuec find new partitions      nter = 0      do 50 l = 1, nblok      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et3(1,l) + es(1,l)      anpr = et3(1,l) - apav - anpavc left boundary is on the left      if (anpl.lt.0.) then         if ((anpl+eg(1,l)).ge.0.) then            edges(1,l) = (et3(2,l) - es(2,l)) + anpl*eg(2,l)/eg(1,l)c left boundary is even further to left         else            nter = nter + 1         endifc left boundary is inside      else if (et3(1,l).ge.apav) then         edges(1,l) = (et3(2,l) - es(2,l)) + anpl*es(2,l)/es(1,l)      endifc right going data will need to be sent      if (anpr.gt.es(1,l)) nter = nter + 1      if (kl.gt.0) then         et3(1,l) = et3(1,l) - es(1,l)         et3(2,l) = et3(2,l) - es(2,l)         es(1,l) = eg(1,l)         es(2,l) = eg(2,l)      endif   50 continuec get more data from left      if (nter.gt.0) go to 30      iter = nvp + 2c restore partition data      do 60 l = 1, nblok      et3(1,l) = et3(3,l)      et3(2,l) = et3(4,l)      es(1,l) = et3(5,l)      es(2,l) = et3(6,l)   60 continuec continue moving partitions   70 iter = iter + 2c get partition from right      do 80 l = 1, nblok      kr = l + ks + 2      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et3(1,l) + es(1,l)c this segment is used for shared memory computersc     if (et3(1,l).lt.apav) thenc        eg(1,l) = es(1,kr)c        eg(2,l) = es(2,kr)c     endifc this segment is used for mpi computersc post receive from right      if (et3(1,l).lt.apav) then         call MPI_IRECV(eg,idps,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send partition to left      if (anpl.gt.anpav) then         call MPI_SEND(es,idps,mreal,kl-1,iter,lgrp,ierr)      endifc wait for partition to arrive      if (et3(1,l).lt.apav) call MPI_WAIT(msid,istatus,ierr)   80 continuec find new partitions      nter = 0      do 90 l = 1, nblok      kr = l + ks + 2      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et3(1,l) + es(1,l)      anpr = et3(1,l) - apav - anpavc left boundary is on the right      if (et3(1,l).lt.apav) then         if ((et3(1,l)+eg(1,l)).ge.apav) then            edges(1,l) = et3(2,l) - (anpr + anpav)*eg(2,l)/eg(1,l)c left boundary is even further to right         else            nter = nter + 1         endif      endifc left going data will need to be sent      if ((anpl-es(1,l)).gt.anpav) nter = nter + 1      if (kr.le.nvp) then         et3(1,l) = et3(1,l) + eg(1,l)         et3(2,l) = et3(2,l) + eg(2,l)         es(1,l) = eg(1,l)         es(2,l) = eg(2,l)      endif   90 continuec get more data from right      if (nter.gt.0) go to 70c send left edge to processor on right      iter = 2      do 100 l = 1, nblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if (kr.le.nvp) thenc        edges(2,l) = edges(1,kr)c     elsec        edges(2,l) = et3(4,l)c     endifc this segment is used for mpi computersc post receive from right      if (kr.le.nvp) then         call MPI_IRECV(edges(2,l),1,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send left edge to left      if (kl.gt.0) then         call MPI_SEND(edges(1,l),1,mreal,kl-1,iter,lgrp,ierr)      endifc wait for edge to arrive      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         edges(2,l) = et3(4,l)      endif  100 continuec calculate number of grids and offsets in new partitions      do 110 l = 1, nblok      kl = edges(1,l) + .5      noff(l) = kl      kr = edges(2,l) + .5      nyp(l) = kr - kl      if (inorder.eq.1) then         edges(1,l) = real(kl)         edges(2,l) = real(kr)      endif  110 continuec find minimum and maximum partition size      nypmin = nyp(1)      nypmax = nyp(1)      do 120 l = 1, nblok      nypmin = min0(nypmin,nyp(l))      nypmax = max0(nypmax,nyp(l))  120 continue      ibflg(1) = -nypmin      ibflg(2) = nypmax      call PIMAX(ibflg,iwork,2,1)      nypmin = -ibflg(1)      nypmax = ibflg(2) + 1      if (inorder.eq.2) nypmax = nypmax + 2      return      endc-----------------------------------------------------------------------      subroutine REPARTD2(edges,edg,eds,eg,es,et2,npic,noff,nyp,npav,nyp     1min,nypmax,kstrt,nvp,nblok,idps,nypm,inorder)c this subroutines finds new partitions boundaries (edges,noff,nyp)c from old partition information (npic,nyp).c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc edg/eds/eg/es/et2 = scratch arraysc npic(l) = number of particles per grid in partition lc noff(l) = lowermost global gridpoint in particle partition lc nyp(l) = number of primary gridpoints in particle partition lc npav = average number of particles per partition desiredc nypmin = minimum value of nyp in new partitionc nypmax = maximum value of nyp plus guard cells in new partitionc kstrt = starting data block numberc nvp = number of real or virtual processorsc nblok = number of field partitions.c idps = number of partition boundariesc nypm = maximum size of particle partitionc inorder = (1,2) (linear,quadratic) interpolation used      implicit none      real edges, edg, eds, eg, es, et2      integer npic, noff, nyp      integer npav, nypmin, nypmax, kstrt, nvp, nblok, idps, nypm      integer inorder      dimension edges(idps,nblok)      dimension edg(nypm,nblok), eds(nypm,nblok)      dimension eg(idps,nblok), es(idps,nblok), et2(2*idps,nblok)      dimension npic(nypm,nblok), noff(nblok), nyp(nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, iter, nter, nyp1, k1, kl, kr, k, l, ierr      real sum1, at1, at2, anpav, apav, anpl, anpr      integer msid, istatus      integer ibflg, iwork      dimension istatus(lstat)      dimension ibflg(2), iwork(2)c exit if flag is set      ks = kstrt - 2      iter = 2      anpav = real(npav)c copy number of particles and grid in current partition      do 20 l = 1, nblok      sum1 = 0.      do 10 k = 1, nyp(l)      at1 = npic(k,l)      sum1 = sum1 + at1      eds(k,l) = at1   10 continue      edges(1,l) = sum1      edges(2,l) = nyp(l)      et2(1,l) = edges(1,l)      et2(2,l) = edges(2,l)   20 continuec perform running sum      call PSCAN(edges,eg,es,idps,nblok)      do 30 l = 1, nblok      es(1,l) = et2(1,l)      es(2,l) = et2(2,l)      et2(1,l) = edges(1,l)      et2(2,l) = edges(2,l)      et2(3,l) = et2(1,l)      et2(4,l) = et2(2,l)      eg(1,l) = 0.      eg(2,l) = 0.      edges(2,l) = 1.0   30 continuec move partitions   40 iter = iter + 2c get partition from left      do 60 l = 1, nblok      kr = l + ks + 2      kl = l + ksc apav = desired number of particles on processor to left      apav = real(kl)*anpavc anpl = deficit of particles on processor to left      anpl = apav - et2(1,l) + es(1,l)c anpr = excess of particles on current processor      anpr = et2(1,l) - apav - anpavc this segment is used for shared memory computersc     if (anpl.lt.0.) thenc        nyp1 = es(2,kl)c        do 50 k = 1, nyp1c        edg(k,l) = eds(k,kl)c  50    continuec        eg(1,l) = es(1,kl)c        eg(2,l) = es(2,kl)c     endifc this segment is used for mpi computersc post receive from left      if (anpl.lt.0.) then         call MPI_IRECV(edg,nypm,mreal,kl-1,iter-1,lgrp,msid,ierr)      endifc send partition to right      if (anpr.gt.0.) then         nyp1 = es(2,l)         call MPI_SEND(eds,nyp1,mreal,kr-1,iter-1,lgrp,ierr)      endifc wait for partition to arrive      if (anpl.lt.0.) then         call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nyp1,ierr)         eg(2,l) = nyp1         sum1 = 0.         do 50 k = 1, nyp1         sum1 = sum1 + edg(k,l)   50    continue         eg(1,l) = sum1      endif   60 continuec find new partitions      nter = 0      do 100 l = 1, nblok      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et2(1,l) + es(1,l)      anpr = et2(1,l) - apav - anpavc left boundary is on the left      if (anpl.lt.0.) then         if ((anpl+eg(1,l)).ge.0.) thenc           edges(1,l) = (et2(2,l) - es(2,l)) + anpl*eg(2,l)/eg(1,l)            nyp1 = eg(2,l)            k1 = nyp1            sum1 = 0.   70       at1 = sum1            sum1 = sum1 - edg(k1,l)            k1 = k1 - 1            if ((sum1.gt.anpl).and.(k1.gt.0)) go to 70            at1 = real(nyp1 - k1 - 1) + (anpl - at1)/(sum1 - at1)            edges(1,l) = (et2(2,l) - es(2,l)) - at1c left boundary is even further to left         else            nter = nter + 1         endifc left boundary is inside      else if (et2(1,l).ge.apav) thenc        edges(1,l) = (et2(2,l) - es(2,l)) + anpl*es(2,l)/es(1,l)         nyp1 = es(2,l)         k1 = 1         sum1 = 0.   80    at1 = sum1         sum1 = sum1 + eds(k1,l)         k1 = k1 + 1         if ((sum1.lt.anpl).and.(k1.le.nyp1)) go to 80         at1 = real(k1 - 2) + (anpl - at1)/(sum1 - at1)         edges(1,l) = (et2(2,l) - es(2,l)) + at1      endifc right going data will need to be sent      if (anpr.gt.es(1,l)) nter = nter + 1      if (kl.gt.0) then         nyp1 = eg(2,l)         do 90 k = 1, nyp1         eds(k,l) = edg(k,l)   90    continue         et2(1,l) = et2(1,l) - es(1,l)         et2(2,l) = et2(2,l) - es(2,l)         es(1,l) = eg(1,l)         es(2,l) = eg(2,l)      endif  100 continuec get more data from left      if (nter.gt.0) go to 40      iter = nvp + 2c restore partition data      do 120 l = 1, nblok      sum1 = 0.      do 110 k = 1, nyp(l)      at1 = npic(k,l)      sum1 = sum1 + at1      eds(k,l) = at1  110 continue      et2(1,l) = et2(3,l)      et2(2,l) = et2(4,l)      es(1,l) = sum1      es(2,l) = nyp(l)      eg(1,l) = 0.      eg(2,l) = 0.  120 continuec continue moving partitions  130 iter = iter + 2c get partition from right      do 150 l = 1, nblok      kr = l + ks + 2      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et2(1,l) + es(1,l)c this segment is used for shared memory computersc     if (et2(1,l).lt.apav) thenc        nyp1 = es(2,kr)c        do 140 k = 1, nyp1c        edg(k,l) = eds(k,kr)c 140    continuec        eg(1,l) = es(1,kr)c        eg(2,l) = es(2,kr)c     endifc this segment is used for mpi computersc post receive from right      if (et2(1,l).lt.apav) then         call MPI_IRECV(edg,nypm,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send partition to left      if (anpl.gt.anpav) then         nyp1 = es(2,l)         call MPI_SEND(eds,nyp1,mreal,kl-1,iter,lgrp,ierr)      endifc wait for partition to arrive      if (et2(1,l).lt.apav) then         call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nyp1,ierr)         eg(2,l) = nyp1         sum1 = 0.         do 140 k = 1, nyp1         sum1 = sum1 + edg(k,l)  140    continue         eg(1,l) = sum1      endif  150 continuec find new partitions      nter = 0      do 180 l = 1, nblok      kr = l + ks + 2      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et2(1,l) + es(1,l)      anpr = et2(1,l) - apav - anpavc left boundary is on the right      if (et2(1,l).lt.apav) then         if ((et2(1,l)+eg(1,l)).ge.apav) thenc           edges(1,l) = et2(2,l) - (anpr + anpav)*eg(2,l)/eg(1,l)            nyp1 = eg(2,l)            k1 = 1            sum1 = 0.            at2 = - (anpr + anpav)  160       at1 = sum1            sum1 = sum1 + edg(k1,l)            k1 = k1 + 1            if ((sum1.lt.at2).and.(k1.le.nyp1)) go to 160            at1 = real(k1 - 2) + (at2 - at1)/(sum1 - at1)            edges(1,l) = et2(2,l) + at1c left boundary is even further to right         else            nter = nter + 1         endif      endifc left going data will need to be sent      if ((anpl-es(1,l)).gt.anpav) nter = nter + 1      if (kr.le.nvp) then         nyp1 = eg(2,l)         do 170 k = 1, nyp1         eds(k,l) = edg(k,l)  170    continue         et2(1,l) = et2(1,l) + eg(1,l)         et2(2,l) = et2(2,l) + eg(2,l)         es(1,l) = eg(1,l)         es(2,l) = eg(2,l)      endif  180 continuec get more data from right      if (nter.gt.0) go to 130c send left edge to processor on right      iter = 2      do 190 l = 1, nblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if (kr.le.nvp) thenc        edges(2,l) = edges(1,kr)c     elsec        edges(2,l) = et2(4,l)c     endifc this segment is used for mpi computersc post receive from right      if (kr.le.nvp) then         call MPI_IRECV(edges(2,l),1,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send left edge to left      if (kl.gt.0) then         call MPI_SEND(edges(1,l),1,mreal,kl-1,iter,lgrp,ierr)      endifc wait for edge to arrive      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         edges(2,l) = et2(4,l)      endif  190 continuec calculate number of grids and offsets in new partitions      do 200 l = 1, nblok      kl = edges(1,l) + .5      noff(l) = kl      kr = edges(2,l) + .5      nyp(l) = kr - kl      if (inorder.eq.1) then         edges(1,l) = real(kl)         edges(2,l) = real(kr)      endif  200 continuec find minimum and maximum partition size      nypmin = nyp(1)      nypmax = nyp(1)      do 210 l = 1, nblok      nypmin = min0(nypmin,nyp(l))      nypmax = max0(nypmax,nyp(l))  210 continue      ibflg(1) = -nypmin      ibflg(2) = nypmax      call PIMAX(ibflg,iwork,2,1)      nypmin = -ibflg(1)      nypmax = ibflg(2) + 1      if (inorder.eq.2) nypmax = nypmax + 2      return      end