c-----------------------------------------------------------------------      program dsmcc dynamic load balancing demo programc viktor k. decyk, ucla      implicit none      integer i, j, l      integer indx, indy, npx, npy, indnvp, idps, idimp, mshare, npxy      integer np, nx, ny, nloop, nvp, nblok, npmax, nbmax, ntmax, nypm1      integer ipbc, idproc, id0, kstrt, itime, npav, nppmx      integer nypmin, nypmax, isign, ierr      real tend, dt, vtx, vty, zero, anly, pibal      integer nyp, noff, npp, nps, ihole, jsl, jsr, jss, info, npic      real part, edges, sbufl, sbufr, rbufl, rbufr      real edg, eds, eg, es, et2c indx/indy = exponent which determines length in x/y direction,c where nx=2**indx, ny=2**indyc npx/npy = initial number of particles distributed in x/y direction      parameter( indx =   6, indy =   7, npx =     384, npy =     768)c tend = time at end of simulation, in units of plasma frequencyc dt = time interval between successive calculations      parameter( tend =  65.000, dt = 0.2000000e+00)c vty = thermal velocity of electrons in y direction      parameter( vty =   1.000)c indnvp = exponent determining number of real or virtual processorsc indnvp must be <= indyc idps = number of partition boundariesc idimp = dimension of phase space = 4c mshare = (0,1) = (no,yes) architecture is shared memory      parameter( indnvp =   2, idps =    2, idimp =   4, mshare =   0)c np = total number of electrons in simulation      parameter(npxy=npx*npy,np=npxy)      parameter(nx=2**indx,ny=2**indy)c nloop = number of time steps in simulation      parameter(nloop=tend/dt+.0001)c nvp = number of real or virtual processors, nvp = 2**indnvpc nblok = number of particle partitions      parameter(nvp=2**indnvp,nblok=1+mshare*(nvp-1))c npmax = maximum number of particles in each partition      parameter(npmax=(np/nvp)*4.0)c nbmax = size of buffer for passing particles between processors      parameter(nbmax=1+4*(2*(npxy*vty))*dt/ny)c ntmax = size of hole array for particles leaving processors      parameter(ntmax=2*nbmax)c dimensions for repartitioning arrays      parameter(nypm1=(ny+ny-1)/nvp+2)c ipbc = particle boundary condition is 2d periodic      parameter(ipbc=1)      common /large/ partc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition l      dimension part(idimp,npmax,nblok)c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition l      dimension edges(idps,nblok)c nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.      dimension nyp(nblok), noff(nblok)c npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition l      dimension npp(nblok), nps(nblok)c sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processor      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)c rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processor      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)c ihole = location of holes left in particle arrays      dimension ihole(ntmax,nblok)c jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition l      dimension jsl(idps,nblok), jsr(idps,nblok)c jss(idps,l) = scratch array for particle partition l      dimension jss(idps,nblok)c info = particle manager status information      dimension info(7)c scratch data for repartitioning      dimension npic(nypm1,nblok)      dimension edg(nypm1,nblok), eds(nypm1,nblok)      dimension eg(idps,nblok), es(idps,nblok), et2(2*idps,nblok)c vtx = thermal velocity of electrons in x direction      data vtx /1.0/c initialize for parallel processing      call PPINIT(idproc,id0,nvp)      kstrt = idproc + 1c initialize constants      itime = 0      zero = 0.c calculate uniform partition variables      call DCOMP2L(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c initialize density profile and velocity distribution      do 130 l = 1, nblok      nps(l) = 1      npp(l) = 0  130 continuec anly = initial linear density weight in y direction      anly = 18./11.      if (npxy.gt.0) thenc real space distribution         call PLDISTR2(part,nps,zero,anly,npx,npy,nx,ny,idimp,npmax,nblo     1k,kstrt,nvp,ipbc,ierr)         if (ierr.ne.0) then            call ppexit            stopc velocity space distribution         endif         call PVDISTR2(part,npp,nps,vtx,vty,zero,zero,npx,npy,idimp,npma     1x,nblok,kstrt,nvp,ierr)         if (ierr.ne.0) then            call ppexit            stop         endif      endifcc * * * start main iteration loop * * *c  500 if (nloop.le.itime) go to 800      if (id0.eq.0) write (6,*) 'itime=',itimec push particles      do 630 l = 1, nblok      do 620 j = 1, npp(l)      do 610 i = 1, 2      part(i,j,l) = part(i,j,l) + part(i+2,j,l)*dt  610 continue  620 continue  630 continuec move particles into appropriate spatial regions      call PMOVE2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,info)      if (info(1).ne.0) then         call PPEXIT         stop      endifc calculate percent imbalance      npav = info(7)/nvp      if (npav.gt.0) then         nppmx = max0(info(2)-npav,npav-info(3))         pibal = real(nppmx)/real(npav)      endifc repartition if imbalance is greater than 5%      if (pibal.lt.(.05)) go to 700      if (id0.eq.0) write (6,*) 'maximum percent imbalance =',pibalc count the number of particles per cell      isign = 1      call PCOUNT2YL(part,isign,npic,npp,noff,nyp,idimp,npmax,nblok,nypm     11)c determine new repartitioning edges      call REPARTD2(edges,edg,eds,eg,es,et2,npic,noff,nyp,npav,nypmin,ny     1pmax,kstrt,nvp,nblok,idps,nypm1)      if ((nypmin.lt.1).or.((nypmax+1).gt.nypm1)) then         write (6,*) 'Field size error: nypmin,nypmax=',nypmin,nypmax         call PPEXIT         stop      endifc move particles into appropriate spatial regions      call PMOVE2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,info)      if (info(1).ne.0) then         call PPEXIT         stop      endif      if (id0.eq.0) write (6,*) 'repartitioning complete'      write (6,*) 'kstrt,itime,edges=',kstrt,itime,edges(1,1),edges(2,1)  700 itime = itime + 1      go to 500  800 continuecc * * * end main iteration loop * * *c      call PPEXIT      if (id0.eq.0) write (6,*) 'q.e.d.'      stop      endc-----------------------------------------------------------------------      subroutine PPINIT(idproc,id0,nvp)c this subroutine initializes parallel processingc creates a communicator with number of processors equal to a power of 2c output: idproc, id0, nvpc idproc = processor id in lgrp communicatorc id0 = processor id in MPI_COMM_WORLDc nvp = number of real or virtual processors obtained      implicit none      integer idproc, id0, nvpc get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for realsc mint = default datatype for integersc mcplx = default datatype for complex typec mdouble = default double precision typec lworld = MPI_COMM_WORLD communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ierror, ndprec, i, k      logical flag      real small, prec, vresult      save /PPARMS/      data small /1.0e-12/      prec = 1.0 + smallc ndprec = (0,1) = (no,yes) use (normal,autodouble) precision      if (vresult(prec).gt.1.0) then         ndprec = 1      else         ndprec = 0      endifc this segment is used for shared memory computersc     nproc = nvpc     id0 = 0c     idproc = 0c this segment is used for mpi computers      if (MPI_STATUS_SIZE.gt.lstat) then         write (2,*) ' status size too small, actual/required = ', lstat     1, MPI_STATUS_SIZE         stop      endifc indicate whether MPI_INIT has been called      call MPI_INITIALIZED(flag,ierror)      if (.not.flag) thenc initialize the MPI execution environment         call MPI_INIT(ierror)         if (ierror.ne.0) stopc already initialized      else         call PPID(idproc,id0,nvp)         return      endif      lworld = MPI_COMM_WORLDc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lworld,id0,ierror)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lworld,nproc,ierror)c set default datatypes      mint = MPI_INTEGER      mdouble = MPI_DOUBLE_PRECISIONc single precision      if (ndprec.eq.0) then         mreal = MPI_REAL         mcplx = MPI_COMPLEXc double precision      elsec        mint = MPI_INTEGER8         mreal = MPI_DOUBLE_PRECISION         mcplx = MPI_DOUBLE_COMPLEX      endifc set nvp to largest power of 2 contained in nproc      nvp = 1      idproc = id0   10 i = nvp + nvp      if (i.le.nproc) then         nvp = i         go to 10      endifc check if nproc is a power of 2      if (nproc.gt.nvp) then         idproc = idproc - 1      endifc create communicator which contains power of 2 number of processorsc exclude processor 0 in MPI_COMM_WORLD if nproc is not a power of 2c this processor can be used as a diagnostic node      if ((idproc.ge.0).and.(idproc.lt.nvp)) then         k = 2      else         idproc = MPI_PROC_NULL         k = 1      endif      call MPI_COMM_SPLIT(lworld,k,1,lgrp,ierror)c requested number of processors not obtained      if (ierror.ne.0) then         write (2,*) ' MPI_COMM_SPLIT error: nvp, nproc=', nvp, nproc         call PPEXIT         stop      endifc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lgrp,k,ierror)      if (idproc.ge.0) then         idproc = k      else         idproc = -(k + 1)      endifc determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lgrp,nproc,ierror)      return      endc-----------------------------------------------------------------------      function vresult(prec)      implicit none      real prec, vresult      vresult = prec      return      endc-----------------------------------------------------------------------      subroutine PPID(idproc,id0,nvp)c this subroutine gets processor ids and number of processorsc output: idproc, id0, nvpc idproc = processor id in lgrp communicatorc id0 = processor id in MPI_COMM_WORLDc nvp = number of real or virtual processors obtainedc output: all      implicit none      integer idproc, id0, nvpc common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplx,  mdouble, lworldc nproc = number of real or virtual processors obtainedc lgrp = current communicatorc lworld = MPI_COMM_WORLD communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ierrorc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lworld,id0,ierror)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lworld,nvp,ierror)c special case if diagnostic nodes are present      if (nvp.gt.nproc) then         idproc = id0 - 1         if (idproc.ge.nproc) idproc = nproc - idproc - 2      else         idproc = id0      endif      nvp = nproc      return      endc-----------------------------------------------------------------------      subroutine PPEXITc this subroutine terminates parallel processing      implicit nonec common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc lgrp = current communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworld      integer ierror      logical flagc indicate whether MPI_INIT has been called      call MPI_INITIALIZED(flag,ierror)      if (flag) thenc synchronize processes         call MPI_BARRIER(lworld,ierror)c terminate MPI execution environment         call MPI_FINALIZE(ierror)      endif      return      endc-----------------------------------------------------------------------      subroutine DCOMP2L(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c this subroutine determines spatial boundaries for particlec decomposition, calculates number of grid points in each spatialc region, and the offset of these grid points from the global addressc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.c ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idps = number of partition boundariesc nblok = number of particle partitions.      dimension edges(idps,nblok)      dimension nyp(nblok), noff(nblok)      ks = kstrt - 2      at1 = float(ny)/float(nvp)      do 10 l = 1, nblok      kb = l + ks      edges(1,l) = at1*float(kb)      noff(l) = edges(1,l)      edges(2,l) = at1*float(kb + 1)      kr = edges(2,l)      nyp(l) = kr - noff(l)   10 continue      return      endc-----------------------------------------------------------------------      subroutine PLDISTR2(part,nps,anlx,anly,npx,npy,nx,ny,idimp,npmax,n     1blok,kstrt,nvp,ipbc,ierr)c for 2d code, this subroutine calculates initial particle co-ordinatesc with the following bi-linear density profile:c n(x,y) = n(x)*n(y), where n(x) = n0x*(1. + anlx*(x/nx - .5)) and c n(y) = n0y*(1. + anly*(y/ny - .5)) and wherec n0x = npx/(nx - 2*edgelx) and n0y = npy/(ny - 2*edgely)c for distributed data.c particles are not necessarily in the correct processor.c part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc nps(l) = starting address of particles in partition lc anlx/anly = initial linear density weight in x/y directionc npx/npy = initial number of particles distributed in x/y directionc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitionsc kstrt = starting data block numberc nvp = number of real or virtual processorsc ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)c ierr = (0,1) = (no,yes) error condition existsc with spatial decomposition      implicit none      integer npx, npy, nx, ny, idimp, npmax, nblok, kstrt, nvp, ipbc      integer ierr, nps      real anlx, anly      real part      dimension part(idimp,npmax,nblok)      dimension nps(nblok)c local data      integer ks, nppv, npxy, j, k, l, n, nn, koff, noff, iwork      real edgelx, edgely, at1, at2, bt1, bt2, antx, anty, xt, yt      ierr = 0c particle distribution constants      ks = kstrt - 2      nppv = min((npx*npy)/nvp,npmax)      npxy = nppv*nvpc check for errors      if (npxy.ne.(npx*npy)) ierr = 1      call PISUM(ierr,iwork,1,1)      if (ierr.gt.0) then         ierr = npxy - npx*npy         write (2,*) 'particle distribution truncated, np = ', npxy         return      endifc set boundary values      edgelx = 0.      edgely = 0.      at1 = float(nx)/float(npx)      at2 = float(ny)/float(npy)      if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         at1 = float(nx-2)/float(npx)         at2 = float(ny-2)/float(npy)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         at1 = float(nx-2)/float(npx)         at2 = float(ny)/float(npy)      endif      if (anly.ne.0.) then         anty = anly/float(ny)         at2 = 2.*anty*at2         bt2 = 1. - .5*anty*(float(ny) - 2.*edgely)      endif      if (anlx.ne.0.) then         antx = anlx/float(nx)         at1 = 2.*antx*at1         bt1 = 1. - .5*antx*(float(nx) - 2.*edgelx)      endifc uniform density profile      do 20 l = 1, nblok      koff = nppv*(l + ks)      noff = nps(l) - 1      do 10 n = 1, nppvc j, k = used to determine spatial location of this particle      nn = n + koff      k = (nn - 1)/npx + 1      j = nn - npx*(k - 1)c linear density in x      if (anlx.ne.0.) then         xt = edgelx + (sqrt(bt1*bt1 + at1*(float(j) - .5)) - bt1)/antxc uniform density in x      else         xt = edgelx + at1*(float(j) - .5)      endifc linear density in y      if (anly.ne.0.) then         yt = edgely + (sqrt(bt2*bt2 + at2*(float(k) - .5)) - bt2)/antyc uniform density in y      else         yt = edgely + at2*(float(k) - .5)      endif      part(1,n+noff,l) = xt      part(2,n+noff,l) = yt   10 continue   20 continue      return      endc-----------------------------------------------------------------------      subroutine PVDISTR2(part,npp,nps,vtx,vty,vdx,vdy,npx,npy,idimp,npm     1ax,nblok,kstrt,nvp,ierr)c for 2d code, this subroutine calculates initial particle velocitiesc with maxwellian velocity with drift for distributed data.c part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition lc vtx/vty = thermal velocity of electrons in x/y directionc vdx/vdy = drift velocity of beam electrons in x/y directionc npx/npy = initial number of particles distributed in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitionsc kstrt = starting data block numberc nvp = number of real or virtual processorsc ierr = (0,1) = (no,yes) error condition existsc ranorm = gaussian random number with zero mean and unit variancec with spatial decomposition      implicit none      integer npx, npy, idimp, npmax, nblok, kstrt, nvp, ierr      real vtx, vty, vdx, vdy      integer npp, nps      real part      dimension part(idimp,npmax,nblok)      dimension npp(nblok), nps(nblok)c local data      integer ks, nppv, npxy, i, j, k, l, joff, imin, npt, iwork      real vxt, vyt, at1      double precision ranorm      double precision dsum1, dsum2      real sum2, work2      dimension sum2(2), work2(2)      ierr = 0c particle distribution constants      ks = kstrt - 2      nppv = min((npx*npy)/nvp,npmax)      npxy = nppv*nvpc maxwellian velocity distribution      do 30 k = 1, npy      joff = npx*(k - 1)      do 20 j = 1, npx      i = j + joffc maxwellian velocity distribution      vxt = vtx*ranorm()      vyt = vty*ranorm()      do 10 l = 1, nblok      imin = nppv*(l + ks) + 1      if ((i.ge.imin).and.(i.lt.(imin+nppv))) then         npt = npp(l) + 1         part(3,npt,l) = vxt         part(4,npt,l) = vyt         npp(l) = npt      endif   10 continue   20 continue   30 continue      npxy = 0c add correct drift      sum2(1) = 0.      sum2(2) = 0.      do 50 l = 1, nblok      dsum1 = 0.0d0      dsum2 = 0.0d0      do 40 j = nps(l), npp(l)      npxy = npxy + 1      dsum1 = dsum1 + part(3,j,l)      dsum2 = dsum2 + part(4,j,l)   40 continue      sum2(1) = sum2(1) + dsum1      sum2(2) = sum2(2) + dsum2   50 continue      call PISUM(npxy,iwork,1,1)      call PSUM(sum2,work2,2,1)      at1 = 1./float(npxy)      sum2(1) = at1*sum2(1) - vdx      sum2(2) = at1*sum2(2) - vdy      do 70 l = 1, nblok      do 60 j = nps(l), npp(l)      part(3,j,l) = part(3,j,l) - sum2(1)      part(4,j,l) = part(4,j,l) - sum2(2)   60 continue   70 continuec process errors      if (npxy.ne.(npx*npy)) then         ierr = npxy - npx*npy         write (2,*) 'velocity distribution truncated, np = ', npxy      endif      return      endc-----------------------------------------------------------------------      subroutine PSUM(f,g,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c f(j,k) = sum over k of f(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = f(j,kb+kxs)c     elsec        g(j,k) = f(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      f(j,k) = f(j,k) + g(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PISUM(if,ig,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c if(j,k) = sum over k of if(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c if = input and output integer datac ig = scratch integer arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      integer if, ig, nxp, nblok      dimension if(nxp,nblok), ig(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mint = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, nsid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        ig(j,k) = if(j,kb+kxs)c     elsec        ig(j,k) = if(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_ISEND(if,nxp,mint,kb+kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb+kxs-1,l+nxp,lgrp,istatus,ierr)      else         call MPI_ISEND(if,nxp,mint,kb-kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb-kxs-1,l+nxp,lgrp,istatus,ierr)      endif      call MPI_WAIT(nsid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      if(j,k) = if(j,k) + ig(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PIMAX(if,ig,nxp,nblok)c this subroutine finds parallel maximum for each element of a vectorc that is, if(j,k) = maximum as a function of k of if(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c if = input and output integer datac ig = scratch integer arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      integer if, ig      integer nxp, nblok      dimension if(nxp,nblok), ig(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        ig(j,k) = if(j,kb+kxs)c     elsec        ig(j,k) = if(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(ig,nxp,mint,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(if,nxp,mint,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(ig,nxp,mint,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(if,nxp,mint,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec find maximum      do 50 k = 1, nblok      do 40 j = 1, nxp      if(j,k) = max0(if(j,k),ig(j,k))   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PMOVE2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,info)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc info = status informationc info(1) = ierr = (0,N) = (no,yes) error condition existsc info(2) = maximum number of particles per processorc info(3) = minimum number of particles per processorc info(4) = maximum number of buffer overflowsc info(5) = maximum number of particle passes requiredc info(6) = total number of particles on entryc info(7) = total number of particles on exit      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, info      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)      dimension info(7)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer iy      parameter(iy=2)      integer ierr, l, ks, iter, npr, nps, npt, kb, kl, kr, j, j1, j2      integer i, nbsize, nter, mter, itermax      integer msid, istatus      integer ibflg, iwork      real any, yt      dimension msid(4), istatus(lstat)      dimension ibflg(4), iwork(4)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0      do 5 j = 1, 7      info(j) = 0    5 continue      itermax = 2000c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 mter = 0      do 50 l = 1, nblok      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 30 j = 1, npp(l)      yt = part(iy,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            do 23 i = 1, idimp            sbufr(i,jsr(1,l),l) = part(i,j,l)   23       continue            sbufr(iy,jsr(1,l),l) = yt            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endifc particles going down      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            do 27 i = 1, idimp            sbufl(i,jsl(1,l),l) = part(i,j,l)   27       continue            sbufl(iy,jsl(1,l),l) = yt            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endif      endif   30 continue   40 jss(1,l) = jsl(1,l) + jsr(1,l)   50 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = max0(nps,jss(2,l))   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      mter = mter + 1      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computersc     jsl(2,l) = jsr(1,kl)c     do 110 j = 1, jsl(2,l)c     do 105 i = 1, idimpc     rbufl(i,j,l) = sbufr(i,j,kl)c 105 continuec 110 continuec     jsr(2,l) = jsl(1,kr)c     do 120 j = 1, jsr(2,l)c     do 115 i = 1, idimpc     rbufr(i,j,l) = sbufl(i,j,kr)c 115 continuec 120 continuec this segment is used for mpi computersc post receive      call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)      call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particles      call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)     1,ierr)      call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),i     1err)c wait for particles to arrive      call MPI_WAIT(msid(1),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsl(2,l) = nps/idimp      call MPI_WAIT(msid(2),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(iy,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(iy,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (2,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(iy,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(iy,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (2,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = max0(nps,jsl(1,l)+jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sent      call MPI_WAIT(msid(3),istatus,ierr)      call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(iy,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         rbufr(iy,j,l) = yt         do 163 i = 1, idimp         sbufl(i,jsl(1,l),l) = rbufr(i,j,l)  163    continuec particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         rbufr(iy,j,l) = yt         do 165 i = 1, idimp         sbufr(i,jsr(1,l),l) = rbufr(i,j,l)  165    continuec particles staying here      else         jss(2,l) = jss(2,l) + 1         do 167 i = 1, idimp         rbufr(i,jss(2,l),l) = rbufr(i,j,l)  167    continue      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(iy,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            rbufl(iy,j,l) = yt            do 173 i = 1, idimp            sbufr(i,jsr(1,l),l) = rbufl(i,j,l)  173       continue             else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            rbufl(iy,j,l) = yt            do 175 i = 1, idimp            sbufl(i,jsl(1,l),l) = rbufl(i,j,l)  175       continue         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         do 177 i = 1, idimp         rbufl(i,jss(2,l),l) = rbufl(i,j,l)  177    continue      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      npt = npmax      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l)        nps = max0(nps,jss(2,l))      npt = min0(npt,jss(2,l))  220 continue      ibflg(1) = nps      ibflg(4) = -npt      call PIMAX(ibflg,iwork,4,1)      info(2) = ibflg(1)      info(3) = -ibflg(4)      ierr = ibflg(1) - npmax      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr         info(1) = ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      do 225 i = 1, idimp      part(i,ihole(j,l),l) = rbufl(i,j,l)  225 continue  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         do 233 i = 1, idimp         part(i,ihole(j+jsl(2,l),l),l) = rbufr(i,j,l)  233    continue      elsec no more holesc distribute remaining particles from below into bottom         do 237 i = 1, idimp         part(i,j+npp(l),l) = rbufl(i,j+jss(1,l),l)  237    continue      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            do 243 i = 1, idimp            part(i,ihole(j2,l),l) = part(i,j1,l)  243       continue         endif      elsec no more holesc distribute remaining particles from above into bottom         do 247 i = 1, idimp         part(i,j+npp(l),l) = rbufr(i,j+jss(1,l),l)  247    continue      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      info(5) = max0(info(5),mter)      if (ibflg(2).gt.0) then         write (2,*) 'Info: particles being passed further = ', ibflg(2)         if (ibflg(3).gt.0) ibflg(3) = 1         if (iter.lt.itermax) go to 100         ierr = -((iter-2)/2)         write (2,*) 'Iteration overflow, iter = ', ierr         info(1) = ierr         go to 280      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         info(4) = nter         go to 20      endifc debugging section: count total number of particles after move      nps = 0      do 270 l = 1, nblok      nps = nps + npp(l)  270 continue      ibflg(2) = nps      ibflg(1) = npr      call PISUM(ibflg,iwork,2,1)      info(6) = ibflg(1)      info(7) = ibflg(2)      if (ibflg(1).ne.ibflg(2)) then         write (2,*) 'particle number error, old/new=',ibflg(1),ibflg(2)         info(1) = 1      endifc information  280 if (nter.gt.0) then         write (2,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine PCOUNT2YL(part,isign,npic,npp,noff,nyp,idimp,npmax,nblo     1k,nypm1)c this subroutine counts particles by y grid, accumulating into npicc npic array is initialized to zero, unless isign = 0c part = particle arrayc isign = (0,1) = (no,yes) initialize npic array to zeroc part(2,j,l) = position y of particle j in partition lc npic = number of particles per gridc npp(l) = number of particles in partition lc noff(l) = backmost global gridpoint in particle partition lc nyp(l) = number of primary gridpoints in particle partition lc idimp = size of phase spacec npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nypm1 = maximum size of particle partition plus one      implicit none      real part      integer npic, npp, noff, nyp      integer isign, idimp, npmax, nblok, nypm1      dimension part(idimp,npmax,nblok)      dimension npic(nypm1,nblok), npp(nblok), noff(nblok), nyp(nblok)      integer j, l, m, mnoff, nyp1      do 30 l = 1, nblok      mnoff = noff(l) - 1      nyp1 = nyp(l) + 1      if (isign.ne.0) then         do 10 j = 1, nyp1         npic(j,l) = 0   10    continue      endifc find how many particles in each grid      do 20 j = 1, npp(l)      m = part(2,j,l)      m = m - mnoff      npic(m,l) = npic(m,l) + 1   20 continue   30 continue      return      endc-----------------------------------------------------------------------      subroutine REPARTD2(edges,edg,eds,eg,es,et2,npic,noff,nyp,npav,nyp     1min,nypmax,kstrt,nvp,nblok,idps,nypm)c this subroutines finds new partitions boundaries (edges,noff,nyp)c from old partition information (npic,nyp).c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc edg/eds/eg/es/et2 = scratch arraysc npic(l) = number of particles per grid in partition lc noff(l) = lowermost global gridpoint in particle partition lc nyp(l) = number of primary gridpoints in particle partition lc npav = average number of particles per partition desiredc nypmin/nypmax = minimum/maximum value of nyp in new partitionc kstrt = starting data block numberc nvp = number of real or virtual processorsc nblok = number of field partitions.c idps = number of partition boundariesc nypm = maximum size of particle partition      implicit none      real edges, edg, eds, eg, es, et2      integer npic, noff, nyp      integer npav, nypmin, nypmax, kstrt, nvp, nblok, idps, nypm      dimension edges(idps,nblok)      dimension edg(nypm,nblok), eds(nypm,nblok)      dimension eg(idps,nblok), es(idps,nblok), et2(2*idps,nblok)      dimension npic(nypm,nblok), noff(nblok), nyp(nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, iter, nter, nyp1, k1, kl, kr, k, l, ierr      real sum1, at1, at2, anpav, apav, anpl, anpr      integer msid, istatus      integer ibflg, iwork      dimension istatus(lstat)      dimension ibflg(2), iwork(2)c exit if flag is set      ks = kstrt - 2      iter = 2      anpav = real(npav)c copy number of particles and grid in current partition      do 20 l = 1, nblok      sum1 = 0.      do 10 k = 1, nyp(l)      at1 = npic(k,l)      sum1 = sum1 + at1      eds(k,l) = at1   10 continue      edges(1,l) = sum1      edges(2,l) = nyp(l)      et2(1,l) = edges(1,l)      et2(2,l) = edges(2,l)   20 continuec perform running sum      call PSCAN(edges,eg,es,idps,nblok)      do 30 l = 1, nblok      es(1,l) = et2(1,l)      es(2,l) = et2(2,l)      et2(1,l) = edges(1,l)      et2(2,l) = edges(2,l)      et2(3,l) = et2(1,l)      et2(4,l) = et2(2,l)      eg(1,l) = 0.      eg(2,l) = 0.      edges(2,l) = 1.0   30 continuec move partitions   40 iter = iter + 2c get partition from left      do 60 l = 1, nblok      kr = l + ks + 2      kl = l + ksc apav = desired number of particles on processor to left      apav = real(kl)*anpavc anpl = deficit of particles on processor to left      anpl = apav - et2(1,l) + es(1,l)c anpr = excess of particles on current processor      anpr = et2(1,l) - apav - anpavc this segment is used for shared memory computersc     if (anpl.lt.0.) thenc        nyp1 = es(2,kl)c        do 50 k = 1, nyp1c        edg(k,l) = eds(k,kl)c  50    continuec        eg(1,l) = es(1,kl)c        eg(2,l) = es(2,kl)c     endifc this segment is used for mpi computersc post receive from left      if (anpl.lt.0.) then         call MPI_IRECV(edg,nypm,mreal,kl-1,iter-1,lgrp,msid,ierr)      endifc send partition to right      if (anpr.gt.0.) then         nyp1 = es(2,l)         call MPI_SEND(eds,nyp1,mreal,kr-1,iter-1,lgrp,ierr)      endifc wait for partition to arrive      if (anpl.lt.0.) then         call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nyp1,ierr)         eg(2,l) = nyp1         sum1 = 0.         do 50 k = 1, nyp1         sum1 = sum1 + edg(k,l)   50    continue         eg(1,l) = sum1      endif   60 continuec find new partitions      nter = 0      do 100 l = 1, nblok      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et2(1,l) + es(1,l)      anpr = et2(1,l) - apav - anpavc left boundary is on the left      if (anpl.lt.0.) then         if ((anpl+eg(1,l)).ge.0.) then            nyp1 = eg(2,l)            k1 = nyp1            sum1 = 0.   70       at1 = sum1            sum1 = sum1 - edg(k1,l)            k1 = k1 - 1            if ((sum1.gt.anpl).and.(k1.gt.0)) go to 70            at1 = real(nyp1 - k1 - 1) + (anpl - at1)/(sum1 - at1)            edges(1,l) = (et2(2,l) - es(2,l)) - at1c left boundary is even further to left         else            nter = nter + 1         endifc left boundary is inside      else if (et2(1,l).ge.apav) then         nyp1 = es(2,l)         k1 = 1         sum1 = 0.   80    at1 = sum1         sum1 = sum1 + eds(k1,l)         k1 = k1 + 1         if ((sum1.lt.anpl).and.(k1.le.nyp1)) go to 80         at2 = real(k1 - 2)         if (sum1.gt.at1) at2 = at2 + (anpl - at1)/(sum1 - at1)         edges(1,l) = (et2(2,l) - es(2,l)) + at2      endifc right going data will need to be sent      if (anpr.gt.es(1,l)) nter = nter + 1      if (kl.gt.0) then         nyp1 = eg(2,l)         do 90 k = 1, nyp1         eds(k,l) = edg(k,l)   90    continue         et2(1,l) = et2(1,l) - es(1,l)         et2(2,l) = et2(2,l) - es(2,l)         es(1,l) = eg(1,l)         es(2,l) = eg(2,l)      endif  100 continuec get more data from left      if (nter.gt.0) go to 40      iter = nvp + 2c restore partition data      do 120 l = 1, nblok      sum1 = 0.      do 110 k = 1, nyp(l)      at1 = npic(k,l)      sum1 = sum1 + at1      eds(k,l) = at1  110 continue      et2(1,l) = et2(3,l)      et2(2,l) = et2(4,l)      es(1,l) = sum1      es(2,l) = nyp(l)      eg(1,l) = 0.      eg(2,l) = 0.  120 continuec continue moving partitions  130 iter = iter + 2c get partition from right      do 150 l = 1, nblok      kr = l + ks + 2      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et2(1,l) + es(1,l)c this segment is used for shared memory computersc     if (et2(1,l).lt.apav) thenc        nyp1 = es(2,kr)c        do 140 k = 1, nyp1c        edg(k,l) = eds(k,kr)c 140    continuec        eg(1,l) = es(1,kr)c        eg(2,l) = es(2,kr)c     endifc this segment is used for mpi computersc post receive from right      if (et2(1,l).lt.apav) then         call MPI_IRECV(edg,nypm,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send partition to left      if (anpl.gt.anpav) then         nyp1 = es(2,l)         call MPI_SEND(eds,nyp1,mreal,kl-1,iter,lgrp,ierr)      endifc wait for partition to arrive      if (et2(1,l).lt.apav) then         call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nyp1,ierr)         eg(2,l) = nyp1         sum1 = 0.         do 140 k = 1, nyp1         sum1 = sum1 + edg(k,l)  140    continue         eg(1,l) = sum1      endif  150 continuec find new partitions      nter = 0      do 180 l = 1, nblok      kr = l + ks + 2      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et2(1,l) + es(1,l)      anpr = et2(1,l) - apav - anpavc left boundary is on the right      if (et2(1,l).lt.apav) then         if ((et2(1,l)+eg(1,l)).ge.apav) then            nyp1 = eg(2,l)            k1 = 1            sum1 = 0.            at2 = - (anpr + anpav)  160       at1 = sum1            sum1 = sum1 + edg(k1,l)            k1 = k1 + 1            if ((sum1.lt.at2).and.(k1.le.nyp1)) go to 160            at1 = real(k1 - 2) + (at2 - at1)/(sum1 - at1)            edges(1,l) = et2(2,l) + at1c left boundary is even further to right         else            nter = nter + 1         endif      endifc left going data will need to be sent      if ((anpl-es(1,l)).gt.anpav) nter = nter + 1      if (kr.le.nvp) then         nyp1 = eg(2,l)         do 170 k = 1, nyp1         eds(k,l) = edg(k,l)  170    continue         et2(1,l) = et2(1,l) + eg(1,l)         et2(2,l) = et2(2,l) + eg(2,l)         es(1,l) = eg(1,l)         es(2,l) = eg(2,l)      endif  180 continuec get more data from right      if (nter.gt.0) go to 130c send left edge to processor on right      iter = 2      do 190 l = 1, nblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if (kr.le.nvp) thenc        edges(2,l) = edges(1,kr)c     elsec        edges(2,l) = et2(4,l)c     endifc this segment is used for mpi computersc post receive from right      if (kr.le.nvp) then         call MPI_IRECV(edges(2,l),1,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send left edge to left      if (kl.gt.0) then         call MPI_SEND(edges(1,l),1,mreal,kl-1,iter,lgrp,ierr)      endifc wait for edge to arrive      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         edges(2,l) = et2(4,l)      endif  190 continuec calculate number of grids and offsets in new partitions      do 200 l = 1, nblok      kl = edges(1,l) + .5      noff(l) = kl      kr = edges(2,l) + .5      nyp(l) = kr - kl      edges(1,l) = real(kl)      edges(2,l) = real(kr)  200 continuec find minimum and maximum partition size      nypmin = nyp(1)      nypmax = nyp(1)      do 210 l = 1, nblok      nypmin = min0(nypmin,nyp(l))      nypmax = max0(nypmax,nyp(l))  210 continue      ibflg(1) = -nypmin      ibflg(2) = nypmax      call PIMAX(ibflg,iwork,2,1)      nypmin = -ibflg(1)      nypmax = ibflg(2)      return      endc-----------------------------------------------------------------------      subroutine PSCAN(f,g,s,nxp,nblok)c this subroutine performs a parallel prefix reduction of a vector,c that is: f(j,k) = sum over k of f(j,k), where the sum is over k valuesc less than idproc.c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g, s = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g, s      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok), s(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c initialize global sum      do 20 k = 1, nblok      do 10 j = 1, nxp      s(j,k) = f(j,k)   10 continue   20 continuec main iteration loop   30 if (kxs.ge.nproc) go to 90c shift data      do 60 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 40 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = s(j,kb+kxs)c     elsec        g(j,k) = s(j,kb-kxs)c     endifc  40 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(s,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(s,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)c perform prefix scan      if (lb.ne.0) then         do 50 j = 1, nxp         f(j,k) = f(j,k) + g(j,k)   50    continue      endif   60 continuec perform sum      do 80 k = 1, nblok      do 70 j = 1, nxp      s(j,k) = s(j,k) + g(j,k)   70 continue   80 continue      l = l + 1      kxs = kxs + kxs      go to 30   90 return      endc-----------------------------------------------------------------------      function ranorm()c this program calculates a random number y from a gaussian distributionc with zero mean and unit variance, according to the method ofc mueller and box:c    y(k) = (-2*ln(x(k)))**1/2*sin(2*pi*x(k+1))c    y(k+1) = (-2*ln(x(k)))**1/2*cos(2*pi*x(k+1)),c where x is a random number uniformly distributed on (0,1).c written for the ibm by viktor k. decyk, ucla      integer r1,r2,r4,r5      double precision ranorm,h1l,h1u,h2l,r0,r3,asc,bsc,temp      save iflg,r1,r2,r4,r5,h1l,h1u,h2l,r0      data r1,r2,r4,r5 /885098780,1824280461,1396483093,55318673/      data h1l,h1u,h2l /65531.0d0,32767.0d0,65525.0d0/      data iflg,r0 /0,0.0d0/      if (iflg.eq.0) go to 10      ranorm = r0      r0 = 0.0d0      iflg = 0      return   10 isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r1 - (r1/isc)*isc      r3 = h1l*dble(r1) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r2/isc      isc = r2 - i1*isc      r0 = h1l*dble(r2) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r2 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r1 = r3 - dble(isc)*bsc      temp = dsqrt(-2.0d0*dlog((dble(r1) + dble(r2)*asc)*asc))      isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r4 - (r4/isc)*isc      r3 = h2l*dble(r4) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r5/isc      isc = r5 - i1*isc      r0 = h2l*dble(r5) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r5 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r4 = r3 - dble(isc)*bsc      r0 = 6.28318530717959d0*((dble(r4) + dble(r5)*asc)*asc)      ranorm = temp*dsin(r0)      r0 = temp*dcos(r0)      iflg = 1      return      end