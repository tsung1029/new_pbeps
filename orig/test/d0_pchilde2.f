c * * * periodic 2d electrostatic particle simulation kernel code * * *c this is a simple 2d skeleton particle-in-cell code designed forc exploring new computer architectures.  it contains the critical piecesc needed for depositing charge and current, advancing particles, andc solving the fields.  the code moves only electrons, with periodicc electromagnetic forces obtained by solving maxwells's equation withc fast fourier transforms.c the only diagnostic is particle and field energy.c portable gcpic kernel code, using algorithm described in:c p. c. liewer and v. k. decyk, j. computational phys. 85, 302 (1989).c written by viktor k. decyk, uclac for mpi distributed memory computersc update: february 10, 2006      program beps2kc indx/indy = exponent which determines length in x/y direction,c where nx=2**indx, ny=2**indyc npx/npy = initial number of particles distributed in x/y directionc     parameter( indx =   5, indy =   6, npx =      96, npy =     192)      parameter( indx =   6, indy =   7, npx =     384, npy =     768)c     parameter( indx =   7, indy =   8, npx =    1280, npy =    2560)c npxb/npyb = initial number of particles in beam in x/y directionc     parameter( npxb =  32, npyb =  64)      parameter( npxb = 128, npyb = 256)c     parameter( npxb = 384, npyb = 768)c tend = time at end of simulation, in units of plasma frequencyc dt = time interval between successive calculationsc     parameter( tend =  65.000, dt = 0.2000000e+00)      parameter( tend =  85.000, dt = 0.025)c vty = thermal velocity of electrons in y directionc vtdy = thermal velocity of beam electrons in y direction      parameter( vty =   1.000, vtdy =   0.500)c avdy = absolute value of drift velocity of beam electrons y direction      parameter( avdy =   5.000)c indnvp = exponent determining number of real or virtual processorsc indnvp must be <= indyc idps = number of partition boundariesc idimp = dimension of phase space = 5c mshare = (0,1) = (no,yes) architecture is shared memory      parameter( indnvp =   2, idps =    2, idimp =   5, mshare =   0)c np = total number of electrons in simulation      parameter(npxy=npx*npy,npxyb=npxb*npyb,np=npxy+npxyb)      parameter(nx=2**indx,ny=2**indy,nxh=nx/2,nyh=ny/2)      parameter(nxv=nx+2,nyv=ny+2,nxvh=nxv/2)      parameter(nxe=nx+4,nxeh=nxe/2)c nloop = number of time steps in simulation      parameter(nloop=tend/dt+.0001)c nvp = number of real or virtual processors, nvp = 2**indnvpc nblok = number of particle partitions      parameter(nvp=2**indnvp,nblok=1+mshare*(nvp-1))c npmax = maximum number of particles in each partitionc nypmx = maximum size of particle partition, including guard cells.      parameter(npmax=(np/nvp)*1.01+7000,nypmx=(ny-1)/nvp+4)      parameter(nxvyp=nxv*nypmx,nxeyp=nxe*nypmx)c kyp = number of complex grids in each field partition in y directionc kxp = number of complex grids in each field partition in x direction      parameter(kyp=(ny-1)/nvp+1,kxp=(nxh-1)/nvp+1)c kyb = number of processors in yc kxb = number of processors in x      parameter(kyb=ny/kyp,kxb=nxh/kxp,jkmx=kxb*(kyb/kxb)+kyb*(kxb/kyb))c kxyb = maximum(kxb,kyb)      parameter(kxyb=jkmx/(2-kxb/jkmx-kyb/jkmx))c kblok = number of field partitions in y direction      parameter(kbmin=1+(1-mshare)*(kxyb/kxb-1))      parameter(kblok=1+mshare*(ny/kyp-1))c jblok = number of field partitions in x direction      parameter(jbmin=1+(1-mshare)*(kxyb/kyb-1))      parameter(jblok=1+mshare*(nxh/kxp-1))c ngds = number of guard cells      parameter(ngds=3*((idps-1)/2+1))c nxyh = maximum(nx,ny)/2c nxhy = maximum(nx/2,ny)      parameter(nmx=nx*(ny/nx)+ny*(nx/ny),nxy=nmx/(2-nx/nmx-ny/nmx))      parameter(nxyh=nxy/2,nmxh=nxh*(ny/nxh)+ny*(nxh/ny))      parameter(nxhy=nmxh/(2-nxh/nmxh-ny/nmxh))c nbmax = size of buffer for passing particles between processors      parameter(nbmax=1+(2*(npxy*vty+npxyb*vtdy)+1.4*npxyb*avdy)*dt/ny)c ntmax = size of hole array for particles leaving processors      parameter(ntmax=2*nbmax)c npd = size of scratch buffers for vectorized charge deposition      parameter(npd=128,nine=9,n27=27,n18=18)c dimensions for sorting arrays      parameter(nypm1=(ny-1)/nvp+2)c ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)      parameter(psolve=6,ipbc=2)      complex qt, fxyt, cut, bxyt, bzt, exyz, bxyz, ffc, bs, br, sct      common /large/ partc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc part(5,n,l) = velocity vz of particle n in partition l      dimension part(idimp,npmax,nblok)c maskp = scratch array for particle addressesc     dimension maskp(npmax,nblok)c in real space, qe(j+1,k,l) = charge density at grid point (j,kk)c where kk = k + noff(l) - 1      dimension qe(nxe,nypmx*kbmin,kblok)c in real space, cu(3,j+1,k,l) = current density at grid point (j,kk)      dimension cu(3,nxe,nypmx*kbmin,kblok)c     dimension cu(2,nxe,nypmx*kbmin,kblok)c in real space, fxyze(i,j+1,k,l) = i component of force/charge at c grid point (j,kk)c in other words, fxyze are the convolutions of the electric fieldc over the particle shape, where kk = k + noff(l) - 1      dimension fxyze(3,nxe,nypmx*kbmin,kblok)      dimension fxye(2,nxe,nypmx*kbmin,kblok)c bxye(i,j+1,k,l) = i component of magnetic field at grid (j,kk).      dimension bxye(3,nxe,nypmx*kbmin,kblok)      dimension bze(nxe,nypmx*kbmin,kblok)c qt(k,j,l) = complex charge density for fourier mode jj-1,k-1c fxyt(1,k,j,l) = x component of force/charge for fourier mode jj-1,k-1c fxyt(2,k,j,l) = y component of force/charge for fourier mode jj-1,k-1c where jj = j + kxp*(l - 1)      dimension qt(nyv,kxp,jblok), cut(3,nyv,kxp,jblok)      dimension fxyt(3,nyv,kxp,jblok), bxyt(3,nyv,kxp,jblok)c     dimension qt(nyv,kxp,jblok), cut(2,nyv,kxp,jblok)c     dimension fxyt(2,nyv,kxp,jblok), bxyt(2,nyv,kxp,jblok)      dimension bzt(nyv,kxp,jblok)c in fourier space, exyz = transverse electric fieldc in fourier space, bxyz = magnetic field      dimension exyz(3,nyv,kxp,jblok), bxyz(3,nyv,kxp,jblok)c ffc = form factor array for poisson solver      dimension ffc(nyh,kxp,jblok)c mixup, sct = arrays for fft      dimension mixup(nxhy), sct(nxyh)c bs, br = complex buffer arrays for transposec     dimension bs(3,kxp,kyp,kblok), br(3,kxp,kyp,jblok)c msid, mrid = scratch arrays for identifying asynchronous messagesc     dimension msid(kxb), mrid(kyb)c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition l      dimension edges(idps,nblok)c nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.      dimension nyp(nblok), noff(nblok)c npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition l      dimension npp(nblok), nps(nblok)c sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processor      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)c rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processor      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)c ihole = location of holes left in particle arrays      dimension ihole(ntmax,nblok)c jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition l      dimension jsl(idps,nblok), jsr(idps,nblok)c jss(idps,l) = scratch array for particle partition lc scr(j,ngds,l) = complex scratch array for particle partition l      dimension jss(idps,nblok), scr(3*nxe,ngds,nblok)c nn = scratch address array for vectorized charge depositionc amxy = scratch weight array for vectorized charge depositionc     dimension nn(n27,npd,nblok), amxy(n27,npd,nblok)c wtot = total energy      dimension wtot(6), work(6)c sorting arrays      dimension pt(npmax,nblok), ip(npmax,nblok), npic(nypm1,nblok)      character*12 labelc potential and display      complex fxt, fyt      dimension fx(nxe,nypmx,kblok), fy(nxe,nypmx,kblok)      dimension fz(nxe,nypmx,kblok)      dimension fxt(nyv,kxp,jblok), fyt(nyv,kxp,jblok)      dimension pot(nxe,nypmx,kblok)      dimension g(nxe,nypmx,kblok), lf(nxe,nypmx,kblok)c      parameter(indx1=indx+1,indy1=indy+1,nx2=2*nx,ny2=2*ny,nx2e=2*nxe)      parameter(nxhy2=2*nxhy,nxyh2=2*nxyh)      parameter(kxp2=(nx-1)/nvp+1,j2blok=1+mshare*(nx/kxp2-1))      parameter(kxb2=nx/kxp2)      parameter(jkmx1=kxb2*(kyb/kxb2)+kyb*(kxb2/kyb))      parameter(kxyb1=jkmx1/(2-kxb2/jkmx1-kyb/jkmx1))      parameter(kbmin1=1+(1-mshare)*(kxyb1/kxb2-1))      parameter(kxp21=kxp2+1)c      complex qt1, fxyt1, fxt1, fyt1, cut1, bxyt1, ffb, sct1, bs1, br1      complex exyz1, bxyz1, bzt1      dimension q1(nx2e,kyp*kbmin1,kblok)      dimension cu1(3,nx2e,kyp*kbmin1,kblok)c     dimension cu1(2,nx2e,kyp*kbmin1,kblok)      dimension fxy1(3,nx2e,kyp*kbmin1,kblok)c     dimension fxy1(2,nx2e,kyp*kbmin1,kblok)      dimension bxy1(3,nx2e,kyp*kbmin1,kblok)      dimension bz1(nx2e,kyp*kbmin1,kblok)      dimension pot1(nx2e,kyp*kbmin1,kblok)      dimension qt1(nyv,kxp2,j2blok), fxyt1(3,nyv,kxp2,j2blok)c     dimension qt1(nyv,kxp2,j2blok), fxyt1(2,nyv,kxp2,j2blok)      dimension fxt1(nyv,kxp2,j2blok), fyt1(nyv,kxp2,j2blok)      dimension cut1(3,nyv,kxp2,j2blok), bxyt1(3,nyv,kxp2,j2blok)c     dimension cut1(2,nyv,kxp2,j2blok), bzt1(nyv,kxp2,j2blok)      dimension exyz1(3,nyv,kxp2,j2blok), bxyz1(3,nyv,kxp2,j2blok)      dimension ffb(nyh,kxp2,j2blok)      dimension mixup1(nxhy2), sct1(nxyh2)c     dimension bs1(2,kxp2,kyp,kblok), br1(2,kxp2,kyp,jblok)c      parameter(kyp2=(ny2-1)/nvp+1,k2blok=1+mshare*(ny2/kyp2-1))      parameter(kyb2=ny2/kyp2)      parameter(jkmx2=kxb2*(kyb2/kxb2)+kyb2*(kxb2/kyb2))      parameter(kxyb2=jkmx2/(2-kxb2/jkmx2-kyb2/jkmx2))      parameter(kbmin2=1+(1-mshare)*(kxyb2/kxb2-1))c      complex qt2, fxyt2, fxt2, fyt2, cut2, bxyt2, ffd, sct2, bs2, br2      complex exyz2, bxyz2, bzt2      dimension q2(nx2e,kyp2*kbmin2,k2blok)      dimension cu2(3,nx2e,kyp2*kbmin2,k2blok)c     dimension cu2(2,nx2e,kyp2*kbmin2,k2blok)      dimension fxy2(3,nx2e,kyp2*kbmin2,k2blok)c     dimension fxy2(2,nx2e,kyp2*kbmin2,k2blok)      dimension bxy2(3,nx2e,kyp2*kbmin2,k2blok)c     dimension bz2(nx2e,kyp2*kbmin2,k2blok)      dimension pot2(nx2e,kyp2*kbmin2,k2blok)      dimension qt2(ny2,kxp2,j2blok), fxyt2(3,ny2,kxp2,j2blok)c     dimension qt2(ny2,kxp2,j2blok), fxyt2(2,ny2,kxp2,j2blok)      dimension fxt2(ny2,kxp2,j2blok), fyt2(ny2,kxp2,j2blok)      dimension cut2(3,ny2,kxp2,j2blok), bxyt2(3,ny2,kxp2,j2blok)c     dimension cut2(2,ny2,kxp2,j2blok), bzt2(ny2,kxp2,j2blok)      dimension exyz2(3,ny2,kxp2,j2blok), bxyz2(3,ny2,kxp2,j2blok)      dimension ffd(ny,kxp2,j2blok)      dimension mixup2(nxhy2), sct2(nxyh2)c     dimension bs2(3,kxp2,kyp2,k2blok), br2(3,kxp2,kyp2,j2blok)c      dimension qr(nyv,kxp21,j2blok), cur(3,nyv,kxp21,j2blok)      dimension fxyr(3,nyv,kxp21,j2blok)      dimension exyr(3,nyv,kxp21,j2blok), bxyr(3,nyv,kxp21,j2blok)c      dimension ds(3,kxp21,kyp+1,kblok), dr(3,kxp21,kyp+1,j2blok)      dimension fxd(nyv,kxp21,j2blok), fyd(nyv,kxp21,j2blok)      dimension bxyd(3,nyv,kxp21,j2blok), bzd(nyv,kxp21,j2blok)c  991 format (5h T = ,i7)  992 format (19h * * * q.e.d. * * *)  993 format (34h field, kinetic, total energies = ,3e14.7)  994 format (36h electric(l,t), magnetic energies = ,3e14.7)c vtx/vtz = thermal velocity of electrons in x/z directionc qme = charge on electron, in units of ec vdx/vdy/vdz = drift velocity of beam electrons in x/y/z directionc vtdx/vtdz = thermal velocity of beam electrons in x/z direction      data vtx,vtz,qme,vdx,vdy,vdz,vtdx,vtdz /1.,1.,-1.,0.,5.,0.,.5,.5/c ax/ay = half-width of particle in x/y direction      data ax,ay /.866667,.866667/c omx/omy/omz = magnetic field electron cyclotron frequency in x/y/z c direction      data omx,omy,omz /0.2,0.5,1.0/c ntpose = (0,1) = (no,yes) input, output data are transposed in pfft2r      data ntpose /1/c initialize for parallel processing      call ppinit(idproc,nvp)      kstrt = idproc + 1      if (kstrt.eq.1) then         open(unit=6,file='output2',form='formatted',status='unknown')      endifc initialize timer      call TIMERA(-1,'init    ',time)c initialize constants      itime = 0      qbme = qme      dth = .5*dtc     ci = 1.0      ci = 0.1      zero = 0.      if (ipbc.eq.1) then         affp = float(nx*ny)/float(np)      else if (ipbc.eq.2) then         affp = float((nx-2)*(ny-2))/float(np)      else if (ipbc.eq.3) then         affp = float((nx-2)*ny)/float(np)      endif      qi0 = -qme/affpc calculate partition variables      call dcomp2(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c debugc     vtz = 0.c     vdz = 0.c     vtdz = 0.c initialize electromagnetic magnetic fields      do 130 l = 1, nblok      nyp3 = nyp(l) + 3      do 120 k = 1, nyp3      do 110 j = 1, nx      do 100 i = 1, 3      bxye(i,j+1,k,l) = 0.  100 continue      bze(j+1,k,l) = 0.  110 continue  120 continue  130 continue      do 170 l = 1, jblok      do 160 j = 1, kxp      do 150 k = 1, ny      do 140 i = 1, 3      bxyz(i,k,j,l) = 0.      exyz(i,k,j,l) = 0.  140 continue  150 continue  160 continue  170 continue      exyr = 0.      bxyr = 0.      exyz1 = 0.      bxyz1 = 0.      exyz2 = 0.      bxyz2 = 0.      call PBGUARD2X(bxye,nyp,nx,nxe,nypmx,nblok)      call PDGUARD2X(bze,nyp,nx,nxe,nypmx,nblok)c prepare fft tables      isign = 0c     call PFFT2R(qe,qt,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrt,nxec    1h,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX(qe,qt,isign,ntpose,mixup,sct,indx,indy,kstrt,nxeh,nyv     1,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c calculate form factors      call PPOIS23(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,j     1blok,nyh)c     call PPOIS22(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,jc    1blok,nyh)cc     call PFFT2R(q2,qt2,bs2,br2,isign,ntpose,mixup2,sct2,indx1,indy1,ksc    1trt,nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)      call PFFT2RX(q2,qt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,nxe     1,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)      call PPOISDX23(qt2,fxyt2,isign,ffd,ax,ay,affp,wf,nx,ny,kstrt,ny2,k     1xp2,j2blok,ny)c     call PPOISDX22(qt2,fxyt2,isign,ffd,ax,ay,affp,wf,nx,ny,kstrt,ny2,kc    1xp2,j2blok,ny)cc     call PFFT2R(q1,qt1,bs1,br1,isign,ntpose,mixup1,sct1,indx1,indy,kstc    1rt,nxe,nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)      call PFFT2RX(q1,qt1,isign,ntpose,mixup1,sct1,indx1,indy,kstrt,nxe,     1nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)      call PPOISMX23(qt1,fxyt1,isign,ffb,ax,ay,affp,we,nx,ny,kstrt,nyv,k     1xp2,j2blok,nyh)c      call GROPEN      call SETNPLT(4,irc)      call STPALIT(1)cc initialize density profile and velocity distribution      if (ipbc.eq.2) vdy = 0.c background electrons      do 180 l = 1, nblok      nps(l) = 1      npp(l) = 0  180 continuec     if (npxy.gt.0) call PISTR2(part,edges,npp,nps,vtx,vty,zero,zero,npc    1x,npy,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)      if (npxy.gt.0) call PISTR2H(part,edges,npp,nps,vtx,vty,vtz,zero,ze     1ro,zero,npx,npy,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)c beam electrons      do 190 l = 1, nblok      nps(l) = npp(l) + 1  190 continuec     if (npxyb.gt.0) call PISTR2(part,edges,npp,nps,vtdx,vtdy,vdx,vdy,nc    1pxb,npyb,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)      if (npxyb.gt.0) call PISTR2H(part,edges,npp,nps,vtdx,vtdy,vtdz,vdx     1,vdy,vdz,npxb,npyb,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)c calculate actual coordinates from guiding centers      call PGBDISTR2L(part,bxye(1,2,2,1),npp,noff,qbme,nx,ny,idimp,npmax     1,nblok,nxe,nypmx,ipbc)c     call PGBZDISTR2L(part,bze(2,2,1),npp,noff,qbme,nx,ny,idimp,npmax,nc    1blok,nxe,nypmx,ipbc)      call PRETARD2(part,npp,dth,nx,ny,idimp,npmax,nblok,ipbc)c move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc set ion background      if (ipbc.eq.1) then         call PSGUARD2(qe,nyp,qi0,nx,nxe,nypmx,kblok)         call PSCGUARD2(cu,nyp,zero,zero,zero,nx,nxe,nypmx,kblok)c        call PSCGUARD22(cu,nyp,zero,zero,nx,nxe,nypmx,kblok)      else if (ipbc.eq.2) then         call PLSGUARD2X(qe,kstrt,nvp,noff,nyp,qi0,nx,ny,1,1,nxe,nypmx,k     1blok)         call PLSCGUARD2X(cu,kstrt,nvp,noff,nyp,zero,zero,zero,nx,ny,1,1     1,nxe,nypmx,kblok)c        call PLSCGUARD22X(cu,kstrt,nvp,noff,nyp,zero,zero,nx,ny,1,1,nxec    1,nypmx,kblok)c        call PLSGUARD2(qe,kstrt,nvp,nyp,qi0,nx,1,1,nxe,nypmx,kblok)c        call PLSCGUARD2(cu,kstrt,nvp,nyp,zero,zero,zero,nx,1,1,nxe,nypmc    1x,kblok)c        call PLSCGUARD22(cu,kstrt,nvp,nyp,zero,zero,nx,1,1,nxe,nypmx,kbc    1lok)      else if (ipbc.eq.3) then         call PMSGUARD2(qe,nyp,qi0,nx,1,nxe,nypmx,kblok)         call PMSCGUARD2(cu,nyp,zero,zero,zero,nx,1,nxe,nypmx,kblok)c        call PMSCGUARD22(cu,nyp,zero,zero,nx,1,nxe,nypmx,kblok)      endifc deposit current      call PGJPOST2(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxe     1,nypmx,ipbc)c     call PGJPOST22(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxc    1e,nypmx,ipbc)c move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc deposit charge for initial distribution      call PGPOST2(part,qe,npp,noff,qme,idimp,npmax,nblok,nxe,nypmx)c add guard cells      if (ipbc.eq.1) then         call PAGUARD2X(qe,nyp,nx,nxe,nypmx,kblok)         call PACGUARD2X(cu,nyp,nx,nxe,nypmx,kblok)c        call PACGUARD22X(cu,nyp,nx,nxe,nypmx,kblok)         call PAGUARD2(qe,scr,kstrt,nvp,nxe,nypmx,nblok,kyp,kblok,ngds)         call PAGUARD2(cu,scr,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok,ngds     1)c        call PAGUARD2(cu,scr,kstrt,nvp,2*nxe,nypmx,nblok,kyp,kblok,ngdsc    1)      else if (ipbc.eq.2) then         call PLAGUARD2X(qe(2,1,1),nyp,nx-2,nxe,nypmx,kblok)         call PLACGUARD2X(cu(1,2,1,1),nyp,nx-2,nxe,nypmx,kblok)c        call PLACGUARD22X(cu(1,2,1,1),nyp,nx-2,nxe,nypmx,kblok)         call PLAGUARD2(qe,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)         call PLACGUARD2(cu,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)c        call PLACGUARD22(cu,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)         call PLAGUARDS2(qe,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)         call PLACGUARDS2(cu,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c        call PLACGUARDS22(cu,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)      else if (ipbc.eq.3) then         call PLAGUARD2X(qe(2,1,1),nyp,nx-2,nxe,nypmx,kblok)         call PLACGUARD2X(cu(1,2,1,1),nyp,nx-2,nxe,nypmx,kblok)c        call PLACGUARD22X(cu(1,2,1,1),nyp,nx-2,nxe,nypmx,kblok)         call PAGUARD2(qe,scr,kstrt,nvp,nxe,nypmx,nblok,kyp,kblok,ngds)         call PAGUARD2(cu,scr,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok,ngds     1)c        call PAGUARD2(cu,scr,kstrt,nvp,2*nxe,nypmx,nblok,kyp,kblok,ngdsc    1)      endif      call TIMERA(1,'init    ',time)      call TIMERA(-1,'main    ',time)cc * * * start main iteration loop * * *c  500 if (nloop.le.itime) go to 2000      if (kstrt.eq.1) write (6,991) itime      write (label,991) itime      call LOGNAME(label)cc dirichlet boundary conditions      if (psolve.eq.2) thencc     call PDBLSIN2D(qe(2,2,1),q2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kblok,kc    12blok)c transform charge to fourier space      isign = -1c     call PFFT2RX(q2,qt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,nxec    1,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)      call WPFSST2R(qe(2,2,1),qr,ds,dr,isign,ntpose,mixup,sct2,ttp,indx,     1indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxyh2)c solve for electric fieldsc     isign = -1c     call PDMFIELDD2(qt2,qr,nx,ny,kstrt,ny2,nyv,kxp2,j2blok)c     call PPOISD23(qr,fxyr,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp2c    1,j2blok,ny)c     isign = 0c     call PEMFIELDD2(fxyt2,fxyr,isign,nx,ny,kstrt,ny2,nyv,kxp2,j2blok)c     isign = -1c     call PPOISDX23(qt2,fxyt2,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,ny2,kc    1xp2,j2blok,ny)c     call PPOISDX22(qt2,fxyt2,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,ny2,kc    1xp2,j2blok,ny)c solve for electric fields      isign = -1      call PPOISD23(qr,fxyr,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp2     1,j2blok,ny)c     call PPOISD22(qr,fxyr,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp2c    1,j2blok,ny)c transform current to fourier spacec     call PDBLSIN2B(cu(1,2,2,1),cu2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kbloc    1k,k2blok)c     call PDBLSIN2C(cu(1,2,2,1),cu2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kbloc    1k,k2blok)      isign = -1c     call PFFT2RX3(cu2,cut2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,c    1nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call PFFT2RX2(cu2,cut2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,c    1nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)      call WPFCST2R3(cu(1,2,2,1),cur,ds,dr,isign,ntpose,mixup,sct2,ttp,i     1ndx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxy     2h2)c     call WPFCST2R2(cu(1,2,2,1),cur,ds,dr,isign,ntpose,mixup,sct2,ttp,ic    1ndx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxyc    2h2)c take transverse part of currentc     call PCMFIELDD2(cut2,cur,nx,ny,kstrt,ny2,nyv,kxp2,j2blok)c     call PCUPERPD2(cur,nx,ny,kstrt,nyv,kxp2,j2blok)c     call PCUPERPDX2(cut2,nx,ny,kstrt,ny2,kxp2,j2blok)c     call PCUPERP2(cut2,nx2,ny2,kstrt,ny2,kxp2,j2blok)c     call PCUPERPDX22(cut2,nx,ny,kstrt,ny2,kxp2,j2blok)c take transverse part of current      call PCUPERPD2(cur,nx,ny,kstrt,nyv,kxp2,j2blok)c     call PCUPERPD22(cur,nx,ny,kstrt,nyv,kxp2,j2blok)c calculate magnetic field in fourier spacec     isign = -1c     call PBPOISDX23(cut2,bxyt2,isign,ffd,ax,ay,affp,ci,wm,nx,ny,kstrt,c    1ny2,kxp2,j2blok,ny)c     call PBPOISP23(cut2,bxyt2,isign,ffd,ax,ay,affp,ci,wm,nx2,ny2,kstrtc    1,ny2,kxp2,j2blok,ny)c     call PBPOISDX22(cut2,bxyt2,bzt2,isign,ffd,ax,ay,affp,ci,wm,nx,ny,kc    1strt,ny2,kxp2,j2blok,ny)c     call PBPOISP22(cut2,bxyt2,bzt2,isign,ffd,ax,ay,affp,ci,wm,nx2,ny2,c    1kstrt,ny2,kxp2,j2blok,ny)c     wf = 0.c     if (itime.eq.0) thenc        call IPBPOISD23(cur,bxyr,ffd,ci,wm,nx,ny,kstrt,nyv,kxp2,j2blok,c    1ny)c        call IPBPOISDX23(cut2,bxyz2,ffd,ci,wm,nx,ny,kstrt,ny2,kxp2,j2blc    1ok,ny)c        call IPBPOISP23(cut2,bxyz2,ffd,ci,wm,nx2,ny2,kstrt,ny2,kxp2,j2bc    1lok,ny)c        wm = .25*wmc        wf = 0.c     elsec        call PMAXWELD2(exyr,bxyr,cur,ffd,affp,ci,dt,wf,wm,nx,ny,kstrt,nc    1yv,kxp2,j2blok,ny)c        call PMAXWELDX2(exyz2,bxyz2,cut2,ffd,affp,ci,dt,wf,wm,nx,ny,kstc    1rt,ny2,kxp2,j2blok,ny)c        call PMAXWEL2(exyz2,bxyz2,cut2,ffd,affp,ci,dt,wf,wm,nx2,ny2,kstc    1rt,ny2,kxp2,j2blok,ny)c        wf = .25*wfc        wm = .25*wmc     endifc calculate magnetic field in fourier spacec     isign = -1c     call PBPOISD23(cur,bxyd,isign,ffd,ax,ay,affp,ci,wm,nx,ny,kstrt,nyvc    1,kxp2,j2blok,ny)c     call PBPOISD22(cur,bxyd,bzd,isign,ffd,ax,ay,affp,ci,wm,nx,ny,kstrtc    1,nyv,kxp2,j2blok,ny)c     wf = 0.      if (itime.eq.0) then         call IPBPOISD23(cur,bxyr,ffd,ci,wm,nx,ny,kstrt,nyv,kxp2,j2blok,     1ny)         wf = 0.      else         call PMAXWELD2(exyr,bxyr,cur,ffd,affp,ci,dt,wf,wm,nx,ny,kstrt,n     1yv,kxp2,j2blok,ny)      endifc add longitudinal and transverse electric fieldsc     isign = 1c     call PEMFIELDD2(fxyt2,exyr,ffd,isign,nx,ny,kstrt,ny2,nyv,kxp2,j2blc    1ok,ny)c     call PEMFIELD2(fxyt2,exyz2,ffd,isign,nx2,ny2,kstrt,ny2,kxp2,j2blokc    1,ny)c add longitudinal and transverse electric fields      isign = 1      call PEMFIELDR2(fxyr,exyr,ffd,isign,nx,ny,kstrt,nyv,kxp2,j2blok,ny     1)c copy magnetic fieldc     isign = -1c     call PEMFIELDD2(bxyt2,bxyr,ffd,isign,nx,ny,kstrt,ny2,nyv,kxp2,j2blc    1ok,ny)c     call PEMFIELD2(bxyt2,bxyz2,ffd,isign,nx2,ny2,kstrt,ny2,kxp2,j2blokc    1,ny)c copy magnetic field      isign = -1      call PEMFIELDR2(bxyd,bxyr,ffd,isign,nx,ny,kstrt,nyv,kxp2,j2blok,ny     1)c transform force/charge to real spacec     isign = 1c     call PFFT2RX3(fxy2,fxyt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrc    1t,nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call PFFT2RX2(fxy2,fxyt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrc    1t,nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call PHAFDBL2B(fxyze(1,2,2,1),fxy2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,c    1kblok,k2blok)c     call PHAFDBL2C(fxye(1,2,2,1),fxy2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kc    1blok,k2blok)c     call PLBGUARD2(fxyze,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLBGUARD2X(fxyze,nyp,nx,nxe,nypmx,kblok)c     call PLCGUARD2(fxye,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLCGUARD2X(fxye,nyp,nx,nxe,nypmx,kblok)c transform force/charge to real space      isign = 1      call WPFCST2R3(fxyze(1,2,2,1),fxyr,ds,dr,isign,ntpose,mixup,sct2,t     1tp,indx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy     2,nxyh2)c     call WPFCST2R2(fxye(1,2,2,1),fxyr,ds,dr,isign,ntpose,mixup,sct2,ttc    1p,indx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,c    2nxyh2)      call PLBGUARD2(fxyze,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)      call PLBGUARD2X(fxyze,nyp,nx,nxe,nypmx,kblok)c     call PLCGUARD2(fxye,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLCGUARD2X(fxye,nyp,nx,nxe,nypmx,kblok)c transform magnetic field to real spacec     isign = 1c     call PFFT2RX3(bxy2,bxyt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrc    1t,nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call PFFT2RX(bz2,bzt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,nc    1xe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call PHAFDBL2B(bxye(1,2,2,1),bxy2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kc    1blok,k2blok)c     call PHAFDBL2D(bze(2,2,1),bz2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kblokc    1,k2blok)c     call PLBGUARD2(bxye,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLBGUARD2X(bxye,nyp,nx,nxe,nypmx,kblok)c     call PLDGUARD2(bze,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLDGUARD2X(bze,nyp,nx,nxe,nypmx,kblok)c transform magnetic field to real space      isign = 1      call WPFSCT2R3(bxye(1,2,2,1),bxyd,ds,dr,isign,ntpose,mixup,sct2,tt     1p,indx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,     2nxyh2)c     call WPFCCT2R(bze(2,2,1),bzd,ds,dr,isign,ntpose,mixup,sct2,ttp,indc    1x,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxyh2c    2)      call PLBGUARD2(bxye,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)      call PLBGUARD2X(bxye,nyp,nx,nxe,nypmx,kblok)c     call PLDGUARD2(bze,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLDGUARD2X(bze,nyp,nx,nxe,nypmx,kblok)c solve for potentialc     isign = 1c     call PPOISDX2(qt2,fxt2,fyt2,isign,ffd,ax,ay,affp,wg,nx,ny,kstrt,nyc    12,kxp2,j2blok,ny)c     call PFFT2RX(pot2,fxt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,c    1nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call PHAFDBL2D(pot(2,2,1),pot2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kbloc    1k,k2blok)c     call PLDGUARD2(pot,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLDGUARD2X(pot,nyp,nx,nxe,nypmx,kblok)c solve for potential      isign = 1      call PPOISD2(qr,fxd,fyd,isign,ffd,ax,ay,affp,wg,nx,ny,kstrt,nyv,kx     1p2,j2blok,ny)      call WPFSST2R(pot(2,2,1),fxd,ds,dr,isign,ntpose,mixup,sct2,ttp,ind     1x,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxyh2     2)      call PLDGUARD2(pot,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)      call PLDGUARD2X(pot,nyp,nx,nxe,nypmx,kblok)c display      do 630 l = 1, kblok      do 620 k = 1, nypmx-1      do 610 j = 1, nx+1      fx(j,k,l) = fxyze(1,j+1,k+1,l)      fy(j,k,l) = fxyze(2,j+1,k+1,l)      fz(j,k,l) = fxyze(3,j+1,k+1,l)c     fx(j,k,l) = fxye(1,j+1,k+1,l)c     fy(j,k,l) = fxye(2,j+1,k+1,l)c     fz(j,k,l) = 0.  610 continue  620 continue  630 continue      call PCONTUR(pot(2,2,1),g,lf,nvp,'BOUND POT',999,0,nx+1,ny,nxe,nyp     1mx,kblok,' ',6,irc)c     call PCARPET(pot(2,2,1),g,nvp,'BOUND POT',999,0,nx+1,ny,nxe,nypmx,c    1kblok,' ',64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fx,g,lf,nvp,'BOUND FX',999,0,nx+1,ny,nxe,nypmx,kblok,     1' ',6,irc)c     call PCARPET(fx,g,nvp,'BOUND FX',999,0,nx+1,ny,nxe,nypmx,kblok,' 'c    1,64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fy,g,lf,nvp,'BOUND FY',999,0,nx+1,ny,nxe,nypmx,kblok,     1' ',6,irc)c     call PCARPET(fy,g,nvp,'BOUND FY',999,0,nx+1,ny,nxe,nypmx,kblok,' 'c    1,64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fz,g,lf,nvp,'BOUND FZ',999,0,nx+1,ny,nxe,nypmx,kblok,     1' ',6,irc)c     call PCARPET(fz,g,nvp,'BOUND FZ',999,0,nx+1,ny,nxe,nypmx,kblok,' 'c    1,64,irc)      if (irc.eq.1) go to 3000cc semi-periodic boundary conditions      else if (psolve.eq.3) thenc      call PSGLSIN2D(qe(2,2,1),q1,nx,kyp,nxe,nypmx,kyp,kblok)c transform charge to fourier space      isign = -1      call PFFT2RX(q1,qt1,isign,ntpose,mixup1,sct1,indx1,indy,kstrt,nxe,     1nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)c solve for electric fields      isign = -1      call PDMFIELDM2(qt1,qr,nx,ny,kstrt,nyv,nyh,kxp2,j2blok)      call PPOISM23(qr,fxyr,isign,ffb,ax,ay,affp,we,nx,ny,kstrt,nyh,kxp2     1,j2blok,nyh)      isign = 0      call PEMFIELDM2(fxyt1,fxyr,ffb,isign,nx,ny,kstrt,nyv,nyh,kxp2,j2bl     1ok,nyh)c     isign = -1c     call PPOISMX23(qt1,fxyt1,isign,ffb,ax,ay,affp,we,nx,ny,kstrt,nyv,kc    1xp2,j2blok,nyh)c     call PPOISMX22(qt1,fxyt1,isign,ffb,ax,ay,affp,we,nx,ny,kstrt,nyv,kc    1xp2,j2blok,nyh)c transform current to fourier space      call PSGLSIN2B(cu(1,2,2,1),cu1,nx,kyp,nxe,nypmx,kyp,kblok)c     call PSGLSIN2C(cu(1,2,2,1),cu1,nx,kyp,nxe,nypmx,kyp,kblok)      isign = -1      call PFFT2RX3(cu1,cut1,isign,ntpose,mixup1,sct1,indx1,indy,kstrt,n     1xe,nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)c     call PFFT2RX2(cu1,cut1,isign,ntpose,mixup1,sct1,indx1,indy,kstrt,nc    1xe,nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)c take transverse part of current      call PCMFIELDM2(cut1,cur,nx,ny,kstrt,nyv,nyh,kxp2,j2blok)      call PCUPERPM2(cur,nx,ny,kstrt,nyh,kxp2,j2blok)c     call PCUPERPMX2(cut1,nx,ny,kstrt,nyv,kxp2,j2blok)c     call PCUPERP2(cut1,nx2,ny,kstrt,nyv,kxp2,j2blok)c     call PCUPERPMX22(cut1,nx,ny,kstrt,nyv,kxp2,j2blok)c calculate magnetic field in fourier spacec     isign = -1c     call PBPOISM23(cur,bxyr,isign,ffb,ax,ay,affp,ci,wm,nx,ny,kstrt,nyhc    1,kxp2,j2blok,nyh)c     call PBPOISMX23(cut1,bxyt1,isign,ffb,ax,ay,affp,ci,wm,nx,ny,kstrt,c    1nyv,kxp2,j2blok,nyh)c     call PBPOISP23(cut1,bxyt1,isign,ffb,ax,ay,affp,ci,wm,nx2,ny,kstrt,c    1nyv,kxp2,j2blok,nyh)c     call PBPOISMX22(cut1,bxyt1,bzt1,isign,ffb,ax,ay,affp,ci,wm,nx,ny,kc    1strt,nyv,kxp2,j2blok,nyh)c     call PBPOISP22(cut1,bxyt1,bzt1,isign,ffb,ax,ay,affp,ci,wm,nx2,ny,kc    1strt,nyv,kxp2,j2blok,nyh)c     wf = 0.      if (itime.eq.0) then         call IPBPOISM23(cur,bxyr,ffb,ci,wm,nx,ny,kstrt,nyh,kxp2,j2blok,     1nyh)c        call IPBPOISMX23(cut1,bxyz1,ffb,ci,wm,nx,ny,kstrt,nyv,kxp2,j2blc    1ok,nyh)c        call IPBPOISP23(cut1,bxyz1,ffb,ci,wm,nx2,ny,kstrt,nyv,kxp2,j2blc    1ok,nyh)c        wm = .5*wm         wf = 0.      else         call PMAXWELM2(exyr,bxyr,cur,ffb,affp,ci,dt,wf,wm,nx,ny,kstrt,n     1yh,kxp2,j2blok,nyh)c        call PMAXWELMX2(exyz1,bxyz1,cut1,ffb,affp,ci,dt,wf,wm,nx,ny,kstc    1rt,nyv,kxp2,j2blok,nyh)c        call PMAXWEL2(exyz1,bxyz1,cut1,ffb,affp,ci,dt,wf,wm,nx2,ny,kstrc    1t,nyv,kxp2,j2blok,nyh)c        wf = .5*wfc        wm = .5*wm      endifc add longitudinal and transverse electric fields      isign = 1      call PEMFIELDM2(fxyt1,exyr,ffb,isign,nx,ny,kstrt,nyv,nyh,kxp2,j2bl     1ok,nyh)c     call PEMFIELD2(fxyt1,exyz1,ffb,isign,nx2,ny,kstrt,nyv,kxp2,j2blok,c    1nyh)c copy magnetic field      isign = -1      call PEMFIELDM2(bxyt1,bxyr,ffb,isign,nx,ny,kstrt,nyv,nyh,kxp2,j2bl     1ok,nyh)c     call PEMFIELD2(bxyt1,bxyz1,ffb,isign,nx2,ny,kstrt,nyv,kxp2,j2blok,c    1nyh)c transform force/charge to real space      isign = 1      call PFFT2RX3(fxy1,fxyt1,isign,ntpose,mixup1,sct1,indx1,indy,kstrt     1,nxe,nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)c     call PFFT2RX2(fxy1,fxyt1,isign,ntpose,mixup1,sct1,indx1,indy,kstrtc    1,nxe,nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)      call PHAFSGL2B(fxyze(1,2,2,1),fxy1,nx,kyp,nxe,nypmx,kyp,kblok)c     call PHAFSGL2C(fxye(1,2,2,1),fxy1,nx,kyp,nxe,nypmx,kyp,kblok)      call PCGUARD2(fxyze,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok)      call PLBGUARD2X(fxyze,nyp,nx,nxe,nypmx,kblok)c     call PCGUARD2(fxye,kstrt,nvp,2*nxe,nypmx,nblok,kyp,kblok)c     call PLCGUARD2X(fxye,nyp,nx,nxe,nypmx,kblok)c transform magnetic field to real space      isign = 1      call PFFT2RX3(bxy1,bxyt1,isign,ntpose,mixup1,sct1,indx1,indy,kstrt     1,nxe,nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)c     call PFFT2RX(bz1,bzt1,isign,ntpose,mixup1,sct1,indx1,indy,kstrt,nxc    1e,nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)      call PHAFSGL2B(bxye(1,2,2,1),bxy1,nx,kyp,nxe,nypmx,kyp,kblok)c     call PHAFSGL2D(bze(2,2,1),bz1,nx,kyp,nxe,nypmx,kyp,kblok)      call PCGUARD2(bxye,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok)      call PLBGUARD2X(bxye,nyp,nx,nxe,nypmx,kblok)c     call PCGUARD2(bze,kstrt,nvp,nxe,nypmx,nblok,kyp,kblok)c     call PLDGUARD2X(bze,nyp,nx,nxe,nypmx,kblok)c solve for potential      isign = 1      call PPOISMX2(qt1,fxt1,fyt1,isign,ffb,ax,ay,affp,wg,nx,ny,kstrt,ny     1v,kxp2,j2blok,nyh)      call PFFT2RX(pot1,fxt1,isign,ntpose,mixup1,sct1,indx1,indy,kstrt,n     1xe,nyv,kxp2,kyp,kyp,j2blok,kblok,nxhy2,nxyh2)      call PHAFSGL2D(pot(2,2,1),pot1,nx,kyp,nxe,nypmx,kyp,kblok)      call PCGUARD2(pot,kstrt,nvp,nxe,nypmx,nblok,kyp,kblok)      call PLDGUARD2X(pot,nyp,nx,nxe,nypmx,kblok)c display      do 660 l = 1, kblok      do 650 k = 1, nypmx-1      do 640 j = 1, nx+1      fx(j,k,l) = fxyze(1,j+1,k+1,l)      fy(j,k,l) = fxyze(2,j+1,k+1,l)      fz(j,k,l) = fxyze(3,j+1,k+1,l)c     fx(j,k,l) = fxye(1,j+1,k+1,l)c     fy(j,k,l) = fxye(2,j+1,k+1,l)c     fz(j,k,l) = 0.  640 continue  650 continue  660 continue      call PCONTUR(pot(2,2,1),g,lf,nvp,'BOUND POT',999,0,nx+1,ny,nxe,nyp     1mx,kblok,' ',6,irc)c     call PCARPET(pot(2,2,1),g,nvp,'BOUND POT',999,0,nx+1,ny,nxe,nypmx,c    1kblok,' ',64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fx,g,lf,nvp,'BOUND FX',999,0,nx+1,ny,nxe,nypmx,kblok,     1' ',6,irc)c     call PCARPET(fx,g,nvp,'BOUND FX',999,0,nx+1,ny,nxe,nypmx,kblok,' 'c    1,64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fy,g,lf,nvp,'BOUND FY',999,0,nx+1,ny,nxe,nypmx,kblok,     1' ',6,irc)c     call PCARPET(fy,g,nvp,'BOUND FY',999,0,nx+1,ny,nxe,nypmx,kblok,' 'c    1,64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fz,g,lf,nvp,'BOUND FZ',999,0,nx+1,ny,nxe,nypmx,kblok,     1' ',6,irc)c     call PCARPET(fz,g,nvp,'BOUND FZ',999,0,nx+1,ny,nxe,nypmx,kblok,' 'c    1,64,irc)      if (irc.eq.1) go to 3000cc neumann boundary conditions      else if (psolve.eq.6) thenc      call PDBLCOS2D(qe(2,2,1),q2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kblok,k     12blok)c transform charge to fourier space      isign = -1      call PFFT2RX(q2,qt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,nxe     1,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call WPFSST2R(qe(2,2,1),qr,ds,dr,isign,ntpose,mixup,sct2,ttp,indx,c    1indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxyh2)c solve for electric fieldsc     isign = -1c     call PDMFIELDD2(qt2,qr,nx,ny,kstrt,ny2,nyv,kxp2,j2blok)c     call PPOISD23(qr,fxyr,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp2c    1,j2blok,ny)c     isign = 0c     call PEMFIELDD2(fxyt2,fxyr,isign,nx,ny,kstrt,ny2,nyv,kxp2,j2blok)      isign = -1c     call PPOIS23(qt2,fxyt2,isign,ffd,ax,ay,affp,we,nx2,ny2,kstrt,ny2,kc    1xp2,j2blok,ny)c     we = .25*we      call PPOISNX23(qt2,fxyt2,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,ny2,k     1xp2,j2blok,ny)c     call PPOISNX22(qt2,fxyt2,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,ny2,kc    1xp2,j2blok,ny)c solve for electric fieldsc     isign = -1c     call PPOISD23(qr,fxyr,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp2c    1,j2blok,ny)c     call PPOISD22(qr,fxyr,isign,ffd,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp2c    1,j2blok,ny)c transform current to fourier space      call PDBLCOS2B(cu(1,2,2,1),cu2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kblo     1k,k2blok)c     call PDBLCOS2C(cu(1,2,2,1),cu2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kbloc    1k,k2blok)      isign = -1      call PFFT2RX3(cu2,cut2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,     1nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call PFFT2RX2(cu2,cut2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,c    1nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call WPFCST2R3(cu(1,2,2,1),cur,ds,dr,isign,ntpose,mixup,sct2,ttp,ic    1ndx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxyc    2h2)c     call WPFCST2R2(cu(1,2,2,1),cur,ds,dr,isign,ntpose,mixup,sct2,ttp,ic    1ndx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxyc    2h2)c take transverse part of current      call PCMFIELDN2(cut2,cur,nx,ny,kstrt,ny2,nyv,kxp2,j2blok)      call PCUPERPN2(cur,nx,ny,kstrt,nyv,kxp2,j2blok)c     call PCUPERPNX2(cut2,nx,ny,kstrt,ny2,kxp2,j2blok)c     call PCUPERP2(cut2,nx2,ny2,kstrt,ny2,kxp2,j2blok)c     call PCUPERPNX22(cut2,nx,ny,kstrt,ny2,kxp2,j2blok)c take transverse part of currentc     call PCUPERPD2(cur,nx,ny,kstrt,nyv,kxp2,j2blok)c     call PCUPERPD22(cur,nx,ny,kstrt,nyv,kxp2,j2blok)c calculate magnetic field in fourier spacec     isign = -1c     call PBPOISN23(cur,bxyd,isign,ffd,ax,ay,affp,ci,wm,nx,ny,kstrt,nyvc    1,kxp2,j2blok,ny)c     call PBPOISNX23(cut2,bxyt2,isign,ffd,ax,ay,affp,ci,wm,nx,ny,kstrt,c    1ny2,kxp2,j2blok,ny)c     call PBPOISP23(cut2,bxyt2,isign,ffd,ax,ay,affp,ci,wm,nx2,ny2,kstrtc    1,ny2,kxp2,j2blok,ny)c     wm = .25*wmc     call PBPOISNX22(cut2,bxyt2,bzt2,isign,ffd,ax,ay,affp,ci,wm,nx,ny,kc    1strt,ny2,kxp2,j2blok,ny)c     call PBPOISP22(cut2,bxyt2,bzt2,isign,ffd,ax,ay,affp,ci,wm,nx2,ny2,c    1kstrt,ny2,kxp2,j2blok,ny)c     wm = .25*wmc     wf = 0.      if (itime.eq.0) then         call IPBPOISN23(cur,bxyr,ffd,ci,wm,nx,ny,kstrt,nyv,kxp2,j2blok,     1ny)c        call IPBPOISNX23(cut2,bxyz2,ffd,ci,wm,nx,ny,kstrt,ny2,kxp2,j2blc    1ok,ny)c        call IPBPOISP23(cut2,bxyz2,ffd,ci,wm,nx2,ny2,kstrt,ny2,kxp2,j2bc    1lok,ny)c        wm = .25*wm         wf = 0.      else         call PMAXWELN2(exyr,bxyr,cur,ffd,affp,ci,dt,wf,wm,nx,ny,kstrt,n     1yv,kxp2,j2blok,ny)c        call PMAXWELNX2(exyz2,bxyz2,cut2,ffd,affp,ci,dt,wf,wm,nx,ny,kstc    1rt,ny2,kxp2,j2blok,ny)c        call PMAXWEL2(exyz2,bxyz2,cut2,ffd,affp,ci,dt,wf,wm,nx2,ny2,kstc    1rt,ny2,kxp2,j2blok,ny)c        wf = .25*wfc        wm = .25*wm      endifc calculate magnetic field in fourier spacec     isign = -1c     call PBPOISD23(cur,bxyd,isign,ffd,ax,ay,affp,ci,wm,nx,ny,kstrt,nyvc    1,kxp2,j2blok,ny)c     call PBPOISD22(cur,bxyd,bzd,isign,ffd,ax,ay,affp,ci,wm,nx,ny,kstrtc    1,nyv,kxp2,j2blok,ny)c     wf = 0.c     if (itime.eq.0) thenc        call IPBPOISD23(cur,bxyr,ffd,ci,wm,nx,ny,kstrt,nyv,kxp2,j2blok,c    1ny)c        wf = 0.c     elsec        call PMAXWELD2(exyr,bxyr,cur,ffd,affp,ci,dt,wf,wm,nx,ny,kstrt,nc    1yv,kxp2,j2blok,ny)c     endifc add longitudinal and transverse electric fields      isign = 1      call PEMFIELDN2(fxyt2,exyr,ffd,isign,nx,ny,kstrt,ny2,nyv,kxp2,j2bl     1ok,ny)c     call PEMFIELD2(fxyt2,exyz2,ffd,isign,nx2,ny2,kstrt,ny2,kxp2,j2blokc    1,ny)c add longitudinal and transverse electric fieldsc     isign = 1c     call PEMFIELDR2(fxyr,exyr,ffd,isign,nx,ny,kstrt,nyv,kxp2,j2blok,nyc    1)c copy magnetic field      isign = -1      call PEMFIELDN2(bxyt2,bxyr,ffd,isign,nx,ny,kstrt,ny2,nyv,kxp2,j2bl     1ok,ny)c     call PEMFIELD2(bxyt2,bxyz2,ffd,isign,nx2,ny2,kstrt,ny2,kxp2,j2blokc    1,ny)c copy magnetic fieldc     isign = -1c     call PEMFIELDR2(bxyd,bxyr,ffd,isign,nx,ny,kstrt,nyv,kxp2,j2blok,nyc    1)c transform force/charge to real space      isign = 1      call PFFT2RX3(fxy2,fxyt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstr     1t,nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call PFFT2RX2(fxy2,fxyt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrc    1t,nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)      call PHAFDBL2B(fxyze(1,2,2,1),fxy2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,     1kblok,k2blok)c     call PHAFDBL2C(fxye(1,2,2,1),fxy2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kc    1blok,k2blok)      call PLBGUARD2(fxyze,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)      call PLBGUARD2X(fxyze,nyp,nx,nxe,nypmx,kblok)c     call PLCGUARD2(fxye,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLCGUARD2X(fxye,nyp,nx,nxe,nypmx,kblok)c transform force/charge to real spacec     isign = 1c     call WPFCST2R3(fxyze(1,2,2,1),fxyr,ds,dr,isign,ntpose,mixup,sct2,tc    1tp,indx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhyc    2,nxyh2)c     call WPFCST2R2(fxye(1,2,2,1),fxyr,ds,dr,isign,ntpose,mixup,sct2,ttc    1p,indx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,c    2nxyh2)c     call PLBGUARD2(fxyze,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLBGUARD2X(fxyze,nyp,nx,nxe,nypmx,kblok)c     call PLCGUARD2(fxye,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLCGUARD2X(fxye,nyp,nx,nxe,nypmx,kblok)c transform magnetic field to real space      isign = 1      call PFFT2RX3(bxy2,bxyt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstr     1t,nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)c     call PFFT2RX(bz2,bzt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,nc    1xe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)      call PHAFDBL2B(bxye(1,2,2,1),bxy2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,k     1blok,k2blok)c     call PHAFDBL2D(bze(2,2,1),bz2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kblokc    1,k2blok)      call PLBGUARD2(bxye,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)      call PLBGUARD2X(bxye,nyp,nx,nxe,nypmx,kblok)c     call PLDGUARD2(bze,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLDGUARD2X(bze,nyp,nx,nxe,nypmx,kblok)c transform magnetic field to real spacec     isign = 1c     call WPFSCT2R3(bxye(1,2,2,1),bxyd,ds,dr,isign,ntpose,mixup,sct2,ttc    1p,indx,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,c    2nxyh2)c     call WPFCCT2R(bze(2,2,1),bzd,ds,dr,isign,ntpose,mixup,sct2,ttp,indc    1x,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxyh2c    2)c     call PLBGUARD2(bxye,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLBGUARD2X(bxye,nyp,nx,nxe,nypmx,kblok)c     call PLDGUARD2(bze,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLDGUARD2X(bze,nyp,nx,nxe,nypmx,kblok)c solve for potential      isign = 1c     call PPOISP2(qt2,fxt2,fyt2,isign,ffd,ax,ay,affp,wg,nx2,ny2,kstrt,nc    1y2,kxp2,j2blok,ny)c     wg = .25*wg      call PPOISNX2(qt2,fxt2,fyt2,isign,ffd,ax,ay,affp,wg,nx,ny,kstrt,ny     12,kxp2,j2blok,ny)      call PFFT2RX(pot2,fxt2,isign,ntpose,mixup2,sct2,indx1,indy1,kstrt,     1nxe,ny2,kxp2,kyp2,kyp2,j2blok,k2blok,nxhy2,nxyh2)      call PHAFDBL2D(pot(2,2,1),pot2,nx,ny,kstrt,nxe,kyp,nypmx,kyp2,kblo     1k,k2blok)      call PLDGUARD2(pot,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)      call PLDGUARD2X(pot,nyp,nx,nxe,nypmx,kblok)c solve for potentialc     isign = 1c     call PPOISD2(qr,fxd,fyd,isign,ffd,ax,ay,affp,wg,nx,ny,kstrt,nyv,kxc    1p2,j2blok,ny)c     call WPFSST2R(pot(2,2,1),fxd,ds,dr,isign,ntpose,mixup,sct2,ttp,indc    1x,indy,kstrt,nxeh,nyv,kxp2,kyp,nypmx,kxp21,j2blok,kblok,nxhy,nxyh2c    2)c     call PLDGUARD2(pot,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c     call PLDGUARD2X(pot,nyp,nx,nxe,nypmx,kblok)c display      do 730 l = 1, kblok      do 720 k = 1, nypmx-1      do 710 j = 1, nx+1      fx(j,k,l) = fxyze(1,j+1,k+1,l)      fy(j,k,l) = fxyze(2,j+1,k+1,l)      fz(j,k,l) = fxyze(3,j+1,k+1,l)c     fx(j,k,l) = fxye(1,j+1,k+1,l)c     fy(j,k,l) = fxye(2,j+1,k+1,l)c     fz(j,k,l) = 0.  710 continue  720 continue  730 continue      call PCONTUR(pot(2,2,1),g,lf,nvp,'BOUND POT',999,0,nx+1,ny,nxe,nyp     1mx,kblok,' ',6,irc)c     call PCARPET(pot(2,2,1),g,nvp,'BOUND POT',999,0,nx+1,ny,nxe,nypmx,c    1kblok,' ',64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fx,g,lf,nvp,'BOUND FX',999,0,nx+1,ny,nxe,nypmx,kblok,     1' ',6,irc)c     call PCARPET(fx,g,nvp,'BOUND FX',999,0,nx+1,ny,nxe,nypmx,kblok,' 'c    1,64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fy,g,lf,nvp,'BOUND FY',999,0,nx+1,ny,nxe,nypmx,kblok,     1' ',6,irc)c     call PCARPET(fy,g,nvp,'BOUND FY',999,0,nx+1,ny,nxe,nypmx,kblok,' 'c    1,64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fz,g,lf,nvp,'BOUND FZ',999,0,nx+1,ny,nxe,nypmx,kblok,     1' ',6,irc)c     call PCARPET(fz,g,nvp,'BOUND FZ',999,0,nx+1,ny,nxe,nypmx,kblok,' 'c    1,64,irc)      if (irc.eq.1) go to 3000cc periodic boundary conditions      else if (psolve.eq.1) thencc transform charge to fourier space      isign = -1c     call PFFT2R(qe(2,2,1),qt,bs,br,isign,ntpose,mixup,sct,indx,indy,ksc    1trt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX(qe(2,2,1),qt,isign,ntpose,mixup,sct,indx,indy,kstrt,n     1xeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c calculate force/charge in fourier space      isign = -1      call PPOIS23(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,j     1blok,nyh)c     call PPOIS22(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,jc    1blok,nyh)c transform current to fourier space      isign = -1c     call PFFT2R3(cu(1,2,2,1),cut,bs,br,isign,ntpose,mixup,sct,indx,indc    1y,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX3(cu(1,2,2,1),cut,isign,ntpose,mixup,sct,indx,indy,kst     1rt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2R2(cu(1,2,2,1),cut,bs,br,isign,ntpose,mixup,sct,indx,indc    1y,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2RX2(cu(1,2,2,1),cut,isign,ntpose,mixup,sct,indx,indy,kstc    1rt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c take transverse part of current      call PCUPERP2(cut,nx,ny,kstrt,nyv,kxp,jblok)c     call PCUPERP22(cut,nx,ny,kstrt,nyv,kxp,jblok)c calculate magnetic field in fourier spacec     isign = -1c     call PBPOISP23(cut,bxyt,isign,ffc,ax,ay,affp,ci,wm,nx,ny,kstrt,nyvc    1,kxp,jblok,nyh)c     call PBPOISP22(cut,bxyt,bzt,isign,ffc,ax,ay,affp,ci,wm,nx,ny,kstrtc    1,nyv,kxp,jblok,nyh)c     wf = 0.      if (itime.eq.0) then         call IPBPOISP23(cut,bxyz,ffc,ci,wm,nx,ny,kstrt,nyv,kxp,jblok,ny     1h)         wf = 0.      else         call PMAXWEL2(exyz,bxyz,cut,ffc,affp,ci,dt,wf,wm,nx,ny,kstrt,ny     1v,kxp,jblok,nyh)      endifc add longitudinal and transverse electric fields      isign = 1      call PEMFIELD2(fxyt,exyz,ffc,isign,nx,ny,kstrt,nyv,kxp,jblok,nyh)c copy magnetic field      isign = -1      call PEMFIELD2(bxyt,bxyz,ffc,isign,nx,ny,kstrt,nyv,kxp,jblok,nyh)c transform force/charge to real space      isign = 1c     call PFFT2R3(fxyze(1,2,2,1),fxyt,bs,br,isign,ntpose,mixup,sct,indxc    1,indy,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX3(fxyze(1,2,2,1),fxyt,isign,ntpose,mixup,sct,indx,indy     1,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2R2(fxye(1,2,2,1),fxyt,bs,br,isign,ntpose,mixup,sct,indx,c    1indy,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2RX2(fxye(1,2,2,1),fxyt,isign,ntpose,mixup,sct,indx,indy,c    1kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c copy data from field to particle partition, and copy to guard cells      call PCGUARD2(fxyze,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok)      call PBGUARD2X(fxyze,nyp,nx,nxe,nypmx,nblok)c     call PCGUARD2(fxye,kstrt,nvp,2*nxe,nypmx,nblok,kyp,kblok)c     call PCGUARD2X(fxye,nyp,nx,nxe,nypmx,nblok)c transform magnetic field to real space      isign = 1c     call PFFT2R3(bxye(1,2,2,1),bxyt,bs,br,isign,ntpose,mixup,sct,indx,c    1indy,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX3(bxye(1,2,2,1),bxyt,isign,ntpose,mixup,sct,indx,indy,     1kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2R(bze(2,2,1),bzt,bs,br,isign,ntpose,mixup,sct,indx,indy,c    1kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2RX(bze(2,2,1),bzt,isign,ntpose,mixup,sct,indx,indy,kstrtc    1,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c copy data from field to particle partition, and copy to guard cells      call PCGUARD2(bxye,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok)      call PBGUARD2X(bxye,nyp,nx,nxe,nypmx,nblok)c     call PCGUARD2(bze,kstrt,nvp,nxe,nypmx,nblok,kyp,kblok)c     call PDGUARD2X(bze,nyp,nx,nxe,nypmx,nblok)c display      do 690 l = 1, kblok      do 680 k = 1, nypmx-1      do 670 j = 1, nx+1      fx(j,k,l) = fxyze(1,j+1,k+1,l)      fy(j,k,l) = fxyze(2,j+1,k+1,l)      fz(j,k,l) = fxyze(3,j+1,k+1,l)c     fx(j,k,l) = fxye(1,j+1,k+1,l)c     fy(j,k,l) = fxye(2,j+1,k+1,l)c     fz(j,k,l) = 0.  670 continue  680 continue  690 continue      call PCONTUR(fx,g,lf,nvp,'PERIOD FX',999,0,nx+1,ny,nxe,nypmx,kblok     1,' ',6,irc)c     call PCARPET(fx,g,nvp,'PERIOD FX',999,0,nx+1,ny,nxe,nypmx,kblok,' c   1',64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fy,g,lf,nvp,'PERIOD FY',999,0,nx+1,ny,nxe,nypmx,kblok     1,' ',6,irc)c     call PCARPET(fy,g,nvp,'PERIOD FY',999,0,nx+1,ny,nxe,nypmx,kblok,' c    1',64,irc)      if (irc.eq.1) go to 3000      call PCONTUR(fz,g,lf,nvp,'PERIOD FZ',999,0,nx+1,ny,nxe,nypmx,kblok     1,' ',6,irc)c     call PCARPET(fz,g,nvp,'PERIOD FZ',999,0,nx+1,ny,nxe,nypmx,kblok,' c    1',64,irc)      if (irc.eq.1) go to 3000c solve for potential      isign = 1      call PPOISP2(qt,fxt,fyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kx     1p,jblok,nyh)      call PFFT2RX(pot(2,2,1),fxt,isign,ntpose,mixup,sct,indx,indy,kstrt     1,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PCGUARD2(pot,kstrt,nvp,nxe,nypmx,nblok,kyp,kblok)      call PDGUARD2X(pot,nyp,nx,nxe,nypmx,nblok)      call PCONTUR(pot(2,2,1),g,lf,nvp,'PERIOD POT',999,0,nx+1,ny,nxe,ny     1pmx,kblok,' ',6,irc)c     call PCARPET(pot(2,2,1),g,nvp,'PERIOD POT',999,0,nx+1,ny,nxe,nypmxc    1,kblok,' ',64,irc)      if (irc.eq.1) go to 3000      endifcc particle push and charge density updatec     call timera(-1,'push    ',time)      wke = 0.c push particles      call PGBPUSH23(part,fxyze,bxye,npp,noff,qbme,dt,dth,wke,nx,ny,idim     1p,npmax,nblok,nxe,nypmx,ipbc)c     call PGSBPUSH23(part,fxyze,bxye,npp,noff,qbme,dt,dth,wke,nx,ny,idic    1mp,npmax,nblok,nxe,nxeyp,ipbc)c     call PGBPUSH22(part,fxye,bze,npp,noff,qbme,dt,dth,wke,nx,ny,idimp,c    1npmax,nblok,nxe,nypmx,ipbc)c     call PGSBPUSH22(part,fxye,bze,npp,noff,qbme,dt,dth,wke,nx,ny,idimpc    1,npmax,nblok,nxe,nxeyp,ipbc)c move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c     call pxmov2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,c    1jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc     call timera(1,'push    ',time)c set ion background      if (ipbc.eq.1) then         call PSGUARD2(qe,nyp,qi0,nx,nxe,nypmx,kblok)         call PSCGUARD2(cu,nyp,zero,zero,zero,nx,nxe,nypmx,kblok)c        call PSCGUARD22(cu,nyp,zero,zero,nx,nxe,nypmx,kblok)      else if (ipbc.eq.2) then         call PLSGUARD2X(qe,kstrt,nvp,noff,nyp,qi0,nx,ny,1,1,nxe,nypmx,k     1blok)         call PLSCGUARD2X(cu,kstrt,nvp,noff,nyp,zero,zero,zero,nx,ny,1,1     1,nxe,nypmx,kblok)c        call PLSCGUARD22X(cu,kstrt,nvp,noff,nyp,zero,zero,nx,ny,1,1,nxec    1,nypmx,kblok)c        call PLSGUARD2(qe,kstrt,nvp,nyp,qi0,nx,1,1,nxe,nypmx,kblok)c        call PLSCGUARD2(cu,kstrt,nvp,nyp,zero,zero,zero,nx,1,1,nxe,nypmc    1x,kblok)c        call PLSCGUARD22(cu,kstrt,nvp,nyp,zero,zero,nx,1,1,nxe,nypmx,kbc    1lok)      else if (ipbc.eq.3) then         call PMSGUARD2(qe,nyp,qi0,nx,1,nxe,nypmx,kblok)         call PMSCGUARD2(cu,nyp,zero,zero,zero,nx,1,nxe,nypmx,kblok)c        call PMSCGUARD22(cu,nyp,zero,zero,nx,1,nxe,nypmx,kblok)      endifc deposit current      call PGJPOST2(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxe     1,nypmx,ipbc)c     call PGSJPOST2(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxc    1e,nxeyp,ipbc)c     call PGSJOST2X(part,cu,npp,noff,nn,amxy,qme,dth,nx,ny,idimp,npmax,c    1nblok,nxe,nxeyp,npd,n27,ipbc)c     call PGJPOST22(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxc    1e,nypmx,ipbc)c     call PGSJPOST22(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nc    1xe,nxeyp,ipbc)c     call PGSJOST22X(part,cu,npp,noff,nn,amxy,qme,dth,nx,ny,idimp,npmaxc    1,nblok,nxe,nxeyp,npd,n18,ipbc)c move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c     call pxmov2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,c    1jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc deposit chargec     call PGPOST2(part,qe,npp,noff,qme,idimp,npmax,nblok,nxe,nypmx)      call PGSPOST2(part,qe,npp,noff,qme,idimp,npmax,nblok,nxe,nxeyp)c     call PGSOST2X(part,qe,npp,noff,nn,amxy,qme,idimp,npmax,nblok,nxe,nc    1xeyp,npd,nine)c add guard cells      if (ipbc.eq.1) then         call PAGUARD2X(qe,nyp,nx,nxe,nypmx,kblok)         call PACGUARD2X(cu,nyp,nx,nxe,nypmx,kblok)c        call PACGUARD22X(cu,nyp,nx,nxe,nypmx,kblok)         call PAGUARD2(qe,scr,kstrt,nvp,nxe,nypmx,nblok,kyp,kblok,ngds)         call PAGUARD2(cu,scr,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok,ngds     1)c        call PAGUARD2(cu,scr,kstrt,nvp,2*nxe,nypmx,nblok,kyp,kblok,ngdsc    1)      else if (ipbc.eq.2) then         call PLAGUARD2X(qe(2,1,1),nyp,nx-2,nxe,nypmx,kblok)         call PLACGUARD2X(cu(1,2,1,1),nyp,nx-2,nxe,nypmx,kblok)c        call PLACGUARD22X(cu(1,2,1,1),nyp,nx-2,nxe,nypmx,kblok)         call PLAGUARD2(qe,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)         call PLACGUARD2(cu,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)c        call PLACGUARD22(cu,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)         call PLAGUARDS2(qe,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)         call PLACGUARDS2(cu,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)c        call PLACGUARDS22(cu,kstrt,nvp,nx,nxe,nypmx,kyp,kblok)      else if (ipbc.eq.3) then         call PLAGUARD2X(qe(2,1,1),nyp,nx-2,nxe,nypmx,kblok)         call PLACGUARD2X(cu(1,2,1,1),nyp,nx-2,nxe,nypmx,kblok)c        call PLACGUARD22X(cu(1,2,1,1),nyp,nx-2,nxe,nypmx,kblok)         call PAGUARD2(qe,scr,kstrt,nvp,nxe,nypmx,nblok,kyp,kblok,ngds)         call PAGUARD2(cu,scr,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok,ngds     1)c        call PAGUARD2(cu,scr,kstrt,nvp,2*nxe,nypmx,nblok,kyp,kblok,ngdsc    1)      endifc sort particles      if (mod(itime,50).eq.0) then         call PSORTP2Y(part,pt,ip,npic,npp,noff,nyp,idimp,npmax,nblok,ny     1pm1)      endif c energy diagnostic      wef = we + wf + wm      wtot(1) = wef      wtot(2) = wke      wtot(3) = wef + wke      wtot(4) = we      wtot(5) = wf      wtot(6) = wm      call PSUM(wtot,work,6,1)      if (kstrt.eq.1) write (6,993) wtot(1), wtot(2), wtot(3)      if (kstrt.eq.1) write (6,994) wtot(4), wtot(5), wtot(6)      itime = itime + 1      go to 500 2000 continuecc * * * end main iteration loop * * *c      if (kstrt.eq.1) write (6,992) 3000 continue      call GRCLOSE      call TIMERA(1,'main    ',time)      call ppexit      stop      endc-----------------------------------------------------------------------      subroutine ppinit(idproc,nvp)c this subroutine initializes parallel processingc input: nvp, output: idprocc idproc = processor idc nvp = number of real or virtual processors requested      implicit none      integer idproc, nvpc get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for realsc mint = default datatype for integersc mcplx = default datatype for complex type      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ierror, ndprec      save /pparms/c ndprec = (0,1) = (no,yes) use (normal,autodouble) precision      data ndprec /1/c this segment is used for shared memory computersc     nproc = nvpc     idproc = 0c this segment is used for mpi computers      if (MPI_STATUS_SIZE.gt.lstat) then         write (2,*) ' status size too small, actual/required = ', lstat     1, MPI_STATUS_SIZE         stop      endifc initialize the MPI execution environment      call MPI_INIT(ierror)      if (ierror.ne.0) stop      lgrp = MPI_COMM_WORLDc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lgrp,idproc,ierror)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lgrp,nproc,ierror)c set default datatypes         mint = MPI_INTEGERc single precision      if (ndprec.eq.0) then         mreal = MPI_REAL         mcplx = MPI_COMPLEXc double precision      else         mreal = MPI_DOUBLE_PRECISION         mcplx = MPI_DOUBLE_COMPLEX      endifc requested number of processors not obtained      if (nproc.ne.nvp) then         write (2,*) ' processor number error: nvp, nproc=', nvp, nproc         call ppexit         stop      endif      return      endc-----------------------------------------------------------------------      subroutine ppexitc this subroutine terminates parallel processing      implicit nonec common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplxc lgrp = current communicator      common /pparms/ nproc, lgrp, mreal, mint, mcplx      integer ierrorc synchronize processes      call MPI_BARRIER(lgrp,ierror)c terminate MPI execution environment      call MPI_FINALIZE(ierror)      return      endc-----------------------------------------------------------------------      subroutine dcomp2(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c this subroutine determines spatial boundaries for particlec decomposition, calculates number of grid points in each spatialc region, and the offset of these grid points from the global addressc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.c ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idps = number of partition boundariesc nblok = number of particle partitions.      implicit none      real edges      integer nyp, noff, ny, kstrt, nvp, idps, nblok      dimension edges(idps,nblok)      dimension nyp(nblok), noff(nblok)c local data      integer ks, kb, kr, l      real at1      ks = kstrt - 2      at1 = float(ny)/float(nvp)      do 10 l = 1, nblok      kb = l + ks      edges(1,l) = at1*float(kb)      noff(l) = edges(1,l) + .5      edges(2,l) = at1*float(kb + 1)      kr = edges(2,l) + .5      nyp(l) = kr - noff(l)   10 continue      return      endc-----------------------------------------------------------------------      subroutine PISTR2H(part,edges,npp,nps,vtx,vty,vtz,vdx,vdy,vdz,npx,     1npy,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)c for 2-1/2d code, this subroutine calculates initial particlec co-ordinates and velocities with uniform density and maxwellianc velocity with drift for distributed data.c part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc part(5,n,l) = velocity vz of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition lc vtx/vty/vtz = thermal velocity of electrons in x/y/z directionc vdx/vdy/vdz = drift velocity of beam electrons in x/y/z directionc npx/npy = initial number of particles distributed in x/y directionc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)c ierr = (0,1) = (no,yes) error condition existsc ranorm = gaussian random number with zero mean and unit variancec with spatial decomposition      double precision ranorm      double precision sum0, sum1, sum2      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok), nps(nblok)      dimension sum3(3), isum2(2)      dimension work3(3), iwork2(2)      ierr = 0c set boundary values      edgelx = 0.      edgely = 0.      at1 = float(nx)/float(npx)      at2 = float(ny)/float(npy)      if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         at1 = float(nx-2)/float(npx)         at2 = float(ny-2)/float(npy)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         at1 = float(nx-2)/float(npx)         at2 = float(ny)/float(npy)      endif      do 30 k = 1, npy      yt = edgely + at2*(float(k) - .5)      do 20 j = 1, npxc uniform density profile      xt = edgelx + at1*(float(j) - .5)c maxwellian velocity distribution      vxt = vtx*ranorm()      vyt = vty*ranorm()      vzt = vtz*ranorm()      do 10 l = 1, nblok      if ((yt.ge.edges(1,l)).and.(yt.lt.edges(2,l))) then         npt = npp(l) + 1         if (npt.le.npmax) then            part(1,npt,l) = xt            part(2,npt,l) = yt            part(3,npt,l) = vxt            part(4,npt,l) = vyt            part(5,npt,l) = vzt            npp(l) = npt         else            ierr = ierr + 1         endif      endif   10 continue   20 continue   30 continue      npxy = 0c add correct drift      sum3(1) = 0.      sum3(2) = 0.      sum3(3) = 0.      do 50 l = 1, nblok      sum0 = 0.0d0      sum1 = 0.0d0      sum2 = 0.0d0      do 40 j = nps(l), npp(l)      npxy = npxy + 1      sum0 = sum0 + part(3,j,l)      sum1 = sum1 + part(4,j,l)      sum2 = sum2 + part(5,j,l)   40 continue      sum3(1) = sum3(1) + sum0      sum3(2) = sum3(2) + sum1      sum3(3) = sum3(3) + sum2   50 continue      isum2(1) = ierr      isum2(2) = npxy      call PISUM(isum2,iwork2,2,1)      ierr = isum2(1)      npxy = isum2(2)      call PSUM(sum3,work3,3,1)      at1 = 1./float(npxy)      sum3(1) = at1*sum3(1) - vdx      sum3(2) = at1*sum3(2) - vdy      sum3(3) = at1*sum3(3) - vdz      do 70 l = 1, nblok      do 60 j = nps(l), npp(l)      part(3,j,l) = part(3,j,l) - sum3(1)      part(4,j,l) = part(4,j,l) - sum3(2)      part(5,j,l) = part(5,j,l) - sum3(3)   60 continue   70 continuec process errors      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr      else if (npxy.ne.(npx*npy)) then         write (2,*) 'particle distribution truncated, np = ', npxy      endif      return      endc-----------------------------------------------------------------------      subroutine PGBDISTR2L(part,bxy,npp,noff,qbm,nx,ny,idimp,npmax,nblo     1k,nxv,nypmx,ipbc)c for 2-1/2d code, this subroutine reinterprets curent particlec positions as positions of guiding centers, and calculates the actualc particle positions for distributed datac in converting from guiding center to actual co-ordinates,c the following equations are used:c       x(t) = xg(t) - (vy(t)*omz - vz(t)*omy)/om**2c       y(t) = yg(t) - (vz(t)*omx - vx(t)*omz)/om**2c where omx = (q/m)*bxyz(1,xg(t),yg(t)),c       omy = (q/m)*bxyz(2,xg(t),yg(t)),c and   omz = (q/m)*bxyz(2,xg(t),yg(t)),c and the magnetic field components bxyz(i,x(t),y(t)) are approximatedc by interpolation from the nearest grid points:c bxy(i,x,y) = (1-dy)*((1-dx)*bxy(i,n,m)+dx*bxy(i,n+1,m)) + c               dy*((1-dx)*bxy(i,n,m+1) + dx*bxy(i,n+1.m+1))c where n,m = leftmost grid points and dx = x-n, dy = y-mc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc part(5,n,l) = velocity vz of particle n in partition lc bxyz(i,1,j,k,l) = i component of magnetic field at grid (j,kk)c that is, the convolution of magnetic field over particle shapec where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c qbm = particle charge/mass ratioc nx/ny = system length in x/y directionc idimp = size of phase space = 5c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nxv = first dimension of field arrays, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)      dimension part(idimp,npmax,nblok)      dimension bxy(3,nxv,nypmx,nblok)      dimension npp(nblok), noff(nblok)c set boundary values      if (ipbc.eq.1) then         edgelx = 0.         edgely = 0.         edgerx = float(nx)         edgery = float(ny)      else if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         edgerx = float(nx-1)         edgery = float(ny-1)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         edgerx = float(nx-1)         edgery = float(ny)      endifc calculate actual position from guiding center      do 20 l = 1, nblok      mnoff = noff(l) - 1      do 10 j = 1, npp(l)c find interpolation weights      nn = part(1,j,l)      mm = part(2,j,l)      dxp = qbm*(part(1,j,l) - float(nn))      dyp = part(2,j,l) - float(mm)      nn = nn + 1      mm = mm - mnoff      amx = qbm - dxp      mp = mm + 1      amy = 1. - dyp      np = nn + 1c find magnetic field      omx = dyp*(dxp*bxy(1,np,mp,l) + amx*bxy(1,nn,mp,l)) + amy*(dxp*bxy     1(1,np,mm,l) + amx*bxy(1,nn,mm,l))      omy = dyp*(dxp*bxy(2,np,mp,l) + amx*bxy(2,nn,mp,l)) + amy*(dxp*bxy     1(2,np,mm,l) + amx*bxy(2,nn,mm,l))      omz = dyp*(dxp*bxy(3,np,mp,l) + amx*bxy(3,nn,mp,l)) + amy*(dxp*bxy     1(3,np,mm,l) + amx*bxy(3,nn,mm,l))      at3 = sqrt(omx*omx + omy*omy + omz*omz)      if (at3.ne.0.) at3 = 1./at3      at3 = at3*at3      omxt = omx*at3      omyt = omy*at3      omzt = omz*at3c correct position      dx = part(1,j,l) - (part(4,j,l)*omzt - part(5,j,l)*omyt)      dy = part(2,j,l) - (part(5,j,l)*omxt - part(3,j,l)*omzt)c periodic boundary conditions      if (ipbc.eq.1) then         n = abs(dx)/edgerx         if (dx.lt.edgelx) dx = dx + float(n + 1)*edgerx         if (dx.ge.edgerx) dx = dx - float(n)*edgerx         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgeryc reflecting boundary conditions      else if (ipbc.eq.2) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(dy.g     1e.edgery)) then            if ((dy.ge.edgely).and.(dy.lt.edgery)) thenc if x co-ordinate only is out of bounds, try switching vy               dx = part(1,j,l) + (part(4,j,l)*omzt + part(5,j,l)*omyt)               if ((dx.ge.edgelx).and.(dx.lt.edgerx)) then                  part(4,j,l) = -part(4,j,l)               elsec otherwise, try switching both vy and vz                  dx = part(1,j,l) + (part(4,j,l)*omzt - part(5,j,l)*omy     1t)                  dy = part(2,j,l) + (part(5,j,l)*omxt + part(3,j,l)*omz     1t)                  if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgel     1y).and.(dy.lt.edgery)) then                     part(4,j,l) = -part(4,j,l)                     part(5,j,l) = -part(5,j,l)                  endif               endif            else if ((dx.ge.edgelx).and.(dx.lt.edgerx)) thenc if y co-ordinate only is out of bounds, try switching vx               dy = part(2,j,l) - (part(5,j,l)*omxt + part(3,j,l)*omzt)               if ((dy.ge.edgely).and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)               elsec otherwise, try switching both vx and vz                  dx = part(1,j,l) - (part(4,j,l)*omzt + part(5,j,l)*omy     1t)                  dy = part(2,j,l) + (part(5,j,l)*omxt - part(3,j,l)*omz     1t)                  if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgel     1y).and.(dy.lt.edgery)) then                     part(3,j,l) = -part(3,j,l)                     part(5,j,l) = -part(5,j,l)                  endif               endif            endifc if both co-ordinates are out of bounds, try switching vx, vy, vz            if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(d     1y.ge.edgery)) then               dx = part(1,j,l) + (part(4,j,l)*omzt - part(5,j,l)*omyt)               dy = part(2,j,l) + (part(5,j,l)*omxt - part(3,j,l)*omzt)               if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgely).     1and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)                  part(4,j,l) = -part(4,j,l)                  part(5,j,l) = -part(5,j,l)               elsec give up if larmor radius is too large                  dx = part(1,j,l)                  dy = part(2,j,l)               endif            endif         endifc mixed reflecting/periodic boundary conditions      else if (ipbc.eq.3) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx)) thenc rotate particle position by reversing velocity in y and z            dx = part(1,j,l) + (part(4,j,l)*omzt - part(5,j,l)*omyt)            dy = part(2,j,l) + (part(5,j,l)*omxt + part(3,j,l)*omzt)c give up if larmor radius is too large            if ((dx.lt.edgelx).or.(dx.ge.edgerx)) then               dx = part(1,j,l)               dy = part(2,j,l)            else               part(4,j,l) = -part(4,j,l)               part(5,j,l) = -part(5,j,l)            endif         endif         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgery      endifc set new position      part(1,j,l) = dx      part(2,j,l) = dy   10 continue   20 continue      return      endc-----------------------------------------------------------------------      subroutine PGBZDISTR2L(part,bz,npp,noff,qbm,nx,ny,idimp,npmax,nblo     1k,nxv,nypmx,ipbc)c for 2d code, this subroutine reinterprets curent particlec positions as positions of guiding centers, and calculates the actualc particle positions for distributed datac in converting from guiding center to actual co-ordinates,c the following equations are used:c       x(t) = xg(t) - (vy(t)*omz)/om**2c       y(t) = yg(t) + (vx(t)*omz)/om**2c where omz = (q/m)*bz(xg(t),yg(t)),c and the magnetic field component bz(x(t),y(t)) is approximatedc by interpolation from the nearest grid points:c bz(x,y) = (1-dy)*((1-dx)*bz(n,m)+dx*bz(n+1,m)) + c               dy*((1-dx)*bz(n,m+1) + dx*bz(n+1.m+1))c where n,m = leftmost grid points and dx = x-n, dy = y-mc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc bz(1,j,k,l) = z component of magnetic field at grid (j,kk)c that is, the convolution of magnetic field over particle shapec where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c qbm = particle charge/mass ratioc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nxv = first dimension of field arrays, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)      dimension part(idimp,npmax,nblok)      dimension bz(nxv,nypmx,nblok)      dimension npp(nblok), noff(nblok)c set boundary values      if (ipbc.eq.1) then         edgelx = 0.         edgely = 0.         edgerx = float(nx)         edgery = float(ny)      else if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         edgerx = float(nx-1)         edgery = float(ny-1)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         edgerx = float(nx-1)         edgery = float(ny)      endifc calculate actual position from guiding center      do 20 l = 1, nblok      mnoff = noff(l) - 1      do 10 j = 1, npp(l)c find interpolation weights      nn = part(1,j,l)      mm = part(2,j,l)      dxp = qbm*(part(1,j,l) - float(nn))      dyp = part(2,j,l) - float(mm)      nn = nn + 1      mm = mm - mnoff      amx = qbm - dxp      mp = mm + 1      amy = 1. - dyp      np = nn + 1c find magnetic field      omz = dyp*(dxp*bz(np,mp,l) + amx*bz(nn,mp,l)) + amy*(dxp*bz(np,mm,     1l) + amx*bz(nn,mm,l))      at3 = abs(omz)      if (at3.ne.0.) at3 = 1./at3      at3 = at3*at3      omzt = omz*at3c correct position      dx = part(1,j,l) - part(4,j,l)*omzt      dy = part(2,j,l) + part(3,j,l)*omztc periodic boundary conditions      if (ipbc.eq.1) then         n = abs(dx)/edgerx         if (dx.lt.edgelx) dx = dx + float(n + 1)*edgerx         if (dx.ge.edgerx) dx = dx - float(n)*edgerx         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgeryc reflecting boundary conditions      else if (ipbc.eq.2) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(dy.g     1e.edgery)) then            if ((dy.ge.edgely).and.(dy.lt.edgery)) thenc if x co-ordinate only is out of bounds, try switching vy               dx = part(1,j,l) + part(4,j,l)*omzt               if ((dx.ge.edgelx).and.(dx.lt.edgerx)) then                  part(4,j,l) = -part(4,j,l)               endif            else if ((dx.ge.edgelx).and.(dx.lt.edgerx)) thenc if y co-ordinate only is out of bounds, try switching vx               dy = part(2,j,l) - part(3,j,l)*omzt               if ((dy.ge.edgely).and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)               endif            endifc if both co-ordinates are out of bounds, try switching vx, vy            if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(d     1y.ge.edgery)) then               dx = part(1,j,l) + part(4,j,l)*omzt               dy = part(2,j,l) - part(3,j,l)*omzt               if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgely).     1and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)                  part(4,j,l) = -part(4,j,l)               elsec give up if larmor radius is too large                  dx = part(1,j,l)                  dy = part(2,j,l)               endif            endif         endifc mixed reflecting/periodic boundary conditions      else if (ipbc.eq.3) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx)) thenc rotate particle position by reversing velocity in y            dx = part(1,j,l) + part(4,j,l)*omzt            dy = part(2,j,l) + part(3,j,l)*omztc give up if larmor radius is too large            if ((dx.lt.edgelx).or.(dx.ge.edgerx)) then               dx = part(1,j,l)               dy = part(2,j,l)            else               part(4,j,l) = -part(4,j,l)            endif         endif         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgery      endifc set new position      part(1,j,l) = dx      part(2,j,l) = dy   10 continue   20 continue      return      endc-----------------------------------------------------------------------      subroutine PCGUARD2(f,kstrt,nvp,nxv,nypmx,nblok,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c nblok = number of particle partitions, assumed equal to kblok.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nxv, nypmx, nblok, kyp, kblok      dimension f(nxv,nypmx,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer istatus, msid, ierr      integer ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 30 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computersc     do 10 j = 1, nxvc     f(j,1,l) = f(j,kyp+1,kl)c     f(j,kyp+2,l) = f(j,2,kr)c     f(j,kyp+3,l) = f(j,ngc+1,krr)c  10 continuec this segment is used for mpi computers      call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+3,lgrp,msid,ierr)      call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+3,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+4,lgrp,msid,ie     1rr)      call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+6,lgrp,msi     1d,ierr)         call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kll-1,moff+6,lgrp,ierr)         call MPI_WAIT(msid,istatus,ierr)      endif   30 continue      return      endc-----------------------------------------------------------------------      subroutine PAGUARD2(f,scr,kstrt,nvp,nxv,nypmx,nblok,kyp,kblok,ngds     1)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c scr(j,idps,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c nblok = number of particle partitions, assumed equal to kblok.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nxv, nypmx, nblok, kyp, kblok, ngds      dimension f(nxv,nypmx,nblok), scr(nxv,ngds,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer istatus, msid, ierr      integer ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 40 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computersc     do 10 j = 1, nxvc     scr(j,1,l) = f(j,kyp+2,kl)c     scr(j,2,l) = f(j,kyp+3,kll)c     scr(j,3,l) = f(j,1,kr)c  10 continuec this segment is used for mpi computers      call MPI_IRECV(scr,ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+1,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scr(1,3,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)      call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(scr(1,2,l),ngc*nxv,mreal,kll-1,moff+5,lgrp,msid,     1ierr)         call MPI_SEND(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+5,lgrp,ierr     1)         call MPI_WAIT(msid,istatus,ierr)      endifc add up the guard cells      do 30 j = 1, nxv      f(j,2,l) = f(j,2,l) + scr(j,1,l)      f(j,ngc+1,l) = f(j,ngc+1,l) + scr(j,2,l)      f(j,kyp+1,l) = f(j,kyp+1,l) + scr(j,3,l)   30 continue   40 continue      return      endc-----------------------------------------------------------------------      subroutine pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc ierr = (0,1) = (no,yes) error condition exists      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer nbsize, nter, msid, istatus      real any, yt      dimension msid(4), istatus(lstat), ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 50 l = 1, nblok      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 30 j = 1, npp(l)      yt = part(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = part(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,j,l)            sbufr(4,jsr(1,l),l) = part(4,j,l)            sbufr(5,jsr(1,l),l) = part(5,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endifc particles going down      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = part(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,j,l)            sbufl(4,jsl(1,l),l) = part(4,j,l)            sbufl(5,jsl(1,l),l) = part(5,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endif      endif   30 continue   40 jss(1,l) = jsl(1,l) + jsr(1,l)   50 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computersc     jsl(2,l) = jsr(1,kl)c     do 110 j = 1, jsl(2,l)c     rbufl(1,j,l) = sbufr(1,j,kl)c     rbufl(2,j,l) = sbufr(2,j,kl)c     rbufl(3,j,l) = sbufr(3,j,kl)c     rbufl(4,j,l) = sbufr(4,j,kl)c     rbufl(5,j,l) = sbufr(5,j,kl)c 110 continuec     jsr(2,l) = jsl(1,kr)c     do 120 j = 1, jsr(2,l)c     rbufr(1,j,l) = sbufl(1,j,kr)c     rbufr(2,j,l) = sbufl(2,j,kr)c     rbufr(3,j,l) = sbufl(3,j,kr)c     rbufr(4,j,l) = sbufl(4,j,kr)c     rbufr(5,j,l) = sbufl(5,j,kr)c 120 continuec this segment is used for mpi computersc post receive      call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)      call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particles      call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)     1,ierr)      call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),i     1err)c wait for particles to arrive      call MPI_WAIT(msid(1),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsl(2,l) = nps/idimp      call MPI_WAIT(msid(2),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (6,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (6,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sent      call MPI_WAIT(msid(3),istatus,ierr)      call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)         sbufl(5,jsl(1,l),l) = rbufr(5,j,l)c particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufr(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufr(3,j,l)         sbufr(4,jsr(1,l),l) = rbufr(4,j,l)         sbufr(5,jsr(1,l),l) = rbufr(5,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)         rbufr(5,jss(2,l),l) = rbufr(5,j,l)      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = rbufl(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = rbufl(3,j,l)            sbufr(4,jsr(1,l),l) = rbufl(4,j,l)            sbufr(5,jsr(1,l),l) = rbufl(5,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = rbufl(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = rbufl(3,j,l)            sbufl(4,jsl(1,l),l) = rbufl(4,j,l)            sbufl(5,jsl(1,l),l) = rbufl(5,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)         rbufl(5,jss(2,l),l) = rbufl(5,j,l)      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (6,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)      part(5,ihole(j,l),l) = rbufl(5,j,l)  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)         part(5,ihole(j+jsl(2,l),l),l) = rbufr(5,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)         part(5,j+npp(l),l) = rbufl(5,j+jss(1,l),l)      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)            part(5,ihole(j2,l),l) = part(5,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)         part(5,j+npp(l),l) = rbufr(5,j+jss(1,l),l)      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (6,*) 'Info: particles being passed further = ', ibflg(2)         if (ibflg(3).gt.0) ibflg(3) = 1         go to 100      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after movec     nps = 0c     do 270 l = 1, nblokc     nps = nps + npp(l)c 270 continuec     ibflg(2) = npsc     ibflg(1) = nprc     call PISUM(ibflg,iwork,2,1)c     if (ibflg(1).ne.ibflg(2)) thenc        write (6,*) 'particle number error, old/new=',ibflg(1),ibflg(2)c        ierr = 1c     endifc information      if (nter.gt.0) then         write (6,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine pxmov2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ier     2r)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc maskp = scratch array for particle addressesc ierr = (0,1) = (no,yes) error condition existsc optimized for vector processor      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, maskp, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok), maskp(npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer nbsize, nter, msid, istatus      real any, yt      dimension msid(4), istatus(lstat), ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 80 l = 1, nblok      jss(1,l) = 0      jss(2,l) = 0c find mask function for particles out of bounds      do 30 j = 1, npp(l)      yt = part(2,j,l)      if ((yt.ge.edges(2,l)).or.(yt.lt.edges(1,l))) then         jss(1,l) = jss(1,l) + 1         maskp(j,l) = 1      else         maskp(j,l) = 0      endif   30 continuec set flag if hole buffer would overflow      if (jss(1,l).gt.ntmax) then         jss(1,l) = ntmax         jss(2,l) = 1      endifc accumulate location of holes      do 40 j = 2, npp(l)      maskp(j,l) = maskp(j,l) + maskp(j-1,l)   40 continuec store addresses of particles out of bounds      do 50 j = 2, npp(l)      if ((maskp(j,l).gt.maskp(j-1,l)).and.(maskp(j,l).le.ntmax)) then         ihole(maskp(j,l),l) = j      endif   50 continue      if (maskp(1,l).gt.0) ihole(1,l) = 1      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0c load particle buffers      do 60 j = 1, jss(1,l)      yt = part(2,ihole(j,l),l)c particles going up      if (yt.ge.edges(2,l)) then         if ((kb+1).eq.nvp) yt = yt - any         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            sbufr(1,jsr(1,l),l) = part(1,ihole(j,l),l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,ihole(j,l),l)            sbufr(4,jsr(1,l),l) = part(4,ihole(j,l),l)            sbufr(5,jsr(1,l),l) = part(5,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endifc particles going down      else         if (kb.eq.0) yt = yt + any         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            sbufl(1,jsl(1,l),l) = part(1,ihole(j,l),l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,ihole(j,l),l)            sbufl(4,jsl(1,l),l) = part(4,ihole(j,l),l)            sbufl(5,jsl(1,l),l) = part(5,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endif      endif   60 continue   70 jss(1,l) = jsl(1,l) + jsr(1,l)   80 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computersc     jsl(2,l) = jsr(1,kl)c     do 110 j = 1, jsl(2,l)c     rbufl(1,j,l) = sbufr(1,j,kl)c     rbufl(2,j,l) = sbufr(2,j,kl)c     rbufl(3,j,l) = sbufr(3,j,kl)c     rbufl(4,j,l) = sbufr(4,j,kl)c     rbufl(5,j,l) = sbufr(5,j,kl)c 110 continuec     jsr(2,l) = jsl(1,kr)c     do 120 j = 1, jsr(2,l)c     rbufr(1,j,l) = sbufl(1,j,kr)c     rbufr(2,j,l) = sbufl(2,j,kr)c     rbufr(3,j,l) = sbufl(3,j,kr)c     rbufr(4,j,l) = sbufl(4,j,kr)c     rbufr(5,j,l) = sbufl(5,j,kr)c 120 continuec this segment is used for mpi computersc post receive      call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)      call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particles      call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)     1,ierr)      call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),i     1err)c wait for particles to arrive      call MPI_WAIT(msid(1),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsl(2,l) = nps/idimp      call MPI_WAIT(msid(2),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (6,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (6,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sent      call MPI_WAIT(msid(3),istatus,ierr)      call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)         sbufl(5,jsl(1,l),l) = rbufr(5,j,l)c particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufr(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufr(3,j,l)         sbufr(4,jsr(1,l),l) = rbufr(4,j,l)         sbufr(5,jsr(1,l),l) = rbufr(5,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)         rbufr(5,jss(2,l),l) = rbufr(5,j,l)      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = rbufl(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = rbufl(3,j,l)            sbufr(4,jsr(1,l),l) = rbufl(4,j,l)            sbufr(5,jsr(1,l),l) = rbufl(5,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = rbufl(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = rbufl(3,j,l)            sbufl(4,jsl(1,l),l) = rbufl(4,j,l)            sbufl(5,jsl(1,l),l) = rbufl(5,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)         rbufl(5,jss(2,l),l) = rbufl(5,j,l)      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (6,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)      part(5,ihole(j,l),l) = rbufl(5,j,l)  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)         part(5,ihole(j+jsl(2,l),l),l) = rbufr(5,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)         part(5,j+npp(l),l) = rbufl(5,j+jss(1,l),l)      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)            part(5,ihole(j2,l),l) = part(5,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)         part(5,j+npp(l),l) = rbufr(5,j+jss(1,l),l)      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (6,*) 'Info: particles being passed further = ', ibflg(2)         if (ibflg(3).gt.0) ibflg(3) = 1         go to 100      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after move      nps = 0      do 270 l = 1, nblok      nps = nps + npp(l)  270 continue      ibflg(2) = nps      ibflg(1) = npr      call PISUM(ibflg,iwork,2,1)      if (ibflg(1).ne.ibflg(2)) then         write (6,*) 'particle number error, old/new=',ibflg(1),ibflg(2)         ierr = 1      endifc information      if (nter.gt.0) then         write (6,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine ptpose(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jb     1lok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(nxv,kypd,kblok), g(nyv,kxpd,jblok)      dimension s(kxp,kyp,kblok), t(kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kyb, jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff,j,l) = f(j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,l),kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,msid,     1ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(j,k,l) = f(j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,l),kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,ierr     1)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(k+koff,j,l) = t(j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine p2tpose(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(2,nxv,kypd,kblok), g(2,nyv,kxpd,jblok)      dimension s(2,kxp,kyp,kblok), t(2,kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kyb, jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1,k+koff,j,l) = f(1,j+joff,k,i)c     g(2,k+koff,j,l) = f(2,j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec     returnc this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,1,l),2*kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,m     1sid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(1,j,k,l) = f(1,j+joff,k,l)         s(2,j,k,l) = f(2,j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,1,l),2*kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,     1ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(1,k+koff,j,l) = t(1,j,k,l)         g(2,k+koff,j,l) = t(2,j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine PTPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblok     1,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(nxv*kypd*kblok), g(nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff+nyv*(j-1+kxpd*(l-1))) = f(j+joff+nxv*(k-1+kypd*(i-1)))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, kxp         g(j+kxp*(k+is)) = f(j+joff+nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continuec exchange data      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+kxp*kyp*(ir+koff)),kxp*kyp,mcplx,ir-1,ir+kxy     1m+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+kxp*kyp*(is+ioff)),kxp*kyp,mcplx,is-1,l+ks+kx     1ym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(k+koff+nyv*(j+joff)) = f(j+kxp*(k+ir))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      subroutine P2TPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc msid, mrid = scratch arrays for identifying asynchronous messagesc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kxb/kyb = number of processors in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(2*nxv*kypd*kblok), g(2*nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks) - 1c     do 30 i = 1, kybc     koff = kyp*(i - 1) - 1c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(1+2*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(2+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(2+2*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = 2*kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, 2*kxp         g(j+2*kxp*(k+is)) = f(j+joff+2*nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continue      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+2*kxp*kyp*(ir+koff)),2*kxp*kyp,mcplx,ir-1,ir     1+kxym+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+2*kxp*kyp*(is+ioff)),2*kxp*kyp,mcplx,is-1,l+k     1s+kxym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(2*(k+koff+nyv*(j+joff))-1) = f(2*(j+kxp*(k+ir))-1)         g(2*(k+koff+nyv*(j+joff))) = f(2*(j+kxp*(k+ir)))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      subroutine P3TPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:3,k+kyp*(m-1),j,l) = f(1:3,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(3*nxv*kypd*kblok), g(3*nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks) - 1c     do 30 i = 1, kybc     koff = kyp*(i - 1) - 1c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1+3*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(1+3*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(2+3*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(2+3*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(3+3*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(3+3*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = 3*kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, 3*kxp         g(j+3*kxp*(k+is)) = f(j+joff+3*nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continue      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+3*kxp*kyp*(ir+koff)),3*kxp*kyp,mcplx,ir-1,ir     1+kxym+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+3*kxp*kyp*(is+ioff)),3*kxp*kyp,mcplx,is-1,l+k     1s+kxym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(3*(k+koff+nyv*(j+joff))-2) = f(3*(j+kxp*(k+ir))-2)         g(3*(k+koff+nyv*(j+joff))-1) = f(3*(j+kxp*(k+ir))-1)         g(3*(k+koff+nyv*(j+joff))) = f(3*(j+kxp*(k+ir)))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      subroutine PRTPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c includes an extra guard cell for last row and columnc this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = real input arrayc g = real output arrayc s, t = real scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv = first dimension of f >= nx+1c nyv = first dimension of g >= ny+1c kypd = second dimension of f >= kyp+1c kxpd = second dimension of g >= kxp+1c kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      real f, g, s, t      dimension f(nxv,kypd,kblok), g(nyv,kxpd,jblok)      dimension s(kxp+1,kyp+1,kblok), t(kxp+1,kyp+1,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, kxp1, kyp1, kxpt, kypt      integer jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc set constants to receive extra guard cells      kxp1 = kxp + 1      kyp1 = kyp + 1      kxpt = kxp      if (kstrt.eq.kxb) kxpt = kxp1c this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     kypt = kypc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     if ((l+ks).eq.(kxb-1)) kxpt = kxp1c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     if (i.eq.kyb) kypt = kyp1c     do 20 k = 1, kyptc     do 10 j = 1, kxptc     g(k+koff,j,l) = f(j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         kypt = kyp         if (ir.eq.kyb) kypt = kyp1         call MPI_IRECV(t(1,1,l),kxp1*kyp1,mreal,ir-1,ir+kxym+1,lgrp,msi     1d,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp1         do 10 j = 1, kxp1         s(j,k,l) = f(j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,l),kxp1*kyp1,mreal,is-1,l+ks+kxym+2,lgrp,ie     1rr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kypt         do 30 j = 1, kxpt         g(k+koff,j,l) = t(j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine PR2TPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,     1jblok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c includes an extra guard cell for last row and columnc this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = real input arrayc g = real output arrayc s, t = real scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv = first dimension of f >= nx+1c nyv = first dimension of g >= ny+1c kypd = second dimension of f >= kyp+1c kxpd = second dimension of g >= kxp+1c kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      real f, g, s, t      dimension f(2,nxv,kypd,kblok), g(2,nyv,kxpd,jblok)      dimension s(2,kxp+1,kyp+1,kblok), t(2,kxp+1,kyp+1,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, kxp1, kyp1, kxpt, kypt      integer jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc set constants to receive extra guard cells      kxp1 = kxp + 1      kyp1 = kyp + 1      kxpt = kxp      if (kstrt.eq.kxb) kxpt = kxp1c this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     kypt = kypc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     if ((l+ks).eq.(kxb-1)) kxpt = kxp1c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     if (i.eq.kyb) kypt = kyp1c     do 20 k = 1, kyptc     do 10 j = 1, kxptc     g(1,k+koff,j,l) = f(1,j+joff,k,i)c     g(2,k+koff,j,l) = f(2,j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         kypt = kyp         if (ir.eq.kyb) kypt = kyp1         call MPI_IRECV(t(1,1,1,l),2*kxp1*kyp1,mreal,ir-1,ir+kxym+1,lgrp     1,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp1         do 10 j = 1, kxp1         s(1,j,k,l) = f(1,j+joff,k,l)         s(2,j,k,l) = f(2,j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,1,l),2*kxp1*kyp1,mreal,is-1,l+ks+kxym+2,lgr     1p,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kypt         do 30 j = 1, kxpt         g(1,k+koff,j,l) = t(1,j,k,l)         g(2,k+koff,j,l) = t(2,j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine PR3TPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,     1jblok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:3,k+kyp*(m-1),j,l) = f(1:3,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c includes an extra guard cell for last row and columnc this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = real input arrayc g = real output arrayc s, t = real scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv = first dimension of f >= nx+1c nyv = first dimension of g >= ny+1c kypd = second dimension of f >= kyp+1c kxpd = second dimension of g >= kxp+1c kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      real f, g, s, t      dimension f(3,nxv,kypd,kblok), g(3,nyv,kxpd,jblok)      dimension s(3,kxp+1,kyp+1,kblok), t(3,kxp+1,kyp+1,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, kxp1, kyp1, kxpt, kypt      integer jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc set constants to receive extra guard cells      kxp1 = kxp + 1      kyp1 = kyp + 1      kxpt = kxp      if (kstrt.eq.kxb) kxpt = kxp1c this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     kypt = kypc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     if ((l+ks).eq.(kxb-1)) kxpt = kxp1c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     if (i.eq.kyb) kypt = kyp1c     do 20 k = 1, kyptc     do 10 j = 1, kxptc     g(1,k+koff,j,l) = f(1,j+joff,k,i)c     g(2,k+koff,j,l) = f(2,j+joff,k,i)c     g(3,k+koff,j,l) = f(3,j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         kypt = kyp         if (ir.eq.kyb) kypt = kyp1         call MPI_IRECV(t(1,1,1,l),3*kxp1*kyp1,mreal,ir-1,ir+kxym+1,lgrp     1,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp1         do 10 j = 1, kxp1         s(1,j,k,l) = f(1,j+joff,k,l)         s(2,j,k,l) = f(2,j+joff,k,l)         s(3,j,k,l) = f(3,j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,1,l),3*kxp1*kyp1,mreal,is-1,l+ks+kxym+2,lgr     1p,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kypt         do 30 j = 1, kxpt         g(1,k+koff,j,l) = t(1,j,k,l)         g(2,k+koff,j,l) = t(2,j,k,l)         g(3,k+koff,j,l) = t(3,j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      function ranorm()c this program calculates a random number y from a gaussian distributionc with zero mean and unit variance, according to the method ofc mueller and box:c    y(k) = (-2*ln(x(k)))**1/2*sin(2*pi*x(k+1))c    y(k+1) = (-2*ln(x(k)))**1/2*cos(2*pi*x(k+1)),c where x is a random number uniformly distributed on (0,1).c written for the ibm by viktor k. decyk, ucla      integer r1,r2,r4,r5      double precision ranorm,h1l,h1u,h2l,r0,r3,asc,bsc,temp      save iflg,r1,r2,r4,r5,h1l,h1u,h2l,r0      data r1,r2,r4,r5 /885098780,1824280461,1396483093,55318673/      data h1l,h1u,h2l /65531.0d0,32767.0d0,65525.0d0/      data iflg,r0 /0,0.0d0/      if (iflg.eq.0) go to 10      ranorm = r0      r0 = 0.0d0      iflg = 0      return   10 isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r1 - (r1/isc)*isc      r3 = h1l*dble(r1) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r2/isc      isc = r2 - i1*isc      r0 = h1l*dble(r2) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r2 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r1 = r3 - dble(isc)*bsc      temp = dsqrt(-2.0d0*dlog((dble(r1) + dble(r2)*asc)*asc))      isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r4 - (r4/isc)*isc      r3 = h2l*dble(r4) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r5/isc      isc = r5 - i1*isc      r0 = h2l*dble(r5) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r5 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r4 = r3 - dble(isc)*bsc      r0 = 6.28318530717959d0*((dble(r4) + dble(r5)*asc)*asc)      ranorm = temp*dsin(r0)      r0 = temp*dcos(r0)      iflg = 1      return      endc-----------------------------------------------------------------------      subroutine timera(icntrl,chr,time)c this subroutine performs timingc input: icntrl, chrc icntrl = (-1,0,1) = (initialize,ignore,read) clockc clock should be initialized before it is read!c chr = character variable for labeling timingsc time = elapsed time in secondsc written for mpi      implicit none      integer icntrl      character*8 chr      real timec get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplxc lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer idproc, ierr      real nclock, mclock      double precision jclock      save jclock   91 format (1x,a8,1x,'max/min real time = ',e14.7,1x,e14.7,1x,'sec')      data jclock /0.0d0/      if (icntrl.eq.0) return      if (icntrl.eq.1) go to 10c initialize clock      call MPI_BARRIER(lgrp,ierr)      jclock = MPI_WTIME()      returnc read clock and write time difference from last clock initialization   10 nclock = real(MPI_WTIME() - jclock)      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MIN,lgrp,ierr)      mclock = time      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MAX,lgrp,ierr)      call MPI_COMM_RANK(lgrp,idproc,ierr)      if (idproc.eq.0) write (6,91) chr, time, mclock      return      endc-----------------------------------------------------------------------      subroutine PSUM (f,g,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c f(j,k) = sum over k of f(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer istatus      integer idproc, ierr, kstrt, ks, l, kxs, k, kb, lb, msid, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = f(j,kb+kxs)c     elsec        g(j,k) = f(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      f(j,k) = f(j,k) + g(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PISUM (if,ig,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c if(j,k) = sum over k of if(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c if = input and output integer datac ig = scratch integer arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      integer if, ig, nxp, nblok      dimension if(nxp,nblok), ig(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mint = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer istatus      integer idproc, ierr, kstrt, ks, l, kxs, k, kb, lb, nsid, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        ig(j,k) = if(j,kb+kxs)c     elsec        ig(j,k) = if(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_ISEND(if,nxp,mint,kb+kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb+kxs-1,l+nxp,lgrp,istatus,ierr)      else         call MPI_ISEND(if,nxp,mint,kb-kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb-kxs-1,l+nxp,lgrp,istatus,ierr)      endif      call MPI_WAIT(nsid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      if(j,k) = if(j,k) + ig(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PWTIMERA(icntrl,time,dtime)c this subroutine performs local wall clock timingc input: icntrl, dtimec icntrl = (-1,0,1) = (initialize,ignore,read) clockc clock should be initialized before it is read!c time = elapsed time in secondsc dtime = current timec written for mpi      implicit none      integer icntrl      real time      double precision dtimec local data      double precision jclock      double precision MPI_WTIME      external MPI_WTIMEc initialize clock      if (icntrl.eq.(-1)) then         dtime = MPI_WTIME()c read clock and write time difference from last clock initialization      else if (icntrl.eq.1) then         jclock = dtime         dtime = MPI_WTIME()         time = real(dtime - jclock)      endif      return      endc-----------------------------------------------------------------------      subroutine PMAX(f,g,nxp,nblok)c this subroutine finds parallel maximum for each element of a vectorc that is, f(j,k) = maximum as a function of k of f(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = f(j,kb+kxs)c     elsec        g(j,k) = f(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec find maximum      do 50 k = 1, nblok      do 40 j = 1, nxp      f(j,k) = amax1(f(j,k),g(j,k))   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PDBLSIN2C(cu,cu2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2     1blok)c this subroutine creates a doubled vector array cu2 from a vector arrayc cu, so that various 2d sine/cosine transforms can be performed with ac 2d real to complex fft.  the x component is an odd function in y,c and y component is an odd function in x.c Asummes vector cu vanishes at end pointsc linear interpolation for distributed datac cu2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = first dimension of input array q, must be >= nxc kyp = number of data values per block in yc kypd = second dimension of input array q, must be >= kypc kyp2 = second dimension of output array q2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real cu, cu2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension cu(2,nxv,kypd,kblok), cu2(2,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lsid, msid, nsid, ierr      integer i, j, k, l, nxs, nys, ny2, kyb, kyb2, ks, koff, moff      integer kk, ll, lm, k1, k2, joff      dimension istatus(lstat)      nxs = nx - 1      nys = ny - 1      kyb = ny/kyp      ny2 = ny + ny      kyb2 = ny2/kyp2      ks = kstrt - 2      moff = kypd + kybc copy to double array in x direction      do 80 l = 1, k2blok      koff = kyp2*(l + ks)      ll = koff/kyp + 1      koff = kyp*(l + ks)      lm = koff/kyp2 + 1c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, nys         do 10 j = 1, nxs         cu2(1,j+1,k+1,l) = cu(1,j+1,k+1,l)         cu2(2,j+1,k+1,l) = cu(2,j+1,k+1,l)         cu2(1,nx+j+1,k+1,l) = cu(1,nx-j+1,k+1,l)         cu2(2,nx+j+1,k+1,l) = -cu(2,nx-j+1,k+1,l)         cu2(1,j+1,ny+k+1,l) = -cu(1,j+1,ny-k+1,l)         cu2(2,j+1,ny+k+1,l) = cu(2,j+1,ny-k+1,l)         cu2(1,nx+j+1,ny+k+1,l) = -cu(1,nx-j+1,ny-k+1,l)         cu2(2,nx+j+1,ny+k+1,l) = -cu(2,nx-j+1,ny-k+1,l)   10    continue         cu2(1,1,k+1,l) = 0.         cu2(2,1,k+1,l) = 0.         cu2(1,nx+1,k+1,l) = 0.         cu2(2,nx+1,k+1,l) = 0.         cu2(1,1,k+ny+1,l) = 0.         cu2(2,1,k+ny+1,l) = 0.         cu2(1,nx+1,k+ny+1,l) = 0.         cu2(2,nx+1,k+ny+1,l) = 0.   20    continue         do 30 j = 1, nx         cu2(1,j,1,l) = 0.         cu2(2,j,1,l) = 0.         cu2(1,j+nx,1,l) = 0.         cu2(2,j+nx,1,l) = 0.         cu2(1,j,ny+1,l) = 0.         cu2(2,j,ny+1,l) = 0.         cu2(1,j+nx,ny+1,l) = 0.         cu2(2,j+nx,ny+1,l) = 0.   30    continue         return      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        do 50 k = 1, kypc        do 40 j = 1, nxc        do 35 i = 1, 2c        cu2(i,j,k,l) = cu(i,j,k,ll)c  35    continuec  40    continuec  50    continuec        if (kyp.lt.kyp2) thenc           do 70 k = 1, kypc           do 60 j = 1, nxc           do 55 i = 1, 2c           cu2(j,k+kyp,l) = cu(j,k,ll+1)c  55       continuec  60       continuec  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         call MPI_IRECV(cu2(1,1,1,l),2*kyp*nxv,mreal,ll-1,moff+1,lgrp,ms     1id,ierr)         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),2*kyp*nxv,mreal,ll,moff+1,lg     1rp,nsid,ierr)         endif      endif      if (lm.le.(kyb2/2)) then         call MPI_SEND(cu(1,1,1,l),2*kyp*nxv,mreal,lm-1,moff+1,lgrp,ierr     1)      endifc wait for data and unpack it      if (ll.le.kyb) then         call MPI_WAIT(msid,istatus,ierr)         do 50 k = 2, kyp         k1 = kyp - k + 2         k2 = (k1 - 1)/2 + 1         joff = nxv*(k1 - 2*k2 + 1)         do 40 j = 1, nxv         do 35 i = 1, 2         cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)   35    continue   40    continue   50    continue         if (kyp.lt.kyp2) then            call MPI_WAIT(nsid,istatus,ierr)            do 70 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 60 j = 1, nxv            do 55 i = 1, 2            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   55       continue   60       continue   70       continue         endif      endif   80 continuec copy to double array in y direction      do 140 l = 1, k2blok      koff = kyp2*(l + ks)      ll = (ny2 - koff - 1)/kyp + 1      koff = kyp*(l + ks)      lm = (ny2 - koff - 1)/kyp2 + 1      koff = koff + kyp2*lm - ny2c this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((ll+1).le.kyb) thenc           do 90 j = 1, nxc           do 85 i = 1, 2c           cu2(i,j,1,l) = cu(i,j,1,ll+1)c  85       continuec  90       continuec        endifc        if (kyp.lt.kyp2) thenc           do 110 k = 1, kypc           do 100 j = 1, nxc           do 95 i = 1, 2c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll)c  95       continuec 100       continuec 110       continuec        endifc        if (kyp.gt.1) thenc           do 130 k = 2, kypc           do 120 j = 1, nxc           do 115 i = 1, 2c           cu2(i,j,k,l) = cu(i,j,k,ll-1)c 115       continuec 120       continuec 130       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_IRECV(cu2(1,1,1,l),2*nxv,mreal,ll,moff+2,lgrp,lsid,     1ierr)         endif         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),2*kyp*nxv,mreal,ll-1,moff+2,     1lgrp,msid,ierr)         endif         if (kyp.gt.1) then            call MPI_IRECV(cu2(1,1,2,l),2*(kyp-1)*nxv,mreal,ll-2,moff+2,     1lgrp,nsid,ierr)         endif      endif      if ((lm.gt.(kyb2/2)).and.(lm.le.kyb2)) then         if (koff.eq.0) then            if ((lm+1).le.kyb2) then               call MPI_SEND(cu(1,1,1,l),2*nxv,mreal,lm,moff+2,lgrp,ierr     1)            endif            if (kyp.gt.1) then               call MPI_SEND(cu(1,1,2,l),2*(kyp-1)*nxv,mreal,lm-1,moff+2     1,lgrp,ierr)            endif         else            call MPI_SEND(cu(1,1,1,l),2*kyp*nxv,mreal,lm-1,moff+2,lgrp,i     1err)         endif      endifc wait for data and unpack it      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_WAIT(lsid,istatus,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_WAIT(msid,istatus,ierr)            do 100 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 90 j = 1, nxv            do 85 i = 1, 2            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   85       continue   90       continue  100       continue         endif         if (kyp.gt.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 120 k = 3, kyp            k1 = kyp - k + 3            k2 = k1/2 + 1            joff = nxv*(k1 - 2*k2 + 2)            do 110 j = 1, nxv            do 105 i = 1, 2            cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)  105       continue  110       continue  120       continue         endif      endif  140 continuec create odd array      do 200 l = 1, k2blok      koff = kyp2*(l + ks)      do 190 k = 1, kyp2      kk = k + koff      if ((kk.eq.1).or.(kk.eq.(ny+1))) then         do 150 j = 1, nx         do 145 i = 1, 2         cu2(i,j,k,l) = 0.         cu2(i,j+nx,k,l) = 0.  145    continue  150    continue      else if (kk.le.ny) then         do 160 j = 1, nxs         cu2(1,nx+j+1,k,l) = cu2(1,nx-j+1,k,l)         cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)  160    continue         do 165 i = 1, 2         cu2(i,1,k,l) = 0.         cu2(i,nx+1,k,l) = 0.  165    continue      else if (kk.gt.(ny+1)) then         if (k.eq.1) then            do 170 j = 1, nxs            cu2(1,nx+j+1,k,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)  170       continue         else            do 180 j = 1, nxs            cu2(1,nx+j+1,kyp2-k+2,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,kyp2-k+2,l) = -cu2(2,nx-j+1,k,l)  180       continue         endif         do 185 i = 1, 2         cu2(i,1,k,l) = 0.         cu2(i,nx+1,k,l) = 0.  185    continue      endif  190 continue  200 continuec finish odd array      do 230 l = 1, k2blok      koff = kyp2*(l + ks)      do 220 k = 1, kyp2      kk = k + koff      if (kk.gt.(ny+1)) then         do 210 j = 1, nxs         cu2(1,nx-j+1,k,l) = cu2(1,nx+j+1,k,l)         cu2(2,nx-j+1,k,l) = -cu2(2,nx+j+1,k,l)  210    continue         cu2(1,nx+1,k,l) = cu2(1,nx+1,k,l)         cu2(2,nx+1,k,l) = -cu2(2,nx+1,k,l)      endif  220 continue  230 continue      return      endc-----------------------------------------------------------------------      subroutine PDBLSIN2D(q,q2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2bl     1ok)c this subroutine creates an odd array q2 from an array q, so thatc a 2d sine transform can be performed with a 2d real to complex fft.c linear interpolation for distributed datac q2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = first dimension of input array q, must be >= nxc kyp = number of data values per block in yc kypd = second dimension of input array q, must be >= kypc kyp2 = second dimension of output array q2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real q, q2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension q(nxv,kypd,kblok), q2(2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lsid, msid, nsid, ierr      integer j, k, l, nxs, nys, ny2, kyb, kyb2, ks, koff, moff      integer kk, ll, lm, k1, k2, joff      dimension istatus(lstat)      nxs = nx - 1      nys = ny - 1      kyb = ny/kyp      ny2 = ny + ny      kyb2 = ny2/kyp2      ks = kstrt - 2      moff = kypd + kybc copy to double array in x direction      do 80 l = 1, k2blok      koff = kyp2*(l + ks)      ll = koff/kyp + 1      koff = kyp*(l + ks)      lm = koff/kyp2 + 1c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, nys         do 10 j = 1, nxs         q2(j+1,k+1,l) = q(j+1,k+1,l)         q2(nx+j+1,k+1,l) = -q(nx-j+1,k+1,l)         q2(j+1,ny+k+1,l) = -q(j+1,ny-k+1,l)         q2(nx+j+1,ny+k+1,l) = q(nx-j+1,ny-k+1,l)   10    continue         q2(1,k+1,l) = 0.         q2(nx+1,k+1,l) = 0.         q2(1,k+ny+1,l) = 0.         q2(nx+1,k+ny+1,l) = 0.   20    continue         do 30 j = 1, nx         q2(j,1,l) = 0.         q2(j+nx,1,l) = 0.         q2(j,ny+1,l) = 0.         q2(j+nx,ny+1,l) = 0.   30    continue         return      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        do 50 k = 1, kypc        do 40 j = 1, nxc        q2(j,k,l) = q(j,k,ll)c  40    continuec  50    continuec        if (kyp.lt.kyp2) thenc           do 70 k = 1, kypc           do 60 j = 1, nxc           q2(j,k+kyp,l) = q(j,k,ll+1)c  60       continuec  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         call MPI_IRECV(q2(1,1,l),kyp*nxv,mreal,ll-1,moff+1,lgrp,msid,ie     1rr)         if (kyp.lt.kyp2) then            call MPI_IRECV(q2(1,kyp+1,l),kyp*nxv,mreal,ll,moff+1,lgrp,ns     1id,ierr)         endif      endif      if (lm.le.(kyb2/2)) then         call MPI_SEND(q(1,1,l),kyp*nxv,mreal,lm-1,moff+1,lgrp,ierr)      endifc wait for data and unpack it      if (ll.le.kyb) then         call MPI_WAIT(msid,istatus,ierr)         do 50 k = 2, kyp         k1 = kyp - k + 2         k2 = (k1 - 1)/2 + 1         joff = nxv*(k1 - 2*k2 + 1)         do 40 j = 1, nxv         q2(j,k1,l) = q2(j+joff,k2,l)   40    continue   50    continue         if (kyp.lt.kyp2) then            call MPI_WAIT(nsid,istatus,ierr)            do 70 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 60 j = 1, nxv            q2(j,k1+kyp,l) = q2(j+joff,k2+kyp,l)   60       continue   70       continue         endif      endif   80 continuec copy to double array in y direction      do 140 l = 1, k2blok      koff = kyp2*(l + ks)      ll = (ny2 - koff - 1)/kyp + 1      koff = kyp*(l + ks)      lm = (ny2 - koff - 1)/kyp2 + 1      koff = koff + kyp2*lm - ny2c this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((ll+1).le.kyb) thenc           do 90 j = 1, nxc           q2(j,1,l) = q(j,1,ll+1)c  90       continuec        endifc        if (kyp.lt.kyp2) thenc           do 110 k = 1, kypc           do 100 j = 1, nxc           q2(j,k+kyp,l) = q(j,k,ll)c 100       continuec 110       continuec        endifc        if (kyp.gt.1) thenc           do 130 k = 2, kypc           do 120 j = 1, nxc           q2(j,k,l) = q(j,k,ll-1)c 120       continuec 130       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_IRECV(q2(1,1,l),nxv,mreal,ll,moff+2,lgrp,lsid,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_IRECV(q2(1,kyp+1,l),kyp*nxv,mreal,ll-1,moff+2,lgrp,     1msid,ierr)         endif         if (kyp.gt.1) then            call MPI_IRECV(q2(1,2,l),(kyp-1)*nxv,mreal,ll-2,moff+2,lgrp,     1nsid,ierr)         endif      endif      if ((lm.gt.(kyb2/2)).and.(lm.le.kyb2)) then         if (koff.eq.0) then            if ((lm+1).le.kyb2) then               call MPI_SEND(q(1,1,l),nxv,mreal,lm,moff+2,lgrp,ierr)            endif            if (kyp.gt.1) then               call MPI_SEND(q(1,2,l),(kyp-1)*nxv,mreal,lm-1,moff+2,lgrp     1,ierr)            endif         else            call MPI_SEND(q(1,1,l),kyp*nxv,mreal,lm-1,moff+2,lgrp,ierr)         endif      endifc wait for data and unpack it      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_WAIT(lsid,istatus,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_WAIT(msid,istatus,ierr)            do 100 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 90 j = 1, nxv            q2(j,k1+kyp,l) = q2(j+joff,k2+kyp,l)   90       continue  100       continue         endif         if (kyp.gt.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 120 k = 3, kyp            k1 = kyp - k + 3            k2 = k1/2 + 1            joff = nxv*(k1 - 2*k2 + 2)            do 110 j = 1, nxv            q2(j,k1,l) = q2(j+joff,k2,l)  110       continue  120       continue         endif      endif  140 continuec create odd array      do 200 l = 1, k2blok      koff = kyp2*(l + ks)      do 190 k = 1, kyp2      kk = k + koff      if ((kk.eq.1).or.(kk.eq.(ny+1))) then         do 150 j = 1, nx         q2(j,k,l) = 0.         q2(j+nx,k,l) = 0.  150    continue      else if (kk.le.ny) then         do 160 j = 1, nxs         q2(nx+j+1,k,l) = -q2(nx-j+1,k,l)  160    continue         q2(1,k,l) = 0.         q2(nx+1,k,l) = 0.      else if (kk.gt.(ny+1)) then         if (k.eq.1) then            do 170 j = 1, nxs            q2(nx+j+1,k,l) = q2(nx-j+1,k,l)  170       continue         else            do 180 j = 1, nxs            q2(nx+j+1,kyp2-k+2,l) = q2(nx-j+1,k,l)  180       continue         endif         q2(1,k,l) = 0.         q2(nx+1,k,l) = 0.      endif  190 continue  200 continuec finish odd array      do 230 l = 1, k2blok      koff = kyp2*(l + ks)      do 220 k = 1, kyp2      kk = k + koff      if (kk.gt.(ny+1)) then         do 210 j = 1, nxs         q2(nx-j+1,k,l) = -q2(nx+j+1,k,l)  210    continue         q2(nx+1,k,l) = -q2(nx+1,k,l)      endif  220 continue  230 continue      return      endc-----------------------------------------------------------------------      subroutine PDBLSIN2B(cu,cu2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2     1blok)c this subroutine creates a doubled vector array cu2 from a vector arrayc cu, so that various 2d sine/cosine transforms can be performed with ac 2d real to complex fft.  the x component is an odd function in y,c y component is an odd function in x, and the z component is an oddc function in both x and y.  Asummes vector cu vanishes at end pointsc linear interpolation for distributed datac cu2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = first dimension of input array q, must be >= nxc kyp = number of data values per block in yc kypd = second dimension of input array q, must be >= kypc kyp2 = second dimension of output array q2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real cu, cu2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension cu(3,nxv,kypd,kblok), cu2(3,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lsid, msid, nsid, ierr      integer i, j, k, l, nxs, nys, ny2, kyb, kyb2, ks, koff, moff      integer kk, ll, lm, k1, k2, joff      dimension istatus(lstat)      nxs = nx - 1      nys = ny - 1      kyb = ny/kyp      ny2 = ny + ny      kyb2 = ny2/kyp2      ks = kstrt - 2      moff = kypd + kybc copy to double array in x direction      do 80 l = 1, k2blok      koff = kyp2*(l + ks)      ll = koff/kyp + 1      koff = kyp*(l + ks)      lm = koff/kyp2 + 1c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, nys         do 10 j = 1, nxs         cu2(1,j+1,k+1,l) = cu(1,j+1,k+1,l)         cu2(2,j+1,k+1,l) = cu(2,j+1,k+1,l)         cu2(3,j+1,k+1,l) = cu(3,j+1,k+1,l)         cu2(1,nx+j+1,k+1,l) = cu(1,nx-j+1,k+1,l)         cu2(2,nx+j+1,k+1,l) = -cu(2,nx-j+1,k+1,l)         cu2(3,nx+j+1,k+1,l) = -cu(3,nx-j+1,k+1,l)         cu2(1,j+1,ny+k+1,l) = -cu(1,j+1,ny-k+1,l)         cu2(2,j+1,ny+k+1,l) = cu(2,j+1,ny-k+1,l)         cu2(3,j+1,ny+k+1,l) = -cu(3,j+1,ny-k+1,l)         cu2(1,nx+j+1,ny+k+1,l) = -cu(1,nx-j+1,ny-k+1,l)         cu2(2,nx+j+1,ny+k+1,l) = -cu(2,nx-j+1,ny-k+1,l)         cu2(3,nx+j+1,ny+k+1,l) = cu(3,nx-j+1,ny-k+1,l)   10    continue         cu2(1,1,k+1,l) = 0.         cu2(2,1,k+1,l) = 0.         cu2(3,1,k+1,l) = 0.         cu2(1,nx+1,k+1,l) = 0.         cu2(2,nx+1,k+1,l) = 0.         cu2(3,nx+1,k+1,l) = 0.         cu2(1,1,k+ny+1,l) = 0.         cu2(2,1,k+ny+1,l) = 0.         cu2(3,1,k+ny+1,l) = 0.         cu2(1,nx+1,k+ny+1,l) = 0.         cu2(2,nx+1,k+ny+1,l) = 0.         cu2(3,nx+1,k+ny+1,l) = 0.   20    continue         do 30 j = 1, nx         cu2(1,j,1,l) = 0.         cu2(2,j,1,l) = 0.         cu2(3,j,1,l) = 0.         cu2(1,j+nx,1,l) = 0.         cu2(2,j+nx,1,l) = 0.         cu2(3,j+nx,1,l) = 0.         cu2(1,j,ny+1,l) = 0.         cu2(2,j,ny+1,l) = 0.         cu2(3,j,ny+1,l) = 0.         cu2(1,j+nx,ny+1,l) = 0.         cu2(2,j+nx,ny+1,l) = 0.         cu2(3,j+nx,ny+1,l) = 0.   30    continue         return      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        do 50 k = 1, kypc        do 40 j = 1, nxc        do 35 i = 1, 3c        cu2(i,j,k,l) = cu(i,j,k,ll)c  35    continuec  40    continuec  50    continuec        if (kyp.lt.kyp2) thenc           do 70 k = 1, kypc           do 60 j = 1, nxc           do 55 i = 1, 3c           cu2(j,k+kyp,l) = cu(j,k,ll+1)c  55       continuec  60       continuec  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         call MPI_IRECV(cu2(1,1,1,l),3*kyp*nxv,mreal,ll-1,moff+1,lgrp,ms     1id,ierr)         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),3*kyp*nxv,mreal,ll,moff+1,lg     1rp,nsid,ierr)         endif      endif      if (lm.le.(kyb2/2)) then         call MPI_SEND(cu(1,1,1,l),3*kyp*nxv,mreal,lm-1,moff+1,lgrp,ierr     1)      endifc wait for data and unpack it      if (ll.le.kyb) then         call MPI_WAIT(msid,istatus,ierr)         do 50 k = 2, kyp         k1 = kyp - k + 2         k2 = (k1 - 1)/2 + 1         joff = nxv*(k1 - 2*k2 + 1)         do 40 j = 1, nxv         do 35 i = 1, 3         cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)   35    continue   40    continue   50    continue         if (kyp.lt.kyp2) then            call MPI_WAIT(nsid,istatus,ierr)            do 70 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 60 j = 1, nxv            do 55 i = 1, 3            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   55       continue   60       continue   70       continue         endif      endif   80 continuec copy to double array in y direction      do 140 l = 1, k2blok      koff = kyp2*(l + ks)      ll = (ny2 - koff - 1)/kyp + 1      koff = kyp*(l + ks)      lm = (ny2 - koff - 1)/kyp2 + 1      koff = koff + kyp2*lm - ny2c this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((ll+1).le.kyb) thenc           do 90 j = 1, nxc           do 85 i = 1, 3c           cu2(i,j,1,l) = cu(i,j,1,ll+1)c  85       continuec  90       continuec        endifc        if (kyp.lt.kyp2) thenc           do 110 k = 1, kypc           do 100 j = 1, nxc           do 95 i = 1, 3c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll)c  95       continuec 100       continuec 110       continuec        endifc        if (kyp.gt.1) thenc           do 130 k = 2, kypc           do 120 j = 1, nxc           do 115 i = 1, 3c           cu2(i,j,k,l) = cu(i,j,k,ll-1)c 115       continuec 120       continuec 130       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_IRECV(cu2(1,1,1,l),3*nxv,mreal,ll,moff+2,lgrp,lsid,     1ierr)         endif         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),3*kyp*nxv,mreal,ll-1,moff+2,     1lgrp,msid,ierr)         endif         if (kyp.gt.1) then            call MPI_IRECV(cu2(1,1,2,l),3*(kyp-1)*nxv,mreal,ll-2,moff+2,     1lgrp,nsid,ierr)         endif      endif      if ((lm.gt.(kyb2/2)).and.(lm.le.kyb2)) then         if (koff.eq.0) then            if ((lm+1).le.kyb2) then               call MPI_SEND(cu(1,1,1,l),3*nxv,mreal,lm,moff+2,lgrp,ierr     1)            endif            if (kyp.gt.1) then               call MPI_SEND(cu(1,1,2,l),3*(kyp-1)*nxv,mreal,lm-1,moff+2     1,lgrp,ierr)            endif         else            call MPI_SEND(cu(1,1,1,l),3*kyp*nxv,mreal,lm-1,moff+2,lgrp,i     1err)         endif      endifc wait for data and unpack it      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_WAIT(lsid,istatus,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_WAIT(msid,istatus,ierr)            do 100 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 90 j = 1, nxv            do 85 i = 1, 3            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   85       continue   90       continue  100       continue         endif         if (kyp.gt.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 120 k = 3, kyp            k1 = kyp - k + 3            k2 = k1/2 + 1            joff = nxv*(k1 - 2*k2 + 2)            do 110 j = 1, nxv            do 105 i = 1, 3            cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)  105       continue  110       continue  120       continue         endif      endif  140 continuec create odd array      do 200 l = 1, k2blok      koff = kyp2*(l + ks)      do 190 k = 1, kyp2      kk = k + koff      if ((kk.eq.1).or.(kk.eq.(ny+1))) then         do 150 j = 1, nx         do 145 i = 1, 3         cu2(i,j,k,l) = 0.         cu2(i,j+nx,k,l) = 0.  145    continue  150    continue      else if (kk.le.ny) then         do 160 j = 1, nxs         cu2(1,nx+j+1,k,l) = cu2(1,nx-j+1,k,l)         cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)         cu2(3,nx+j+1,k,l) = -cu2(3,nx-j+1,k,l)  160    continue         do 165 i = 1, 3         cu2(i,1,k,l) = 0.         cu2(i,nx+1,k,l) = 0.  165    continue      else if (kk.gt.(ny+1)) then         if (k.eq.1) then            do 170 j = 1, nxs            cu2(1,nx+j+1,k,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)            cu2(3,nx+j+1,k,l) = cu2(3,nx-j+1,k,l)  170       continue         else            do 180 j = 1, nxs            cu2(1,nx+j+1,kyp2-k+2,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,kyp2-k+2,l) = -cu2(2,nx-j+1,k,l)            cu2(3,nx+j+1,kyp2-k+2,l) = cu2(3,nx-j+1,k,l)  180       continue         endif         do 185 i = 1, 3         cu2(i,1,k,l) = 0.         cu2(i,nx+1,k,l) = 0.  185    continue      endif  190 continue  200 continuec finish odd array      do 230 l = 1, k2blok      koff = kyp2*(l + ks)      do 220 k = 1, kyp2      kk = k + koff      if (kk.gt.(ny+1)) then         do 210 j = 1, nxs         cu2(1,nx-j+1,k,l) = cu2(1,nx+j+1,k,l)         cu2(2,nx-j+1,k,l) = -cu2(2,nx+j+1,k,l)         cu2(3,nx-j+1,k,l) = -cu2(3,nx+j+1,k,l)  210    continue         cu2(1,nx+1,k,l) = cu2(1,nx+1,k,l)         cu2(2,nx+1,k,l) = -cu2(2,nx+1,k,l)         cu2(3,nx+1,k,l) = -cu2(3,nx+1,k,l)      endif  220 continue  230 continue      return      endc-----------------------------------------------------------------------      subroutine PDBLCOS2C(cu,cu2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2     1blok)c this subroutine creates a doubled vector array cu2 from a vector arrayc cu, so that various 2d sine/cosine transforms can be performed with ac 2d real to complex fft.  the x component is an odd function in y,c and y component is an odd function in x.c linear interpolation for distributed datac cu2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = second dimension of input array cu, must be >= nx+1c kyp = number of data values per block in yc kypd = third dimension of input array cu, must be >= kyp+1c kyp2 = third dimension of output array cu2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real cu, cu2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension cu(2,nxv,kypd,kblok), cu2(2,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lsid, msid, nsid, ierr      integer i, j, k, l, nxs, nys, ny2, kyb, kyb2, ks, koff, moff      integer kk, ll, lm, k1, k2, joff      real at1, at2      dimension istatus(lstat)      nxs = nx - 1      nys = ny - 1      kyb = ny/kyp      ny2 = ny + ny      kyb2 = ny2/kyp2      ks = kstrt - 2      moff = kypd + kybc copy to double array in x direction      do 80 l = 1, k2blok      koff = kyp2*(l + ks)      ll = koff/kyp + 1      koff = kyp*(l + ks)      lm = koff/kyp2 + 1c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, nys         do 10 j = 1, nxs         cu2(1,j+1,k+1,l) = cu(1,j+1,k+1,l)         cu2(2,j+1,k+1,l) = cu(2,j+1,k+1,l)         cu2(1,nx+j+1,k+1,l) = -cu(1,nx-j+1,k+1,l)         cu2(2,nx+j+1,k+1,l) = cu(2,nx-j+1,k+1,l)         cu2(1,j+1,ny+k+1,l) = cu(1,j+1,ny-k+1,l)         cu2(2,j+1,ny+k+1,l) = -cu(2,j+1,ny-k+1,l)         cu2(1,nx+j+1,ny+k+1,l) = -cu(1,nx-j+1,ny-k+1,l)         cu2(2,nx+j+1,ny+k+1,l) = -cu(2,nx-j+1,ny-k+1,l)   10    continue         cu2(1,1,k+1,l) = 0.         cu2(2,1,k+1,l) = cu(2,1,k+1,l)         cu2(1,nx+1,k+1,l) = 0.         cu2(2,nx+1,k+1,l) = cu(2,nx+1,k+1,l)         cu2(1,1,k+ny+1,l) = 0.         cu2(2,1,k+ny+1,l) = -cu(2,1,ny-k+1,l)         cu2(1,nx+1,k+ny+1,l) = 0.         cu2(2,nx+1,k+ny+1,l) = -cu(2,nx+1,ny-k+1,l)   20    continue         do 30 j = 1, nxs         cu2(1,j+1,1,l) = cu(1,j+1,1,l)         cu2(2,j+1,1,l) = 0.         cu2(1,j+nx+1,1,l) = -cu(1,nx-j+1,1,l)         cu2(2,j+nx+1,1,l) = 0.         cu2(1,j+1,ny+1,l) = cu(1,j+1,ny+1,l)         cu2(2,j+1,ny+1,l) = 0.         cu2(1,j+nx+1,ny+1,l) = -cu(1,nx-j+1,ny+1,l)         cu2(2,j+nx+1,ny+1,l) = 0.   30    continue         cu2(1,1,1,l) = 0.         cu2(2,1,1,l) = 0.         cu2(1,nx+1,1,l) = 0.         cu2(2,nx+1,1,l) = 0.         cu2(1,1,ny+1,l) = 0.         cu2(2,1,ny+1,l) = 0.         cu2(1,nx+1,ny+1,l) = 0.         cu2(2,nx+1,ny+1,l) = 0.         return      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        do 50 k = 1, kypc        do 40 j = 1, nxc        do 35 i = 1, 2c        cu2(i,j,k,l) = cu(i,j,k,ll)c  35    continuec  40    continuec  50    continuec        if (kyp.lt.kyp2) thenc           do 70 k = 1, kypc           do 60 j = 1, nxc           do 55 i = 1, 2c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll+1)c  55       continuec  60       continuec  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         call MPI_IRECV(cu2(1,1,1,l),2*kyp*nxv,mreal,ll-1,moff+1,lgrp,ms     1id,ierr)         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),2*kyp*nxv,mreal,ll,moff+1,lg     1rp,nsid,ierr)         endif      endif      if (lm.le.(kyb2/2)) then         call MPI_SEND(cu(1,1,1,l),2*kyp*nxv,mreal,lm-1,moff+1,lgrp,ierr     1)      endifc wait for data and unpack it      if (ll.le.kyb) then         call MPI_WAIT(msid,istatus,ierr)         do 50 k = 2, kyp         k1 = kyp - k + 2         k2 = (k1 - 1)/2 + 1         joff = nxv*(k1 - 2*k2 + 1)         do 40 j = 1, nxv         do 35 i = 1, 2         cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)   35    continue   40    continue   50    continue         if (kyp.lt.kyp2) then            call MPI_WAIT(nsid,istatus,ierr)            do 70 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 60 j = 1, nxv            do 55 i = 1, 2            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   55       continue   60       continue   70       continue         endif      endif   80 continuec copy to double array in y direction      do 140 l = 1, k2blok      koff = kyp2*(l + ks)      ll = (ny2 - koff - 1)/kyp + 1      koff = kyp*(l + ks)      lm = (ny2 - koff - 1)/kyp2 + 1      koff = koff + kyp2*lm - ny2c this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((ll+1).le.kyb) thenc           do 90 j = 1, nxc           do 85 i = 1, 2c           cu2(i,j,1,l) = cu(i,j,1,ll+1)c  85       continuec  90       continuec        endifc        if (kyp.lt.kyp2) thenc           do 110 k = 1, kypc           do 100 j = 1, nxc           do 95 i = 1, 2c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll)c  95       continuec 100       continuec 110       continuec        endifc        if (kyp.gt.1) thenc           do 130 k = 2, kypc           do 120 j = 1, nxc           do 115 i = 1, 2c           cu2(i,j,k,l) = cu(i,j,k,ll-1)c 115       continuec 120       continuec 130       continuec        endifc     endifc ny+1 point is specialc     if (kyp2*(l+ks).eq.ny) thenc        do 136 j = 1, nx+1c        do 135 i = 1, 2c        cu2(i,j,1,l) = cu(i,j,kyp+1,kyb)c 135    continuec 136    continuec     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_IRECV(cu2(1,1,1,l),2*nxv,mreal,ll,moff+2,lgrp,lsid,     1ierr)         endif         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),2*kyp*nxv,mreal,ll-1,moff+2,     1lgrp,msid,ierr)         endif         if (kyp.gt.1) then            call MPI_IRECV(cu2(1,1,2,l),2*(kyp-1)*nxv,mreal,ll-2,moff+2,     1lgrp,nsid,ierr)         endif      endif      if ((lm.gt.(kyb2/2)).and.(lm.le.kyb2)) then         if (koff.eq.0) then            if ((lm+1).le.kyb2) then               call MPI_SEND(cu(1,1,1,l),2*nxv,mreal,lm,moff+2,lgrp,ierr     1)            endif            if (kyp.gt.1) then               call MPI_SEND(cu(1,1,2,l),2*(kyp-1)*nxv,mreal,lm-1,moff+2     1,lgrp,ierr)            endif         else            call MPI_SEND(cu(1,1,1,l),2*kyp*nxv,mreal,lm-1,moff+2,lgrp,i     1err)         endif      endifc wait for data and unpack it      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_WAIT(lsid,istatus,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_WAIT(msid,istatus,ierr)            do 100 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 90 j = 1, nxv            do 85 i = 1, 2            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   85       continue   90       continue  100       continue         endif         if (kyp.gt.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 120 k = 3, kyp            k1 = kyp - k + 3            k2 = k1/2 + 1            joff = nxv*(k1 - 2*k2 + 2)            do 110 j = 1, nxv            do 105 i = 1, 2            cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)  105       continue  110       continue  120       continue         endif      endifc ny+1 point is special      if (kyp2*(l+ks).eq.ny) then         call MPI_IRECV(cu2(1,1,1,l),2*nxv,mreal,kyb-1,moff+3,lgrp,msid,     1ierr)      endif      if (kyp*(l+ks+1).eq.ny) then         kk = ny/kyp2         call MPI_SEND(cu(1,1,kyp+1,l),2*nxv,mreal,kk,moff+3,lgrp,ierr)      endif      if (kyp2*(l+ks).eq.ny) then         call MPI_WAIT(msid,istatus,ierr)      endif  140 continuec create even array      do 200 l = 1, k2blok      koff = kyp2*(l + ks)      do 190 k = 1, kyp2      kk = k + koff      if ((kk.eq.1).or.(kk.eq.(ny+1))) then         do 150 j = 1, nxs         cu2(1,j+1,k,l) = cu2(1,j+1,k,l)         cu2(2,j+1,k,l) = 0.         cu2(1,j+nx+1,k,l) = -cu2(1,nx-j+1,k,l)         cu2(2,j+nx+1,k,l) = 0.  150    continue         cu2(1,1,k,l) = 0.         cu2(2,1,k,l) = 0.         cu2(1,nx+1,k,l) = 0.         cu2(2,nx+1,k,l) = 0.      else if (kk.le.ny) then         do 160 j = 1, nxs         cu2(1,nx+j+1,k,l) = -cu2(1,nx-j+1,k,l)         cu2(2,nx+j+1,k,l) = cu2(2,nx-j+1,k,l)  160    continue         cu2(1,1,k,l) = 0.         cu2(2,1,k,l) = cu2(2,1,k,l)         cu2(1,nx+1,k,l) = 0.         cu2(2,nx+1,k,l) = cu2(2,nx+1,k,l)      else if (kk.gt.(ny+1)) then         if (k.eq.1) then            do 170 j = 1, nxs            cu2(1,nx+j+1,k,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)  170       continue            cu2(1,1,k,l) = 0.            cu2(2,1,k,l) = -cu2(2,1,k,l)            cu2(1,nx+1,k,l) = 0.            cu2(2,nx+1,k,l) = -cu2(2,nx+1,k,l)         else            do 180 j = 1, nxs            cu2(1,nx+j+1,kyp2-k+2,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,kyp2-k+2,l) = -cu2(2,nx-j+1,k,l)  180       continue            if (k.le.(kyp2/2+1)) then               at1 = -cu2(2,1,kyp2-k+2,l)               at2 = -cu2(2,nx+1,kyp2-k+2,l)               cu2(2,1,kyp2-k+2,l) = -cu2(2,1,k,l)               cu2(2,nx+1,kyp2-k+2,l) = -cu2(2,nx+1,k,l)               cu2(2,1,k,l) = at1               cu2(2,nx+1,k,l) = at2            endif            cu2(1,1,kyp2-k+2,l) = 0.            cu2(1,nx+1,kyp2-k+2,l) = 0.         endif      endif  190 continue  200 continuec finish even array      do 230 l = 1, k2blok      koff = kyp2*(l + ks)      do 220 k = 1, kyp2      kk = k + koff      if (kk.gt.(ny+1)) then         do 210 j = 1, nxs         cu2(1,nx-j+1,k,l) = -cu2(1,nx+j+1,k,l)         cu2(2,nx-j+1,k,l) = cu2(2,nx+j+1,k,l)  210    continue      endif  220 continue  230 continue      return      endc-----------------------------------------------------------------------      subroutine PDBLCOS2D(q,q2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2bl     1ok)c this subroutine creates an even array q2 from an array q, so thatc a 2d cosine transform can be performed with a 2d real to complex fft.c linear interpolation for distributed datac q2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = first dimension of input array q, must be >= nx+1c kyp = number of data values per block in yc kypd = second dimension of input array q, must be >= kyp+1c kyp2 = second dimension of output array q2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real q, q2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension q(nxv,kypd,kblok), q2(2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lsid, msid, nsid, ierr      integer j, k, l, nxs, nys, ny2, kyb, kyb2, ks, koff, moff      integer kk, ll, lm, k1, k2, joff      real at1, at2      dimension istatus(lstat)      nxs = nx - 1      nys = ny - 1      kyb = ny/kyp      ny2 = ny + ny      kyb2 = ny2/kyp2      ks = kstrt - 2      moff = kypd + kybc copy to double array in x direction      do 80 l = 1, k2blok      koff = kyp2*(l + ks)      ll = koff/kyp + 1      koff = kyp*(l + ks)      lm = koff/kyp2 + 1c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, nys         do 10 j = 1, nxs         q2(j+1,k+1,l) = q(j+1,k+1,l)         q2(nx+j+1,k+1,l) = q(nx-j+1,k+1,l)         q2(j+1,ny+k+1,l) = q(j+1,ny-k+1,l)         q2(nx+j+1,ny+k+1,l) = q(nx-j+1,ny-k+1,l)   10    continue         q2(1,k+1,l) = q(1,k+1,l)         q2(nx+1,k+1,l) = q(nx+1,k+1,l)         q2(1,k+ny+1,l) = q(1,ny-k+1,l)         q2(nx+1,k+ny+1,l) = q(nx+1,ny-k+1,l)   20    continue         do 30 j = 1, nxs         q2(j+1,1,l) = q(j+1,1,l)         q2(j+nx+1,1,l) = q(nx-j+1,1,l)         q2(j+1,ny+1,l) = q(j+1,ny+1,l)         q2(j+nx+1,ny+1,l) = q(nx-j+1,ny+1,l)   30    continue         q2(1,1,l) = q(1,1,l)         q2(nx+1,1,l) = q(nx+1,1,l)         q2(1,ny+1,l) = q(1,ny+1,l)         q2(nx+1,ny+1,l) = q(nx+1,ny+1,l)         return      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        do 50 k = 1, kypc        do 40 j = 1, nxc        q2(j,k,l) = q(j,k,ll)c  40    continuec  50    continuec        if (kyp.lt.kyp2) thenc           do 70 k = 1, kypc           do 60 j = 1, nxc           q2(j,k+kyp,l) = q(j,k,ll+1)c  60       continuec  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         call MPI_IRECV(q2(1,1,l),kyp*nxv,mreal,ll-1,moff+1,lgrp,msid,ie     1rr)         if (kyp.lt.kyp2) then            call MPI_IRECV(q2(1,kyp+1,l),kyp*nxv,mreal,ll,moff+1,lgrp,ns     1id,ierr)         endif      endif      if (lm.le.(kyb2/2)) then         call MPI_SEND(q(1,1,l),kyp*nxv,mreal,lm-1,moff+1,lgrp,ierr)      endifc wait for data and unpack it      if (ll.le.kyb) then         call MPI_WAIT(msid,istatus,ierr)         do 50 k = 2, kyp         k1 = kyp - k + 2         k2 = (k1 - 1)/2 + 1         joff = nxv*(k1 - 2*k2 + 1)         do 40 j = 1, nxv         q2(j,k1,l) = q2(j+joff,k2,l)   40    continue   50    continue         if (kyp.lt.kyp2) then            call MPI_WAIT(nsid,istatus,ierr)            do 70 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 60 j = 1, nxv            q2(j,k1+kyp,l) = q2(j+joff,k2+kyp,l)   60       continue   70       continue         endif      endif   80 continuec copy to double array in y direction      do 140 l = 1, k2blok      koff = kyp2*(l + ks)      ll = (ny2 - koff - 1)/kyp + 1      koff = kyp*(l + ks)      lm = (ny2 - koff - 1)/kyp2 + 1      koff = koff + kyp2*lm - ny2c this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((ll+1).le.kyb) thenc           do 90 j = 1, nxc           q2(j,1,l) = q(j,1,ll+1)c  90       continuec        endifc        if (kyp.lt.kyp2) thenc           do 110 k = 1, kypc           do 100 j = 1, nxc           q2(j,k+kyp,l) = q(j,k,ll)c 100       continuec 110       continuec        endifc        if (kyp.gt.1) thenc           do 130 k = 2, kypc           do 120 j = 1, nxc           q2(j,k,l) = q(j,k,ll-1)c 120       continuec 130       continuec        endifc     endifc ny+1 point is specialc     if (kyp2*(l+ks).eq.ny) thenc        do 135 j = 1, nx+1c        q2(j,1,l) = q(j,kyp+1,kyb)c 135    continuec     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_IRECV(q2(1,1,l),nxv,mreal,ll,moff+2,lgrp,lsid,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_IRECV(q2(1,kyp+1,l),kyp*nxv,mreal,ll-1,moff+2,lgrp,     1msid,ierr)         endif         if (kyp.gt.1) then            call MPI_IRECV(q2(1,2,l),(kyp-1)*nxv,mreal,ll-2,moff+2,lgrp,     1nsid,ierr)         endif      endif      if ((lm.gt.(kyb2/2)).and.(lm.le.kyb2)) then         if (koff.eq.0) then            if ((lm+1).le.kyb2) then               call MPI_SEND(q(1,1,l),nxv,mreal,lm,moff+2,lgrp,ierr)            endif            if (kyp.gt.1) then               call MPI_SEND(q(1,2,l),(kyp-1)*nxv,mreal,lm-1,moff+2,lgrp     1,ierr)            endif         else            call MPI_SEND(q(1,1,l),kyp*nxv,mreal,lm-1,moff+2,lgrp,ierr)         endif      endifc wait for data and unpack it      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_WAIT(lsid,istatus,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_WAIT(msid,istatus,ierr)            do 100 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 90 j = 1, nxv            q2(j,k1+kyp,l) = q2(j+joff,k2+kyp,l)   90       continue  100       continue         endif         if (kyp.gt.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 120 k = 3, kyp            k1 = kyp - k + 3            k2 = k1/2 + 1            joff = nxv*(k1 - 2*k2 + 2)            do 110 j = 1, nxv            q2(j,k1,l) = q2(j+joff,k2,l)  110       continue  120       continue         endif      endifc ny+1 point is special      if (kyp2*(l+ks).eq.ny) then         call MPI_IRECV(q2(1,1,l),nxv,mreal,kyb-1,moff+3,lgrp,msid,ierr)      endif      if (kyp*(l+ks+1).eq.ny) then         kk = ny/kyp2         call MPI_SEND(q(1,kyp+1,l),nxv,mreal,kk,moff+3,lgrp,ierr)      endif      if (kyp2*(l+ks).eq.ny) then         call MPI_WAIT(msid,istatus,ierr)      endif  140 continuec create even array      do 200 l = 1, k2blok      koff = kyp2*(l + ks)      do 190 k = 1, kyp2      kk = k + koff      if ((kk.eq.1).or.(kk.eq.(ny+1))) then         do 150 j = 1, nxs         q2(j+1,k,l) = q2(j+1,k,l)         q2(j+nx+1,k,l) = q2(nx-j+1,k,l)  150    continue         q2(1,k,l) = q2(1,k,l)         q2(nx+1,k,l) = q2(nx+1,k,l)      else if (kk.le.ny) then         do 160 j = 1, nxs         q2(nx+j+1,k,l) = q2(nx-j+1,k,l)  160    continue         q2(1,k,l) = q2(1,k,l)         q2(nx+1,k,l) = q2(nx+1,k,l)      else if (kk.gt.(ny+1)) then         if (k.eq.1) then            do 170 j = 1, nxs            q2(nx+j+1,k,l) = q2(nx-j+1,k,l)  170       continue            q2(1,k,l) = q2(1,k,l)            q2(nx+1,k,l) = q2(nx+1,k,l)         else            do 180 j = 1, nxs            q2(nx+j+1,kyp2-k+2,l) = q2(nx-j+1,k,l)  180       continue            if (k.le.(kyp2/2+1)) then               at1 = q2(1,kyp2-k+2,l)               at2 = q2(nx+1,kyp2-k+2,l)               q2(1,kyp2-k+2,l) = q2(1,k,l)               q2(nx+1,kyp2-k+2,l) = q2(nx+1,k,l)               q2(1,k,l) = at1               q2(nx+1,k,l) = at2            endif         endif      endif  190 continue  200 continuec finish even array      do 230 l = 1, k2blok      koff = kyp2*(l + ks)      do 220 k = 1, kyp2      kk = k + koff      if (kk.gt.(ny+1)) then         do 210 j = 1, nxs         q2(nx-j+1,k,l) = q2(nx+j+1,k,l)  210    continue      endif  220 continue  230 continue      return      endc-----------------------------------------------------------------------      subroutine PDBLCOS2B(cu,cu2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2     1blok)c this subroutine creates a doubled vector array cu2 from a vector arrayc cu, so that various 2d sine/cosine transforms can be performed with ac 2d real to complex fft.  the x component is an odd function in x,c y component is an odd function in x, and the z component is an evenc function in both x and y.c linear interpolation for distributed datac cu2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = first dimension of input array q, must be >= nx+1c kyp = number of data values per block in yc kypd = second dimension of input array q, must be >= kyp+1c kyp2 = second dimension of output array q2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real cu, cu2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension cu(3,nxv,kypd,kblok), cu2(3,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, lsid, msid, nsid, ierr      integer i, j, k, l, nxs, nys, ny2, kyb, kyb2, ks, koff, moff      integer kk, ll, lm, k1, k2, joff      real at1, at2      dimension istatus(lstat)      nxs = nx - 1      nys = ny - 1      kyb = ny/kyp      ny2 = ny + ny      kyb2 = ny2/kyp2      ks = kstrt - 2      moff = kypd + kybc copy to double array in x direction      do 80 l = 1, k2blok      koff = kyp2*(l + ks)      ll = koff/kyp + 1      koff = kyp*(l + ks)      lm = koff/kyp2 + 1c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, nys         do 10 j = 1, nxs         cu2(1,j+1,k+1,l) = cu(1,j+1,k+1,l)         cu2(2,j+1,k+1,l) = cu(2,j+1,k+1,l)         cu2(3,j+1,k+1,l) = cu(3,j+1,k+1,l)         cu2(1,nx+j+1,k+1,l) = -cu(1,nx-j+1,k+1,l)         cu2(2,nx+j+1,k+1,l) = cu(2,nx-j+1,k+1,l)         cu2(3,nx+j+1,k+1,l) = cu(3,nx-j+1,k+1,l)         cu2(1,j+1,ny+k+1,l) = cu(1,j+1,ny-k+1,l)         cu2(2,j+1,ny+k+1,l) = -cu(2,j+1,ny-k+1,l)         cu2(3,j+1,ny+k+1,l) = cu(3,j+1,ny-k+1,l)         cu2(1,nx+j+1,ny+k+1,l) = -cu(1,nx-j+1,ny-k+1,l)         cu2(2,nx+j+1,ny+k+1,l) = -cu(2,nx-j+1,ny-k+1,l)         cu2(3,nx+j+1,ny+k+1,l) = cu(3,nx-j+1,ny-k+1,l)   10    continue         cu2(1,1,k+1,l) = 0.         cu2(2,1,k+1,l) = cu(2,1,k+1,l)         cu2(3,1,k+1,l) = cu(3,1,k+1,l)         cu2(1,nx+1,k+1,l) = 0.         cu2(2,nx+1,k+1,l) = cu(2,nx+1,k+1,l)         cu2(3,nx+1,k+1,l) = cu(3,nx+1,k+1,l)         cu2(1,1,k+ny+1,l) = 0.         cu2(2,1,k+ny+1,l) = -cu(2,1,ny-k+1,l)         cu2(3,1,k+ny+1,l) = cu(3,1,ny-k+1,l)         cu2(1,nx+1,k+ny+1,l) = 0.         cu2(2,nx+1,k+ny+1,l) = -cu(2,nx+1,ny-k+1,l)         cu2(3,nx+1,k+ny+1,l) = cu(3,nx+1,ny-k+1,l)   20    continue         do 30 j = 1, nxs         cu2(1,j+1,1,l) = cu(1,j+1,1,l)         cu2(2,j+1,1,l) = 0.         cu2(3,j+1,1,l) = cu(3,j+1,1,l)         cu2(1,j+nx+1,1,l) = -cu(1,nx-j+1,1,l)         cu2(2,j+nx+1,1,l) = 0.         cu2(3,j+nx+1,1,l) = cu(3,nx-j+1,1,l)         cu2(1,j+1,ny+1,l) = cu(1,j+1,ny+1,l)         cu2(2,j+1,ny+1,l) = 0.         cu2(3,j+1,ny+1,l) = cu(3,j+1,ny+1,l)         cu2(1,j+nx+1,ny+1,l) = -cu(1,nx-j+1,ny+1,l)         cu2(2,j+nx+1,ny+1,l) = 0.         cu2(3,j+nx+1,ny+1,l) = cu(3,nx-j+1,ny+1,l)   30    continue         cu2(1,1,1,l) = 0.         cu2(2,1,1,l) = 0.         cu2(3,1,1,l) = cu(3,1,1,l)         cu2(1,nx+1,1,l) = 0.         cu2(2,nx+1,1,l) = 0.         cu2(3,nx+1,1,l) = cu(3,nx+1,1,l)         cu2(1,1,ny+1,l) = 0.         cu2(2,1,ny+1,l) = 0.         cu2(3,1,ny+1,l) = cu(3,1,ny+1,l)         cu2(1,nx+1,ny+1,l) = 0.         cu2(2,nx+1,ny+1,l) = 0.         cu2(3,nx+1,ny+1,l) = cu(3,nx+1,ny+1,l)         return      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        do 50 k = 1, kypc        do 40 j = 1, nxc        do 35 i = 1, 3c        cu2(i,j,k,l) = cu(i,j,k,ll)c  35    continuec  40    continuec  50    continuec        if (kyp.lt.kyp2) thenc           do 70 k = 1, kypc           do 60 j = 1, nxc           do 55 i = 1, 3c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll+1)c  55       continuec  60       continuec  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         call MPI_IRECV(cu2(1,1,1,l),3*kyp*nxv,mreal,ll-1,moff+1,lgrp,ms     1id,ierr)         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),3*kyp*nxv,mreal,ll,moff+1,lg     1rp,nsid,ierr)         endif      endif      if (lm.le.(kyb2/2)) then         call MPI_SEND(cu(1,1,1,l),3*kyp*nxv,mreal,lm-1,moff+1,lgrp,ierr     1)      endifc wait for data and unpack it      if (ll.le.kyb) then         call MPI_WAIT(msid,istatus,ierr)         do 50 k = 2, kyp         k1 = kyp - k + 2         k2 = (k1 - 1)/2 + 1         joff = nxv*(k1 - 2*k2 + 1)         do 40 j = 1, nxv         do 35 i = 1, 3         cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)   35    continue   40    continue   50    continue         if (kyp.lt.kyp2) then            call MPI_WAIT(nsid,istatus,ierr)            do 70 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 60 j = 1, nxv            do 55 i = 1, 3            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   55       continue   60       continue   70       continue         endif      endif   80 continuec copy to double array in y direction      do 140 l = 1, k2blok      koff = kyp2*(l + ks)      ll = (ny2 - koff - 1)/kyp + 1      koff = kyp*(l + ks)      lm = (ny2 - koff - 1)/kyp2 + 1      koff = koff + kyp2*lm - ny2c this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((ll+1).le.kyb) thenc           do 90 j = 1, nxc           do 85 i = 1, 3c           cu2(i,j,1,l) = cu(i,j,1,ll+1)c  85       continuec  90       continuec        endifc        if (kyp.lt.kyp2) thenc           do 110 k = 1, kypc           do 100 j = 1, nxc           do 95 i = 1, 3c           cu2(i,j,k+kyp,l) = cu(i,j,k,ll)c  95       continuec 100       continuec 110       continuec        endifc        if (kyp.gt.1) thenc           do 130 k = 2, kypc           do 120 j = 1, nxc           do 115 i = 1, 3c           cu2(i,j,k,l) = cu(i,j,k,ll-1)c 115       continuec 120       continuec 130       continuec        endifc     endifc ny+1 point is specialc     if (kyp2*(l+ks).eq.ny) thenc        do 136 j = 1, nx+1c        do 135 i = 1, 3c        cu2(i,j,1,l) = cu(i,j,kyp+1,kyb)c 135    continuec 136    continuec     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_IRECV(cu2(1,1,1,l),3*nxv,mreal,ll,moff+2,lgrp,lsid,     1ierr)         endif         if (kyp.lt.kyp2) then            call MPI_IRECV(cu2(1,1,kyp+1,l),3*kyp*nxv,mreal,ll-1,moff+2,     1lgrp,msid,ierr)         endif         if (kyp.gt.1) then            call MPI_IRECV(cu2(1,1,2,l),3*(kyp-1)*nxv,mreal,ll-2,moff+2,     1lgrp,nsid,ierr)         endif      endif      if ((lm.gt.(kyb2/2)).and.(lm.le.kyb2)) then         if (koff.eq.0) then            if ((lm+1).le.kyb2) then               call MPI_SEND(cu(1,1,1,l),3*nxv,mreal,lm,moff+2,lgrp,ierr     1)            endif            if (kyp.gt.1) then               call MPI_SEND(cu(1,1,2,l),3*(kyp-1)*nxv,mreal,lm-1,moff+2     1,lgrp,ierr)            endif         else            call MPI_SEND(cu(1,1,1,l),3*kyp*nxv,mreal,lm-1,moff+2,lgrp,i     1err)         endif      endifc wait for data and unpack it      if (ll.le.kyb) then         if ((ll+1).le.kyb) then            call MPI_WAIT(lsid,istatus,ierr)         endif         if (kyp.lt.kyp2) then            call MPI_WAIT(msid,istatus,ierr)            do 100 k = 2, kyp            k1 = kyp - k + 2            k2 = (k1 - 1)/2 + 1            joff = nxv*(k1 - 2*k2 + 1)            do 90 j = 1, nxv            do 85 i = 1, 3            cu2(i,j,k1+kyp,l) = cu2(i,j+joff,k2+kyp,l)   85       continue   90       continue  100       continue         endif         if (kyp.gt.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 120 k = 3, kyp            k1 = kyp - k + 3            k2 = k1/2 + 1            joff = nxv*(k1 - 2*k2 + 2)            do 110 j = 1, nxv            do 105 i = 1, 3            cu2(i,j,k1,l) = cu2(i,j+joff,k2,l)  105       continue  110       continue  120       continue         endif      endifc ny+1 point is special      if (kyp2*(l+ks).eq.ny) then         call MPI_IRECV(cu2(1,1,1,l),3*nxv,mreal,kyb-1,moff+3,lgrp,msid,     1ierr)      endif      if (kyp*(l+ks+1).eq.ny) then         kk = ny/kyp2         call MPI_SEND(cu(1,1,kyp+1,l),3*nxv,mreal,kk,moff+3,lgrp,ierr)      endif      if (kyp2*(l+ks).eq.ny) then         call MPI_WAIT(msid,istatus,ierr)      endif  140 continuec create even array      do 200 l = 1, k2blok      koff = kyp2*(l + ks)      do 190 k = 1, kyp2      kk = k + koff      if ((kk.eq.1).or.(kk.eq.(ny+1))) then         do 150 j = 1, nxs         cu2(1,j+1,k,l) = cu2(1,j+1,k,l)         cu2(2,j+1,k,l) = 0.         cu2(3,j+1,k,l) = cu2(3,j+1,k,l)         cu2(1,j+nx+1,k,l) = -cu2(1,nx-j+1,k,l)         cu2(2,j+nx+1,k,l) = 0.         cu2(3,j+nx+1,k,l) = cu2(3,nx-j+1,k,l)  150    continue         cu2(1,1,k,l) = 0.         cu2(2,1,k,l) = 0.         cu2(3,1,k,l) = cu2(3,1,k,l)         cu2(1,nx+1,k,l) = 0.         cu2(2,nx+1,k,l) = 0.         cu2(3,nx+1,k,l) = cu2(3,nx+1,k,l)      else if (kk.le.ny) then         do 160 j = 1, nxs         cu2(1,nx+j+1,k,l) = -cu2(1,nx-j+1,k,l)         cu2(2,nx+j+1,k,l) = cu2(2,nx-j+1,k,l)         cu2(3,nx+j+1,k,l) = cu2(3,nx-j+1,k,l)  160    continue         cu2(1,1,k,l) = 0.         cu2(2,1,k,l) = cu2(2,1,k,l)         cu2(3,1,k,l) = cu2(3,1,k,l)         cu2(1,nx+1,k,l) = 0.         cu2(2,nx+1,k,l) = cu2(2,nx+1,k,l)         cu2(3,nx+1,k,l) = cu2(3,nx+1,k,l)      else if (kk.gt.(ny+1)) then         if (k.eq.1) then            do 170 j = 1, nxs            cu2(1,nx+j+1,k,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,k,l) = -cu2(2,nx-j+1,k,l)            cu2(3,nx+j+1,k,l) = cu2(3,nx-j+1,k,l)  170       continue            cu2(1,1,k,l) = 0.            cu2(2,1,k,l) = -cu2(2,1,k,l)            cu2(3,1,k,l) = cu2(3,1,k,l)            cu2(1,nx+1,k,l) = 0.            cu2(2,nx+1,k,l) = -cu2(2,nx+1,k,l)            cu2(3,nx+1,k,l) = cu2(3,nx+1,k,l)         else            do 180 j = 1, nxs            cu2(1,nx+j+1,kyp2-k+2,l) = -cu2(1,nx-j+1,k,l)            cu2(2,nx+j+1,kyp2-k+2,l) = -cu2(2,nx-j+1,k,l)            cu2(3,nx+j+1,kyp2-k+2,l) = cu2(3,nx-j+1,k,l)  180       continue            if (k.le.(kyp2/2+1)) then               at1 = -cu2(2,1,kyp2-k+2,l)               at2 = -cu2(2,nx+1,kyp2-k+2,l)               cu2(2,1,kyp2-k+2,l) = -cu2(2,1,k,l)               cu2(2,nx+1,kyp2-k+2,l) = -cu2(2,nx+1,k,l)               cu2(2,1,k,l) = at1               cu2(2,nx+1,k,l) = at2               at1 = cu2(3,1,kyp2-k+2,l)               at2 = cu2(3,nx+1,kyp2-k+2,l)               cu2(3,1,kyp2-k+2,l) = cu2(3,1,k,l)               cu2(3,nx+1,kyp2-k+2,l) = cu2(3,nx+1,k,l)               cu2(3,1,k,l) = at1               cu2(3,nx+1,k,l) = at2            endif            cu2(1,1,kyp2-k+2,l) = 0.            cu2(1,nx+1,kyp2-k+2,l) = 0.         endif      endif  190 continue  200 continuec finish even array      do 230 l = 1, k2blok      koff = kyp2*(l + ks)      do 220 k = 1, kyp2      kk = k + koff      if (kk.gt.(ny+1)) then         do 210 j = 1, nxs         cu2(1,nx-j+1,k,l) = -cu2(1,nx+j+1,k,l)         cu2(2,nx-j+1,k,l) = cu2(2,nx+j+1,k,l)         cu2(3,nx-j+1,k,l) = cu2(3,nx+j+1,k,l)  210    continue      endif  220 continue  230 continue      return      endc-----------------------------------------------------------------------      subroutine PHAFDBL2C(fxy,fxy2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,     1k2blok)c this subroutine copies data from a double array to regular arrayc with guard cells for vector field and linear interpolationc for distributed datac fxy array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = second dimension of output array fxy, must be >= nxc kyp = number of data values per block in yc kypd = third dimension of output array fxy, must be >= kyp+1c kyp2 = third dimension of output array fxy2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real fxy, fxy2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension fxy(2,nxv,kypd,kblok), fxy2(2,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer j, k, l, nx1, ny1, kyb, kyb2, kyp1, ks, joff, koff, moff      integer kk, ll, lm      dimension istatus(lstat)      nx1 = nx + 1      ny1 = ny + 1      kyb = ny/kyp      kyb2 = (ny + ny)/kyp2      kyp1 = kyp + 1      ks = kstrt - 2      moff = kypd + kyb      do 90 l = 1, k2blok      koff = kyp2*(l + ks)      lm = koff/kyp + 1      koff = kyp*(l + ks)      ll = koff/kyp2 + 1      koff = koff - kyp2*(ll - 1)c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, ny1         do 10 j = 1, nx1         fxy(1,j,k,l) = fxy2(1,j,k,l)         fxy(2,j,k,l) = fxy2(2,j,k,l)   10    continue   20    continue         go to 90      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((koff.eq.0).and.(kyp.lt.kyp2)) thenc           do 40 k = 1, kyp1c           do 30 j = 1, nx1c           fxy(1,j,k,l) = fxy2(1,j,k,ll)c           fxy(2,j,k,l) = fxy2(2,j,k,ll)c  30       continuec  40       continuec        elsec           do 60 k = 1, kypc           do 50 j = 1, nx1c           fxy(1,j,k,l) = fxy2(1,j,k+koff,ll)c           fxy(2,j,k,l) = fxy2(2,j,k+koff,ll)c  50       continuec  60       continuec           do 70 j = 1, nx1c           fxy(1,j,kyp+1,l) = fxy2(1,j,1,ll+1)c           fxy(2,j,kyp+1,l) = fxy2(2,j,1,ll+1)c  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_IRECV(fxy(1,1,1,l),2*nxv*kyp1,mreal,ll-1,moff+3,lgr     1p,msid,ierr)         else            call MPI_IRECV(fxy(1,1,1,l),2*nxv*kyp,mreal,ll-1,moff+3,lgrp,     1,msid,ierr)            call MPI_IRECV(fxy(1,1,kyp+1,l),2*nxv,mreal,ll,moff+3,lgrp,n     1sid,ierr)         endif      endifc pack data and send it      if (lm.le.kyb) then         if (kyp.lt.kyp2) then            do 40 k = 2, kyp1            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 30 j = 1, nxv            fxy2(1,j+joff,kk,l) = fxy2(1,j,k,l)            fxy2(2,j+joff,kk,l) = fxy2(2,j,k,l)   30       continue   40       continue            call MPI_SEND(fxy2(1,1,1,l),2*nxv*kyp1,mreal,lm-1,moff+3,lgr     1p,ierr)            do 60 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 50 j = 1, nxv            fxy2(1,j+joff,kk+kyp,l) = fxy2(1,j,k+kyp,l)            fxy2(2,j+joff,kk+kyp,l) = fxy2(2,j,k+kyp,l)   50       continue   60       continue            call MPI_SEND(fxy2(1,1,kyp+1,l),2*nxv*kyp,mreal,lm,moff+3,lg     1rp,ierr)         else            do 80 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 70 j = 1, nxv            fxy2(1,j+joff,kk,l) = fxy2(1,j,k,l)            fxy2(2,j+joff,kk,l) = fxy2(2,j,k,l)   70       continue   80       continue            call MPI_SEND(fxy2(1,1,1,l),2*nxv*kyp,mreal,lm-1,moff+3,lgrp     1,ierr)         endif         if (lm.gt.1) then            call MPI_SEND(fxy2(1,1,1,l),2*nxv,mreal,lm-2,moff+3,lgrp,ier     1r)         endif      else if (lm.eq.(kyb+1)) then         call MPI_SEND(fxy2(1,1,1,l),2*nxv,mreal,lm-2,moff+3,lgrp,ierr)      endifc wait for data      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_WAIT(msid,istatus,ierr)         else            call MPI_WAIT(msid,istatus,ierr)            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   90 continue      return      endc-----------------------------------------------------------------------      subroutine PHAFDBL2D(q,q2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,k2bl     1ok)c this subroutine copies data from a double array to regular arrayc with guard cells for scalar field and linear interpolationc for distributed datac q2 array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = second dimension of output array fxy, must be >= nxc kyp = number of data values per block in yc kypd = third dimension of output array fxy, must be >= kyp+1c kyp2 = third dimension of output array fxy2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real q, q2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension q(nxv,kypd,kblok), q2(2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer j, k, l, nx1, ny1, kyb, kyb2, kyp1, ks, joff, koff, moff      integer  kk, ll, lm      dimension istatus(lstat)      nx1 = nx + 1      ny1 = ny + 1      kyb = ny/kyp      kyb2 = (ny + ny)/kyp2      kyp1 = kyp + 1      ks = kstrt - 2      moff = kypd + kyb      do 90 l = 1, k2blok      koff = kyp2*(l + ks)      lm = koff/kyp + 1      koff = kyp*(l + ks)      ll = koff/kyp2 + 1      koff = koff - kyp2*(ll - 1)c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, ny1         do 10 j = 1, nx1         q(j,k,l) = q2(j,k,l)   10    continue   20    continue         go to 90      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((koff.eq.0).and.(kyp.lt.kyp2)) thenc           do 40 k = 1, kyp1c           do 30 j = 1, nx1c           q(j,k,l) = q2(j,k,ll)c  30       continuec  40       continuec        elsec           do 60 k = 1, kypc           do 50 j = 1, nx1c           q(j,k,l) = q2(j,k+koff,ll)c  50       continuec  60       continuec           do 70 j = 1, nx1c           q(j,kyp+1,l) = q2(j,1,ll+1)c  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_IRECV(q(1,1,l),nxv*kyp1,mreal,ll-1,moff+4,lgrp,msid     1,ierr)         else            call MPI_IRECV(q(1,1,l),nxv*kyp,mreal,ll-1,moff+4,lgrp,msid,     1ierr)            call MPI_IRECV(q(1,kyp+1,l),nxv,mreal,ll,moff+4,lgrp,nsid,ie     1rr)         endif      endifc pack data and send it      if (lm.le.kyb) then         if (kyp.lt.kyp2) then            do 40 k = 2, kyp1            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 30 j = 1, nxv            q2(j+joff,kk,l) = q2(j,k,l)   30       continue   40       continue            call MPI_SEND(q2(1,1,l),nxv*kyp1,mreal,lm-1,moff+4,lgrp,ierr     1)            do 60 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 50 j = 1, nxv            q2(j+joff,kk+kyp,l) = q2(j,k+kyp,l)   50       continue   60       continue            call MPI_SEND(q2(1,kyp+1,l),nxv*kyp,mreal,lm,moff+4,lgrp,ier     1r)         else            do 80 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 70 j = 1, nxv            q2(j+joff,kk,l) = q2(j,k,l)   70       continue   80       continue            call MPI_SEND(q2(1,1,l),nxv*kyp,mreal,lm-1,moff+4,lgrp,ierr)         endif         if (lm.gt.1) then            call MPI_SEND(q2(1,1,l),nxv,mreal,lm-2,moff+4,lgrp,ierr)         endif      else if (lm.eq.(kyb+1)) then         call MPI_SEND(q2(1,1,l),nxv,mreal,lm-2,moff+4,lgrp,ierr)      endifc wait for data      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_WAIT(msid,istatus,ierr)         else            call MPI_WAIT(msid,istatus,ierr)            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   90 continue      return      endc-----------------------------------------------------------------------      subroutine PHAFDBL2B(fxy,fxy2,nx,ny,kstrt,nxv,kyp,kypd,kyp2,kblok,     1k2blok)c this subroutine copies data from a double array to regular arrayc with guard cells for vector field and linear interpolationc for distributed datac fxy array may be modifiedc nx/ny = system length in x/y directionc kstrt = starting data block numberc nxv = second dimension of output array fxy, must be >= nxc kyp = number of data values per block in yc kypd = third dimension of output array fxy, must be >= kyp+1c kyp2 = third dimension of output array fxy2, must be >= kyp2c kblok = number of data blocks in yc k2blok = number of data blocks in y for doubled data      implicit none      real fxy, fxy2      integer nx, ny, kstrt, nxv, kyp, kypd, kyp2, kblok, k2blok      dimension fxy(3,nxv,kypd,kblok), fxy2(3,2*nxv,kyp2,k2blok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer j, k, l, nx1, ny1, kyb, kyb2, kyp1, ks, joff, koff, moff      integer kk, ll, lm      dimension istatus(lstat)      nx1 = nx + 1      ny1 = ny + 1      kyb = ny/kyp      kyb2 = (ny + ny)/kyp2      kyp1 = kyp + 1      ks = kstrt - 2      moff = kypd + kyb      do 90 l = 1, k2blok      koff = kyp2*(l + ks)      lm = koff/kyp + 1      koff = kyp*(l + ks)      ll = koff/kyp2 + 1      koff = koff - kyp2*(ll - 1)c special case for one processor      if (kyb2.eq.1) then         do 20 k = 1, ny1         do 10 j = 1, nx1         fxy(1,j,k,l) = fxy2(1,j,k,l)         fxy(2,j,k,l) = fxy2(2,j,k,l)         fxy(3,j,k,l) = fxy2(3,j,k,l)   10    continue   20    continue         go to 90      endifc this segment is used for shared memory computersc     if (ll.le.kyb) thenc        if ((koff.eq.0).and.(kyp.lt.kyp2)) thenc           do 40 k = 1, kyp1c           do 30 j = 1, nx1c           fxy(1,j,k,l) = fxy2(1,j,k,ll)c           fxy(2,j,k,l) = fxy2(2,j,k,ll)c           fxy(3,j,k,l) = fxy2(3,j,k,ll)c  30       continuec  40       continuec        elsec           do 60 k = 1, kypc           do 50 j = 1, nx1c           fxy(1,j,k,l) = fxy2(1,j,k+koff,ll)c           fxy(2,j,k,l) = fxy2(2,j,k+koff,ll)c           fxy(3,j,k,l) = fxy2(3,j,k+koff,ll)c  50       continuec  60       continuec           do 70 j = 1, nx1c           fxy(1,j,kyp+1,l) = fxy2(1,j,1,ll+1)c           fxy(2,j,kyp+1,l) = fxy2(2,j,1,ll+1)c           fxy(3,j,kyp+1,l) = fxy2(3,j,1,ll+1)c  70       continuec        endifc     endifc this segment is used for mpi computers      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_IRECV(fxy(1,1,1,l),3*nxv*kyp1,mreal,ll-1,moff+3,lgr     1p,msid,ierr)         else            call MPI_IRECV(fxy(1,1,1,l),3*nxv*kyp,mreal,ll-1,moff+3,lgrp,     1,msid,ierr)            call MPI_IRECV(fxy(1,1,kyp+1,l),3*nxv,mreal,ll,moff+3,lgrp,n     1sid,ierr)         endif      endifc pack data and send it      if (lm.le.kyb) then         if (kyp.lt.kyp2) then            do 40 k = 2, kyp1            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 30 j = 1, nxv            fxy2(1,j+joff,kk,l) = fxy2(1,j,k,l)            fxy2(2,j+joff,kk,l) = fxy2(2,j,k,l)            fxy2(3,j+joff,kk,l) = fxy2(3,j,k,l)   30       continue   40       continue            call MPI_SEND(fxy2(1,1,1,l),3*nxv*kyp1,mreal,lm-1,moff+3,lgr     1p,ierr)            do 60 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 50 j = 1, nxv            fxy2(1,j+joff,kk+kyp,l) = fxy2(1,j,k+kyp,l)            fxy2(2,j+joff,kk+kyp,l) = fxy2(2,j,k+kyp,l)            fxy2(3,j+joff,kk+kyp,l) = fxy2(3,j,k+kyp,l)   50       continue   60       continue            call MPI_SEND(fxy2(1,1,kyp+1,l),3*nxv*kyp,mreal,lm,moff+3,lg     1rp,ierr)         else            do 80 k = 2, kyp            kk = (k - 1)/2 + 1            joff = nxv*(k - 2*kk + 1)            do 70 j = 1, nxv            fxy2(1,j+joff,kk,l) = fxy2(1,j,k,l)            fxy2(2,j+joff,kk,l) = fxy2(2,j,k,l)            fxy2(3,j+joff,kk,l) = fxy2(3,j,k,l)   70       continue   80       continue            call MPI_SEND(fxy2(1,1,1,l),3*nxv*kyp,mreal,lm-1,moff+3,lgrp     1,ierr)         endif         if (lm.gt.1) then            call MPI_SEND(fxy2(1,1,1,l),3*nxv,mreal,lm-2,moff+3,lgrp,ier     1r)         endif      else if (lm.eq.(kyb+1)) then         call MPI_SEND(fxy2(1,1,1,l),3*nxv,mreal,lm-2,moff+3,lgrp,ierr)      endifc wait for data      if (ll.le.kyb) then         if ((koff.eq.0).and.(kyp.lt.kyp2)) then            call MPI_WAIT(msid,istatus,ierr)         else            call MPI_WAIT(msid,istatus,ierr)            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   90 continue      return      endc-----------------------------------------------------------------------      subroutine PLCGUARD2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.  for vector datac the field is replicated so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(2,nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer nx2, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx2 = nx + 2      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 70 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nxvc        f(1,j,1,l) = f(1,j,kyp+1,kl)c        f(2,j,1,l) = f(2,j,kyp+1,kl)c  10    continuec     elsec        do 20 j = 1, nxvc        f(1,j,1,l) = f(1,j,3,l)c        f(2,j,1,l) = f(2,j,3,l)c  20    continuec     endifc     if (kr.le.nvp) thenc        do 30 j = 1, nxvc        f(1,j,kyp+2,l) = f(1,j,2,kr)c        f(2,j,kyp+2,l) = f(2,j,2,kr)c        f(1,j,kyp+3,l) = f(1,j,ngc+1,krr)c        f(2,j,kyp+3,l) = f(2,j,ngc+1,krr)c  30    continuec     elsec        do 40 j = 2, nx2c        f(1,j,kyp+3,l) = 2.*f(1,j,kyp+2,l) - f(1,j,kyp+1,l)c        f(2,j,kyp+3,l) = 2.*f(2,j,kyp+2,l) - f(2,j,kyp+1,l)c  40    continuec     endifc     if (kyp.eq.1) thenc        if ((kl.eq.0).and.(kr.le.nvp)) thenc           do 50 j = 1, nxvc           f(1,j,1,l) = f(1,j,2,kr)c           f(2,j,1,l) = f(2,j,2,kr)c  50       continuec        endifc        if (kr.eq.nvp) thenc           do 60 j = 1, nxvc           f(1,j,kyp+3,l) = f(1,j,kyp+2,kr)c           f(2,j,kyp+3,l) = f(2,j,kyp+2,kr)c  60       continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kl-1,moff+3,lgrp,msid,ier     1r)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+1,l),2*nxv,mreal,kr-1,moff+3,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nxv         f(1,j,1,l) = f(1,j,3,l)         f(2,j,1,l) = f(2,j,3,l)   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(f(1,1,kyp+2,l),2*ngc*nxv,mreal,kr-1,moff+4,lgrp,     1msid,ierr)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,2,l),2*ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 2, nx2         f(1,j,kyp+3,l) = 2.*f(1,j,kyp+2,l) - f(1,j,kyp+1,l)         f(2,j,kyp+3,l) = 2.*f(2,j,kyp+2,l) - f(2,j,kyp+1,l)   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (krr.le.nvp) then            call MPI_IRECV(f(1,1,kyp+3,l),2*nxv,mreal,krr-1,moff+6,lgrp,     1msid,ierr)         else if (kr.le.nvp) then            call MPI_IRECV(f(1,1,kyp+3,l),2*nxv,mreal,kr-1,moff+6,lgrp,m     1sid,ierr)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kr-1,moff+6,lgrp,nsid,     1ierr)         endif         if (kll.ge.1) then            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,kll-1,moff+6,lgrp,ierr)         else if (kl.eq.1) then            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,kl-1,moff+6,lgrp,ierr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_SEND(f(1,1,kyp+2,l),2*nxv,mreal,kl-1,moff+6,lgrp,ie     1rr)         endif         if (kr.le.nvp) then            call MPI_WAIT(msid,istatus,ierr)         endif         if (kl.eq.0) then            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   70 continuec fix left edge      do 100 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 90 j = 2, nx2         f(1,j,1,l) = 2.*f(1,j,2,l) - f(1,j,1,l)         f(2,j,1,l) = 2.*f(2,j,2,l) - f(2,j,1,l)   90    continue      endif  100 continue      return      endc-----------------------------------------------------------------------      subroutine PLDGUARD2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.c the field is replicated so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer nx2, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx2 = nx + 2      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 70 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nxvc        f(j,1,l) = f(j,kyp+1,kl)c  10    continuec     elsec        do 20 j = 1, nxvc        f(j,1,l) = f(j,3,l)c  20    continuec     endifc     if (kr.le.nvp) thenc        do 30 j = 1, nxvc        f(j,kyp+2,l) = f(j,2,kr)c        f(j,kyp+3,l) = f(j,ngc+1,krr)c  30    continuec     elsec        do 40 j = 2, nx2c        f(j,kyp+3,l) = 2.*f(j,kyp+2,l) - f(j,kyp+1,l)c  40    continuec     endifc     if (kyp.eq.1) thenc        if ((kl.eq.0).and.(kr.le.nvp)) thenc           do 50 j = 1, nxvc           f(j,1,l) = f(j,2,kr)c  50       continuec        endifc        if (kr.eq.nvp) thenc           do 60 j = 1, nxvc           f(j,kyp+3,l) = f(j,kyp+2,kr)c  60       continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+3,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+3,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nxv         f(j,1,l) = f(j,3,l)   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+4,lgrp,msid     1,ierr)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 2, nx2         f(j,kyp+3,l) = 2.*f(j,kyp+2,l) - f(j,kyp+1,l)   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (krr.le.nvp) then            call MPI_IRECV(f(1,kyp+3,l),nxv,mreal,krr-1,moff+6,lgrp,msid     1,ierr)         else if (kr.le.nvp) then            call MPI_IRECV(f(1,kyp+3,l),nxv,mreal,kr-1,moff+6,lgrp,msid,     1ierr)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kr-1,moff+6,lgrp,nsid,ierr     1)         endif         if (kll.ge.1) then            call MPI_SEND(f(1,2,l),nxv,mreal,kll-1,moff+6,lgrp,ierr)         else if (kl.eq.1) then            call MPI_SEND(f(1,2,l),nxv,mreal,kl-1,moff+6,lgrp,ierr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_SEND(f(1,kyp+2,l),nxv,mreal,kl-1,moff+6,lgrp,ierr)         endif         if (kr.le.nvp) then            call MPI_WAIT(msid,istatus,ierr)         endif         if (kl.eq.0) then            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   70 continuec fix left edge      do 100 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 90 j = 2, nx2         f(j,1,l) = 2.*f(j,2,l) - f(j,1,l)   90    continue      endif  100 continue      return      endc-----------------------------------------------------------------------      subroutine PLBGUARD2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.  for vector datac the field is replicated so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(3,nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer nx2, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx2 = nx + 2      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 70 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nxvc        f(1,j,1,l) = f(1,j,kyp+1,kl)c        f(2,j,1,l) = f(2,j,kyp+1,kl)c        f(3,j,1,l) = f(3,j,kyp+1,kl)c  10    continuec     elsec        do 20 j = 1, nxvc        f(1,j,1,l) = f(1,j,3,l)c        f(2,j,1,l) = f(2,j,3,l)c        f(3,j,1,l) = f(3,j,3,l)c  20    continuec     endifc     if (kr.le.nvp) thenc        do 30 j = 1, nxvc        f(1,j,kyp+2,l) = f(1,j,2,kr)c        f(2,j,kyp+2,l) = f(2,j,2,kr)c        f(3,j,kyp+2,l) = f(3,j,2,kr)c        f(1,j,kyp+3,l) = f(1,j,ngc+1,krr)c        f(2,j,kyp+3,l) = f(2,j,ngc+1,krr)c        f(3,j,kyp+3,l) = f(3,j,ngc+1,krr)c  30    continuec     elsec        do 40 j = 2, nx2c        f(1,j,kyp+3,l) = 2.*f(1,j,kyp+2,l) - f(1,j,kyp+1,l)c        f(2,j,kyp+3,l) = 2.*f(2,j,kyp+2,l) - f(2,j,kyp+1,l)c        f(3,j,kyp+3,l) = 2.*f(3,j,kyp+2,l) - f(3,j,kyp+1,l)c  40    continuec     endifc     if (kyp.eq.1) thenc        if ((kl.eq.0).and.(kr.le.nvp)) thenc           do 50 j = 1, nxvc           f(1,j,1,l) = f(1,j,2,kr)c           f(2,j,1,l) = f(2,j,2,kr)c           f(3,j,1,l) = f(3,j,2,kr)c  50       continuec        endifc        if (kr.eq.nvp) thenc           do 60 j = 1, nxvc           f(1,j,kyp+3,l) = f(1,j,kyp+2,kr)c           f(2,j,kyp+3,l) = f(2,j,kyp+2,kr)c           f(3,j,kyp+3,l) = f(3,j,kyp+2,kr)c  60       continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kl-1,moff+3,lgrp,msid,ier     1r)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+1,l),3*nxv,mreal,kr-1,moff+3,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nxv         f(1,j,1,l) = f(1,j,3,l)         f(2,j,1,l) = f(2,j,3,l)         f(3,j,1,l) = f(3,j,3,l)   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(f(1,1,kyp+2,l),3*ngc*nxv,mreal,kr-1,moff+4,lgrp,     1msid,ierr)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,2,l),3*ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 2, nx2         f(1,j,kyp+3,l) = 2.*f(1,j,kyp+2,l) - f(1,j,kyp+1,l)         f(2,j,kyp+3,l) = 2.*f(2,j,kyp+2,l) - f(2,j,kyp+1,l)         f(3,j,kyp+3,l) = 2.*f(3,j,kyp+2,l) - f(3,j,kyp+1,l)   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (krr.le.nvp) then            call MPI_IRECV(f(1,1,kyp+3,l),3*nxv,mreal,krr-1,moff+6,lgrp,     1msid,ierr)         else if (kr.le.nvp) then            call MPI_IRECV(f(1,1,kyp+3,l),3*nxv,mreal,kr-1,moff+6,lgrp,m     1sid,ierr)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kr-1,moff+6,lgrp,nsid,     1ierr)         endif         if (kll.ge.1) then            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,kll-1,moff+6,lgrp,ierr)         else if (kl.eq.1) then            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,kl-1,moff+6,lgrp,ierr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_SEND(f(1,1,kyp+2,l),3*nxv,mreal,kl-1,moff+6,lgrp,ie     1rr)         endif         if (kr.le.nvp) then            call MPI_WAIT(msid,istatus,ierr)         endif         if (kl.eq.0) then            call MPI_WAIT(nsid,istatus,ierr)         endif      endif   70 continuec fix left edge      do 100 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 90 j = 2, nx2         f(1,j,1,l) = 2.*f(1,j,2,l) - f(1,j,1,l)         f(2,j,1,l) = 2.*f(2,j,2,l) - f(2,j,1,l)         f(3,j,1,l) = 2.*f(3,j,2,l) - f(3,j,1,l)   90    continue      endif  100 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARD2(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok,ngds)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(3,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c scr(j,idps,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok, ngds      dimension f(3,nxv,nypmx,kblok), scr(3,nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l, m      dimension istatus(lstat)      nx3 = nx + 3      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 170 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nx3c        do 10 m = 1, 3c        scr(m,j,1,l) = f(m,j,kyp+2,kl)c        scr(m,j,2,l) = f(m,j,kyp+3,kll)c  10    continuec  20    continuec     elsec        do 40 j = 1, nx3c        do 30 m = 1, 3c        scr(m,j,1,l) = 2.*f(m,j,1,l)c        scr(m,j,2,l) = -f(m,j,1,l)c  30    continuec  40    continuec     endifc     if (kr.le.nvp) thenc        do 60 j = 1, nx3c        do 50 m = 1, 3c        scr(m,j,3,l) = f(m,j,1,kr)c  50    continuec  60    continuec     elsec        do 80 j = 1, nx3c        do 70 m = 1, 3c        scr(m,j,3,l) = -f(m,j,kyp+3,l)c        f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + 2.*f(m,j,kyp+3,l)c        f(m,j,kyp+3,l) = 0.c  70    continuec  80    continuec     endifc     if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 100 j = 1, nx3c           do 90 m = 1, 3c           scr(m,j,1,l) = f(m,j,kyp+2,kl)c           scr(m,j,2,l) = -f(m,j,1,kl)c  90       continuec 100       continuec        else if (kl.eq.0) thenc           do 120 j = 1, nx3c           do 110 m = 1, 3c           scr(m,j,2,l) = 0.c 110       continuec 120       continuec        endifc last point is special with only one gridc        if ((kl.eq.(nvp-1)).and.(kl.ge.1)) thenc           do 140 j = 1, nx3c           do 130 m = 1, 3c           f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + f(m,j,kyp+3,kl)c 130    continuec 140    continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(scr,3*ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+2,l),3*ngc*nxv,mreal,kr-1,moff+1,lgrp,i     1err)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nx3         do 10 m = 1, 3         scr(m,j,1,l) = 2.*f(m,j,1,l)         scr(m,j,2,l) = -f(m,j,1,l)   10    continue   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(scr(1,1,3,l),3*nxv,mreal,kr-1,moff+2,lgrp,msid,i     1err)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,1,l),3*nxv,mreal,kl-1,moff+2,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 1, nx3         do 30 m = 1, 3         scr(m,j,3,l) = -f(m,j,kyp+3,l)         f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + 2.*f(m,j,kyp+3,l)         f(m,j,kyp+3,l) = 0.   30    continue   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (kll.ge.1) then            call MPI_IRECV(scr(1,1,2,l),3*nxv,mreal,kll-1,moff+5,lgrp,ms     1id,ierr)         else if (kl.eq.1) then            call MPI_IRECV(scr(1,1,2,l),3*nxv,mreal,kl-1,moff+5,lgrp,msi     1d,ierr)         endif         if (krr.le.nvp) then            call MPI_SEND(f(1,1,kyp+3,l),3*nxv,mreal,krr-1,moff+5,lgrp,i     1err)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_SEND(f(1,1,1,l),3*nxv,mreal,kr-1,moff+5,lgrp,ierr)         endif         if (kl.ge.1) then            call MPI_WAIT(msid,istatus,ierr)            if (kl.eq.1) then               do 60 j = 1, nx3               do 50 m = 1, 3               scr(m,j,2,l) = -scr(m,j,2,l)   50          continue   60          continue            endif         else            do 80 j = 1, nx3            do 70 m = 1, 3            scr(m,j,2,l) = 0.   70       continue   80       continue         endifc last point is special with only one grid         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_IRECV(f(1,1,kyp+3,l),3*nxv,mreal,kl-1,moff+6,lgrp,m     1sid,ierr)         endif         if (kr.eq.nvp) then            call MPI_SEND(f(1,1,kyp+3,l),3*nxv,mreal,kr-1,moff+6,lgrp,ie     1rr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_WAIT(msid,istatus,ierr)            do 140 j = 1, nx3            do 130 m = 1, 3            f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + f(m,j,kyp+3,l)            f(m,j,kyp+3,l) = 0.  130       continue  140       continue         endif      endifc add up the guard cells      do 160 j = 1, nx3      do 150 m = 1, 3      f(m,j,2,l) = f(m,j,2,l) + scr(m,j,1,l)      f(m,j,ngc+1,l) = f(m,j,ngc+1,l) + scr(m,j,2,l)      f(m,j,kyp+1,l) = f(m,j,kyp+1,l) + scr(m,j,3,l)  150 continue  160 continue  170 continuec zero out the left edge      do 200 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 190 j = 1, nx3         do 180 m = 1, 3         f(m,j,1,l) = 0.  180    continue  190    continue      endif  200 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARD22(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok,ngds     1)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(2,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c scr(j,idps,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok, ngds      dimension f(2,nxv,nypmx,kblok), scr(2,nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l, m      dimension istatus(lstat)      nx3 = nx + 3      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 170 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nx3c        do 10 m = 1, 2c        scr(m,j,1,l) = f(m,j,kyp+2,kl)c        scr(m,j,2,l) = f(m,j,kyp+3,kll)c  10    continuec  20    continuec     elsec        do 40 j = 1, nx3c        do 30 m = 1, 2c        scr(m,j,1,l) = 2.*f(m,j,1,l)c        scr(m,j,2,l) = -f(m,j,1,l)c  30    continuec  40    continuec     endifc     if (kr.le.nvp) thenc        do 60 j = 1, nx3c        do 50 m = 1, 2c        scr(m,j,3,l) = f(m,j,1,kr)c  50    continuec  60    continuec     elsec        do 80 j = 1, nx3c        do 70 m = 1, 2c        scr(m,j,3,l) = -f(m,j,kyp+3,l)c        f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + 2.*f(m,j,kyp+3,l)c        f(m,j,kyp+3,l) = 0.c  70    continuec  80    continuec     endifc     if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 100 j = 1, nx3c           do 90 m = 1, 2c           scr(m,j,1,l) = f(m,j,kyp+2,kl)c           scr(m,j,2,l) = -f(m,j,1,kl)c  90       continuec 100       continuec        else if (kl.eq.0) thenc           do 120 j = 1, nx3c           do 110 m = 1, 2c           scr(m,j,2,l) = 0.c 110       continuec 120       continuec        endifc last point is special with only one gridc        if ((kl.eq.(nvp-1)).and.(kl.ge.1)) thenc           do 140 j = 1, nx3c           do 130 m = 1, 2c           f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + f(m,j,kyp+3,kl)c 130    continuec 140    continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(scr,2*ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,1,kyp+2,l),2*ngc*nxv,mreal,kr-1,moff+1,lgrp,i     1err)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nx3         do 10 m = 1, 2         scr(m,j,1,l) = 2.*f(m,j,1,l)         scr(m,j,2,l) = -f(m,j,1,l)   10    continue   20    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(scr(1,1,3,l),2*nxv,mreal,kr-1,moff+2,lgrp,msid,i     1err)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,1,l),2*nxv,mreal,kl-1,moff+2,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 40 j = 1, nx3         do 30 m = 1, 2         scr(m,j,3,l) = -f(m,j,kyp+3,l)         f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + 2.*f(m,j,kyp+3,l)         f(m,j,kyp+3,l) = 0.   30    continue   40    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (kll.ge.1) then            call MPI_IRECV(scr(1,1,2,l),2*nxv,mreal,kll-1,moff+5,lgrp,ms     1id,ierr)         else if (kl.eq.1) then            call MPI_IRECV(scr(1,1,2,l),2*nxv,mreal,kl-1,moff+5,lgrp,msi     1d,ierr)         endif         if (krr.le.nvp) then            call MPI_SEND(f(1,1,kyp+3,l),2*nxv,mreal,krr-1,moff+5,lgrp,i     1err)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_SEND(f(1,1,1,l),2*nxv,mreal,kr-1,moff+5,lgrp,ierr)         endif         if (kl.ge.1) then            call MPI_WAIT(msid,istatus,ierr)            if (kl.eq.1) then               do 60 j = 1, nx3               do 50 m = 1, 2               scr(m,j,2,l) = -scr(m,j,2,l)   50          continue   60          continue            endif         else            do 80 j = 1, nx3            do 70 m = 1, 2            scr(m,j,2,l) = 0.   70       continue   80       continue         endifc last point is special with only one grid         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_IRECV(f(1,1,kyp+3,l),2*nxv,mreal,kl-1,moff+6,lgrp,m     1sid,ierr)         endif         if (kr.eq.nvp) then            call MPI_SEND(f(1,1,kyp+3,l),2*nxv,mreal,kr-1,moff+6,lgrp,ie     1rr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_WAIT(msid,istatus,ierr)            do 140 j = 1, nx3            do 130 m = 1, 2            f(m,j,kyp+2,l) = f(m,j,kyp+2,l) + f(m,j,kyp+3,l)            f(m,j,kyp+3,l) = 0.  130       continue  140       continue         endif      endifc add up the guard cells      do 160 j = 1, nx3      do 150 m = 1, 2      f(m,j,2,l) = f(m,j,2,l) + scr(m,j,1,l)      f(m,j,ngc+1,l) = f(m,j,ngc+1,l) + scr(m,j,2,l)      f(m,j,kyp+1,l) = f(m,j,kyp+1,l) + scr(m,j,3,l)  150 continue  160 continue  170 continuec zero out the left edge      do 200 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 190 j = 1, nx3         do 180 m = 1, 2         f(m,j,1,l) = 0.  180    continue  190    continue      endif  200 continue      return      endc-----------------------------------------------------------------------      subroutine PLAGUARD2(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok,ngds)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c scr(j,idps,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok, ngds      dimension f(nxv,nypmx,kblok), scr(nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx3 = nx + 3      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 90 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1         ngc = 1      endifc this segment is used for shared memory computersc     if (kl.ge.1) thenc        do 10 j = 1, nx3c        scr(j,1,l) = f(j,kyp+2,kl)c        scr(j,2,l) = f(j,kyp+3,kll)c  10    continuec     elsec        do 20 j = 1, nx3c        scr(j,1,l) = 2.*f(j,1,l)c        scr(j,2,l) = -f(j,1,l)c  20    continuec     endifc     if (kr.le.nvp) thenc        do 30 j = 1, nx3c        scr(j,3,l) = f(j,1,kr)c  30    continuec     elsec        do 40 j = 1, nx3c        scr(j,3,l) = -f(j,kyp+3,l)c        f(j,kyp+2,l) = f(j,kyp+2,l) + 2.*f(j,kyp+3,l)c        f(j,kyp+3,l) = 0.c  40    continuec     endifc     if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 50 j = 1, nx3c           scr(j,1,l) = f(j,kyp+2,kl)c           scr(j,2,l) = -f(j,1,kl)c  50       continuec        else if (kl.eq.0) thenc           do 60 j = 1, nx3c           scr(j,2,l) = 0.c  60       continuec        endifc last point is special with only one gridc        if ((kl.eq.(nvp-1)).and.(kl.ge.1)) thenc           do 70 j = 1, nx3c           f(j,kyp+2,l) = f(j,kyp+2,l) + f(j,kyp+3,kl)c  70    continuec        endifc     endifc this segment is used for mpi computers      if (kl.ge.1) then         call MPI_IRECV(scr,ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      endif      if (kr.le.nvp) then         call MPI_SEND(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+1,lgrp,ierr)      endif      if (kl.ge.1) then         call MPI_WAIT(msid,istatus,ierr)      else         do 10 j = 1, nx3         scr(j,1,l) = 2.*f(j,1,l)         scr(j,2,l) = -f(j,1,l)   10    continue      endif      if (kr.le.nvp) then         call MPI_IRECV(scr(1,3,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)      endif      if (kl.ge.1) then         call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)      endif      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         do 20 j = 1, nx3         scr(j,3,l) = -f(j,kyp+3,l)         f(j,kyp+2,l) = f(j,kyp+2,l) + 2.*f(j,kyp+3,l)         f(j,kyp+3,l) = 0.   20    continue      endifc special case of only one grid per processor      if (kyp.eq.1) then         if (kll.ge.1) then            call MPI_IRECV(scr(1,2,l),nxv,mreal,kll-1,moff+5,lgrp,msid,i     1err)         else if (kl.eq.1) then            call MPI_IRECV(scr(1,2,l),nxv,mreal,kl-1,moff+5,lgrp,msid,ie     1rr)         endif         if (krr.le.nvp) then            call MPI_SEND(f(1,kyp+3,l),nxv,mreal,krr-1,moff+5,lgrp,ierr)         endif         if ((kl.eq.0).and.(kr.le.nvp)) then            call MPI_SEND(f(1,1,l),nxv,mreal,kr-1,moff+5,lgrp,ierr)         endif         if (kl.ge.1) then            call MPI_WAIT(msid,istatus,ierr)            if (kl.eq.1) then               do 30 j = 1, nx3               scr(j,2,l) = -scr(j,2,l)   30          continue            endif         else            do 40 j = 1, nx3            scr(j,2,l) = 0.   40       continue         endifc last point is special with only one grid         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_IRECV(f(1,kyp+3,l),nxv,mreal,kl-1,moff+6,lgrp,msid,     1ierr)         endif         if (kr.eq.nvp) then            call MPI_SEND(f(1,kyp+3,l),nxv,mreal,kr-1,moff+6,lgrp,ierr)         endif         if ((kl.eq.(nvp-1)).and.(kl.ge.1)) then            call MPI_WAIT(msid,istatus,ierr)            do 70 j = 1, nx3            f(j,kyp+2,l) = f(j,kyp+2,l) + f(j,kyp+3,l)            f(j,kyp+3,l) = 0.   70       continue         endif      endifc add up the guard cells      do 80 j = 1, nx3      f(j,2,l) = f(j,2,l) + scr(j,1,l)      f(j,ngc+1,l) = f(j,ngc+1,l) + scr(j,2,l)      f(j,kyp+1,l) = f(j,kyp+1,l) + scr(j,3,l)   80 continue   90 continuec zero out the left edge      do 110 l = 1, kblok      kl = l + ks      if (kl.eq.0) then         do 100 j = 1, nx3         f(j,1,l) = 0.  100    continue      endif  110 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARDS2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine corrects the change density data for particle boundaryc conditions which keep particles one grid away from the edgesc the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(3,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(3,nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer ks, moff, kr, krr, kl, kll, j, l, m      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 210 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = klc special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1      endifc fix edges if all points are on the same processor      if (kl.eq.0) then         if (kyp.gt.2) then            do 20 j = 2, nx            do 10 m = 1, 3            f(m,j+1,3,l) = f(m,j+1,3,l) + 2.*f(m,j+1,2,l)            f(m,j+1,4,l) = f(m,j+1,4,l) - f(m,j+1,2,l)            f(m,j+1,2,l) = 0.   10       continue   20       continue         else if (kyp.eq.2) then            do 40 j = 2, nx            do 30 m = 1, 3            f(m,j+1,3,l) = f(m,j+1,3,l) + 2.*f(m,j+1,2,l)   30       continue   40       continue         endif      endif      if (kr.eq.(nvp+1)) then         if (kyp.gt.1) then            do 60 j = 2, nx            do 50 m = 1, 3            f(m,j+1,kyp,l) = f(m,j+1,kyp,l) - f(m,j+1,kyp+2,l)            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) + 2.*f(m,j+1,kyp+2,l)            f(m,j+1,kyp+2,l) = 0.   50       continue   60       continue         else if (kyp.eq.1) then            do 80 j = 2, nx            do 70 m = 1, 3            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) + 2.*f(m,j+1,kyp+2,l)   70       continue   80       continue         endif      endifc this segment is used for shared memory computersc     if (kyp.eq.2) thenc        if (kl.eq.1) thenc           do 120 j = 2, nxc           do 110 m = 1, 3c           f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,2,kl)c 110       continuec 120       continuec        endifc     else if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 140 j = 2, nxc           do 130 m = 1, 3c           f(m,j+1,2,l) = f(m,j+1,2,l) + 2.*f(m,j+1,2,kl)c 130       continuec 140       continuec        endifc        if (kll.eq.1) thenc           do 160 j = 2, nxc           do 150 m = 1, 3c           f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,2,kll)c 150       continuec 160       continuec        endifc        if (kr.eq.nvp) thenc           do 180 j = 2, nxc           do 170 m = 1, 3c           f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) - f(m,j+1,kyp+2,kr)c 170       continuec 180       continuec        endifc     endifc this segment is used for mpi computers      if (kyp.eq.2) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kl-1,moff+1,lgrp,msid,     1ierr)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,kr-1,moff+1,lgrp,ierr)            do 100 j = 2, nx            do 90 m = 1, 3            f(m,j+1,2,l) = 0.   90       continue  100       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 120 j = 2, nx            do 110 m = 1, 3            f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  110       continue  120       continue         endif      else if (kyp.eq.1) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kl-1,moff+1,lgrp,msid,     1ierr)         endif         if (kll.eq.1) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kll-1,moff+1,lgrp,nsid     1,ierr)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,kr-1,moff+1,lgrp,ierr)            call MPI_SEND(f(1,1,2,l),3*nxv,mreal,krr-1,moff+1,lgrp,ierr)            do 140 j = 2, nx            do 130 m = 1, 3            f(m,j+1,2,l) = 0.  130       continue  140       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 160 j = 2, nx            do 150 m = 1, 3            f(m,j+1,2,l) = f(m,j+1,2,l) + 2.*f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  150       continue  160       continue         endif         if (kll.eq.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 180 j = 2, nx            do 170 m = 1, 3            f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  170       continue  180       continue         endif         if (kr.eq.nvp) then            call MPI_IRECV(f(1,1,1,l),3*nxv,mreal,kr-1,moff+2,lgrp,msid,     1ierr)         endif         if (kr.eq.(nvp+1)) then            call MPI_SEND(f(1,1,kyp+2,l),3*nxv,mreal,kl-1,moff+2,lgrp,ie     1rr)         endif         if (kr.eq.nvp) then            call MPI_WAIT(msid,istatus,ierr)            do 200 j = 2, nx            do 190 m = 1, 3            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  190       continue  200       continue         endif      endif  210 continue      return      endc-----------------------------------------------------------------------      subroutine PLACGUARDS22(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine corrects the change density data for particle boundaryc conditions which keep particles one grid away from the edgesc the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(2,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(2,nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer ks, moff, kr, krr, kl, kll, j, l, m      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 210 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = klc special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1      endifc fix edges if all points are on the same processor      if (kl.eq.0) then         if (kyp.gt.2) then            do 20 j = 2, nx            do 10 m = 1, 2            f(m,j+1,3,l) = f(m,j+1,3,l) + 2.*f(m,j+1,2,l)            f(m,j+1,4,l) = f(m,j+1,4,l) - f(m,j+1,2,l)            f(m,j+1,2,l) = 0.   10       continue   20       continue         else if (kyp.eq.2) then            do 40 j = 2, nx            do 30 m = 1, 2            f(m,j+1,3,l) = f(m,j+1,3,l) + 2.*f(m,j+1,2,l)   30       continue   40       continue         endif      endif      if (kr.eq.(nvp+1)) then         if (kyp.gt.1) then            do 60 j = 2, nx            do 50 m = 1, 2            f(m,j+1,kyp,l) = f(m,j+1,kyp,l) - f(m,j+1,kyp+2,l)            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) + 2.*f(m,j+1,kyp+2,l)            f(m,j+1,kyp+2,l) = 0.   50       continue   60       continue         else if (kyp.eq.1) then            do 80 j = 2, nx            do 70 m = 1, 2            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) + 2.*f(m,j+1,kyp+2,l)   70       continue   80       continue         endif      endifc this segment is used for shared memory computersc     if (kyp.eq.2) thenc        if (kl.eq.1) thenc           do 120 j = 2, nxc           do 110 m = 1, 2c           f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,2,kl)c 110       continuec 120       continuec        endifc     else if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 140 j = 2, nxc           do 130 m = 1, 2c           f(m,j+1,2,l) = f(m,j+1,2,l) + 2.*f(m,j+1,2,kl)c 130       continuec 140       continuec        endifc        if (kll.eq.1) thenc           do 160 j = 2, nxc           do 150 m = 1, 2c           f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,2,kll)c 150       continuec 160       continuec        endifc        if (kr.eq.nvp) thenc           do 180 j = 2, nxc           do 170 m = 1, 2c           f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) - f(m,j+1,kyp+2,kr)c 170       continuec 180       continuec        endifc     endifc this segment is used for mpi computers      if (kyp.eq.2) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kl-1,moff+1,lgrp,msid,     1ierr)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,kr-1,moff+1,lgrp,ierr)            do 100 j = 2, nx            do 90 m = 1, 2            f(m,j+1,2,l) = 0.   90       continue  100       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 120 j = 2, nx            do 110 m = 1, 2            f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  110       continue  120       continue         endif      else if (kyp.eq.1) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kl-1,moff+1,lgrp,msid,     1ierr)         endif         if (kll.eq.1) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kll-1,moff+1,lgrp,nsid     1,ierr)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,kr-1,moff+1,lgrp,ierr)            call MPI_SEND(f(1,1,2,l),2*nxv,mreal,krr-1,moff+1,lgrp,ierr)            do 140 j = 2, nx            do 130 m = 1, 2            f(m,j+1,2,l) = 0.  130       continue  140       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 160 j = 2, nx            do 150 m = 1, 2            f(m,j+1,2,l) = f(m,j+1,2,l) + 2.*f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  150       continue  160       continue         endif         if (kll.eq.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 180 j = 2, nx            do 170 m = 1, 2            f(m,j+1,2,l) = f(m,j+1,2,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  170       continue  180       continue         endif         if (kr.eq.nvp) then            call MPI_IRECV(f(1,1,1,l),2*nxv,mreal,kr-1,moff+2,lgrp,msid,     1ierr)         endif         if (kr.eq.(nvp+1)) then            call MPI_SEND(f(1,1,kyp+2,l),2*nxv,mreal,kl-1,moff+2,lgrp,ie     1rr)         endif         if (kr.eq.nvp) then            call MPI_WAIT(msid,istatus,ierr)            do 200 j = 2, nx            do 190 m = 1, 2            f(m,j+1,kyp+1,l) = f(m,j+1,kyp+1,l) - f(m,j+1,1,l)            f(m,j+1,1,l) = 0.  190       continue  200       continue         endif      endif  210 continue      return      endc-----------------------------------------------------------------------      subroutine PLAGUARDS2(f,kstrt,nvp,nx,nxv,nypmx,kyp,kblok)c this subroutine corrects the change density data for particle boundaryc conditions which keep particles one grid away from the edgesc the field is added up so as to disable quadratic interpolationc within half a cell of the edges, and reduce it to linear interpolationc in the y direction.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok      dimension f(nxv,nypmx,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, nsid, ierr      integer ks, moff, kr, krr, kl, kll, j, l      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 110 l = 1, kblok      kr = l + ks + 2      krr = kr      kl = l + ks      kll = klc special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         kll = kll - 1      endifc fix edges if all points are on the same processor      if (kl.eq.0) then         if (kyp.gt.2) then            do 10 j = 2, nx            f(j+1,3,l) = f(j+1,3,l) + 2.*f(j+1,2,l)            f(j+1,4,l) = f(j+1,4,l) - f(j+1,2,l)            f(j+1,2,l) = 0.   10       continue         else if (kyp.eq.2) then            do 20 j = 2, nx            f(j+1,3,l) = f(j+1,3,l) + 2.*f(j+1,2,l)   20       continue         endif      endif      if (kr.eq.(nvp+1)) then         if (kyp.gt.1) then            do 30 j = 2, nx            f(j+1,kyp,l) = f(j+1,kyp,l) - f(j+1,kyp+2,l)            f(j+1,kyp+1,l) = f(j+1,kyp+1,l) + 2.*f(j+1,kyp+2,l)            f(j+1,kyp+2,l) = 0.   30       continue         else if (kyp.eq.1) then            do 40 j = 2, nx            f(j+1,kyp+1,l) = f(j+1,kyp+1,l) + 2.*f(j+1,kyp+2,l)   40       continue         endif      endifc this segment is used for shared memory computersc     if (kyp.eq.2) thenc        if (kl.eq.1) thenc           do 60 j = 2, nxc           f(j+1,2,l) = f(j+1,2,l) - f(j+1,2,kl)c  60       continuec        endifc     else if (kyp.eq.1) thenc        if (kl.eq.1) thenc           do 70 j = 2, nxc           f(j+1,2,l) = f(j+1,2,l) + 2.*f(j+1,2,kl)c  70       continuec        endifc        if (kll.eq.1) thenc           do 80 j = 2, nxc           f(j+1,2,l) = f(j+1,2,l) - f(j+1,2,kll)c  80       continuec        endifc        if (kr.eq.nvp) thenc           do 90 j = 2, nxc           f(j+1,kyp+1,l) = f(j+1,kyp+1,l) - f(j+1,kyp+2,kr)c  90       continuec        endifc     endifc this segment is used for mpi computers      if (kyp.eq.2) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+1,lgrp,msid,ierr     1)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,2,l),nxv,mreal,kr-1,moff+1,lgrp,ierr)            do 50 j = 2, nx            f(j+1,2,l) = 0.   50       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 60 j = 2, nx            f(j+1,2,l) = f(j+1,2,l) - f(j+1,1,l)            f(j+1,1,l) = 0.   60       continue         endif      else if (kyp.eq.1) then         if (kl.eq.1) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+1,lgrp,msid,ierr     1)         endif         if (kll.eq.1) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kll-1,moff+1,lgrp,nsid,ier     1r)         endif         if (kl.eq.0) then            call MPI_SEND(f(1,2,l),nxv,mreal,kr-1,moff+1,lgrp,ierr)            call MPI_SEND(f(1,2,l),nxv,mreal,krr-1,moff+1,lgrp,ierr)            do 70 j = 2, nx            f(j+1,2,l) = 0.   70       continue         endif         if (kl.eq.1) then            call MPI_WAIT(msid,istatus,ierr)            do 80 j = 2, nx            f(j+1,2,l) = f(j+1,2,l) + 2.*f(j+1,1,l)            f(j+1,1,l) = 0.   80       continue         endif         if (kll.eq.1) then            call MPI_WAIT(nsid,istatus,ierr)            do 90 j = 2, nx            f(j+1,2,l) = f(j+1,2,l) - f(j+1,1,l)            f(j+1,1,l) = 0.   90       continue         endif         if (kr.eq.nvp) then            call MPI_IRECV(f(1,1,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr     1)         endif         if (kr.eq.(nvp+1)) then            call MPI_SEND(f(1,kyp+2,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)         endif         if (kr.eq.nvp) then            call MPI_WAIT(msid,istatus,ierr)            do 100 j = 2, nx            f(j+1,kyp+1,l) = f(j+1,kyp+1,l) - f(j+1,1,l)            f(j+1,1,l) = 0.  100       continue         endif      endif  110 continue      return      endc-----------------------------------------------------------------------      subroutine HARTBEAT(msg,n)c participating nodes send a message msg to non-participating nodesc msg = message to sendc n = length of messagec input: msg, nc output: msg      implicit none      integer n      double precision msg      dimension msg(n)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mdouble = default double precision typec lworld = MPI_COMM_WORLD communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, nvp, idproc, ierror      dimension istatus(lstat)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lworld,nvp,ierror)c quit if communicators are the same size      if (nvp.eq.nproc) returnc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lworld,idproc,ierror)c send message      if (idproc.eq.0) then         call MPI_RECV(msg,n,mdouble,1,999,lworld,istatus,ierror)      elseif (idproc.eq.1) then         call MPI_SEND(msg,n,mdouble,0,999,lworld,ierror)      endifc broadcast message to non-participating nodes      if (nvp.gt.(2*nproc)) then         call MPI_BCAST(msg,n,mdouble,0,lgrp,ierror)      endif      return      endc-----------------------------------------------------------------------      subroutine PBICAST(f,nxp)c this subroutine broadcasts integer data fc f = data to be broadcastc nxp = size of data fc input: f, nxpc output: f      implicit none      integer nxp      integer f      dimension f(nxp)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mint = default datatype for integersc lworld = MPI_COMM_WORLD communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, nvp, idproc, np, ioff, i, id, ierr      dimension istatus(lstat)c this segment is used for mpi computersc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lworld,idproc,ierr)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lworld,nvp,ierr)c node 0 sends messages to other nodes      if (idproc.eq.0) thenc no special diagnostic node         if (nvp.eq.nproc) then            np = nvp            ioff = 1c special diagnostic node present         else            np = nvp - nproc            ioff = 0         endifc first send data to remaining nodes         do 10 i = 2, np            id = i - ioff            call MPI_SEND(f,nxp,mint,id,96,lworld,ierr)   10    continuec then send data to node 0         if (ioff.eq.0) then            id = 1            call MPI_SEND(f,nxp,mint,id,96,lworld,ierr)         endifc other nodes receive data from node 0      elseif (idproc.le.(nproc+1)) then         call MPI_RECV(f,nxp,mint,0,96,lworld,istatus,ierr)      endif      return      end