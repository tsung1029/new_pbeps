c * * * periodic 2d electrostatic particle simulation kernel code * * *c this is a simple 2d skeleton particle-in-cell code designed forc exploring new computer architectures.  it contains the critical piecesc needed for depositing charge and current, advancing particles, andc solving the fields.  the code moves only electrons, with periodicc electromagnetic forces obtained by solving maxwells's equation withc fast fourier transforms.c the only diagnostic is particle and field energy.c portable gcpic kernel code, using algorithm described in:c p. c. liewer and v. k. decyk, j. computational phys. 85, 302 (1989).c written by viktor k. decyk, uclac for mpi distributed memory computersc update: august 13, 2004      program beps2kc indx/indy = exponent which determines length in x/y direction,c where nx=2**indx, ny=2**indyc npx/npy = initial number of particles distributed in x/y directionc     parameter( indx =   5, indy =   6, npx =      96, npy =     192)      parameter( indx =   6, indy =   7, npx =     384, npy =     768)c     parameter( indx =   7, indy =   8, npx =    1280, npy =    2560)c npxb/npyb = initial number of particles in beam in x/y directionc     parameter( npxb =  32, npyb =  64)      parameter( npxb = 128, npyb = 256)c     parameter( npxb = 384, npyb = 768)c tend = time at end of simulation, in units of plasma frequencyc dt = time interval between successive calculationsc     parameter( tend =  65.000, dt = 0.2000000e+00)      parameter( tend =  85.000, dt = 0.025)c vty = thermal velocity of electrons in y directionc vdy = drift velocity of beam electrons y directionc vtdy = thermal velocity of beam electrons in y direction      parameter( vty =   1.000, vdy =   5.000, vtdy =   0.500)c avdy = absolute value of drift velocity of beam electrons y direction      parameter( avdy =   5.000)c indnvp = exponent determining number of real or virtual processorsc indnvp must be <= indyc idps = number of partition boundariesc idimp = dimension of phase space = 5c mshare = (0,1) = (no,yes) architecture is shared memory      parameter( indnvp =   2, idps =    2, idimp =   5, mshare =   0)c np = total number of electrons in simulation      parameter(npxy=npx*npy,npxyb=npxb*npyb,np=npxy+npxyb)      parameter(nx=2**indx,ny=2**indy,nxh=nx/2,nyh=ny/2)      parameter(nxv=nx+2,nyv=ny+2,nxvh=nxv/2)      parameter(nxe=nx+4,nxeh=nxe/2)c nloop = number of time steps in simulation      parameter(nloop=tend/dt+.0001)c nvp = number of real or virtual processors, nvp = 2**indnvpc nblok = number of particle partitions      parameter(nvp=2**indnvp,nblok=1+mshare*(nvp-1))c npav = average number of particles per processorc npmax = maximum number of particles in each partitionc nypmx = maximum size of particle partition, including guard cells.c     parameter(npav=np/nvp,npmax=npav*1.01+7000,nypmx=(ny-1)/nvp+4)      parameter(npav=np/nvp,npmax=npav*2,nypmx=(ny+ny-1)/nvp+4)      parameter(nxvyp=nxv*nypmx,nxeyp=nxe*nypmx)c kyp = number of complex grids in each field partition in y directionc kxp = number of complex grids in each field partition in x direction      parameter(kyp=(ny-1)/nvp+1,kxp=(nxh-1)/nvp+1)c kyb = number of processors in yc kxb = number of processors in x      parameter(kyb=ny/kyp,kxb=nxh/kxp,jkmx=kxb*(kyb/kxb)+kyb*(kxb/kyb))c kxyb = maximum(kxb,kyb)      parameter(kxyb=jkmx/(2-kxb/jkmx-kyb/jkmx))c kblok = number of field partitions in y direction      parameter(kbmin=1+(1-mshare)*(kxyb/kxb-1))      parameter(kblok=1+mshare*(ny/kyp-1))c jblok = number of field partitions in x direction      parameter(jbmin=1+(1-mshare)*(kxyb/kyb-1))      parameter(jblok=1+mshare*(nxh/kxp-1))c ngds = number of guard cells      parameter(ngds=3*((idps-1)/2+1))c nxyh = maximum(nx,ny)/2c nxhy = maximum(nx/2,ny)      parameter(nmx=nx*(ny/nx)+ny*(nx/ny),nxy=nmx/(2-nx/nmx-ny/nmx))      parameter(nxyh=nxy/2,nmxh=nxh*(ny/nxh)+ny*(nxh/ny))      parameter(nxhy=nmxh/(2-nxh/nmxh-ny/nmxh))c nbmax = size of buffer for passing particles between processorsc     parameter(nbmax=1+(2*(npxy*vty+npxyb*vtdy)+1.4*npxyb*avdy)*dt/ny)      parameter(nbmax=50000)c ntmax = size of hole array for particles leaving processors      parameter(ntmax=2*nbmax)c npd = size of scratch buffers for vectorized charge deposition      parameter(npd=128,nine=9,n27=27,n18=18)c dimensions for sorting arraysc     parameter(nypm1=(ny-1)/nvp+2)      parameter(nypm1=(ny+ny-1)/nvp+2)c ipbc = particle boundary condition is 2d periodic      parameter(ipbc=1)      complex qt, fxyt, cut, bxyt, bzt, exyz, bxyz, ffc, bs, br, sct      common /large/ partc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc part(5,n,l) = velocity vz of particle n in partition l      dimension part(idimp,npmax,nblok)c maskp = scratch array for particle addressesc     dimension maskp(npmax,nblok)c in real space, qe(j+1,k,l) = charge density at grid point (j,kk)c where kk = k + noff(l) - 1      dimension qe(nxe,nypmx*kbmin,kblok)c in real space, cu(3,j+1,k,l) = current density at grid point (j,kk)      dimension cu(3,nxe,nypmx*kbmin,kblok)c     dimension cu(2,nxe,nypmx*kbmin,kblok)c in real space, fxyze(i,j+1,k,l) = i component of force/charge at c grid point (j,kk)c in other words, fxyze are the convolutions of the electric fieldc over the particle shape, where kk = k + noff(l) - 1      dimension fxyze(3,nxe,nypmx*kbmin,kblok)      dimension fxye(2,nxe,nypmx*kbmin,kblok)c bxye(i,j+1,k,l) = i component of magnetic field at grid (j,kk).      dimension bxye(3,nxe,nypmx*kbmin,kblok)      dimension bze(nxe,nypmx*kbmin,kblok)c qt(k,j,l) = complex charge density for fourier mode jj-1,k-1c fxyt(1,k,j,l) = x component of force/charge for fourier mode jj-1,k-1c fxyt(2,k,j,l) = y component of force/charge for fourier mode jj-1,k-1c where jj = j + kxp*(l - 1)      dimension qt(nyv,kxp,jblok), cut(3,nyv,kxp,jblok)      dimension fxyt(3,nyv,kxp,jblok), bxyt(3,nyv,kxp,jblok)c     dimension qt(nyv,kxp,jblok), cut(2,nyv,kxp,jblok)c     dimension fxyt(2,nyv,kxp,jblok), bxyt(2,nyv,kxp,jblok)      dimension bzt(nyv,kxp,jblok)c in fourier space, exyz = transverse electric fieldc in fourier space, bxyz = magnetic field      dimension exyz(3,nyv,kxp,jblok), bxyz(3,nyv,kxp,jblok)c ffc = form factor array for poisson solver      dimension ffc(nyh,kxp,jblok)c mixup, sct = arrays for fft      dimension mixup(nxhy), sct(nxyh)c bs, br = complex buffer arrays for transposec     dimension bs(3,kxp,kyp,kblok), br(3,kxp,kyp,jblok)c msid, mrid = scratch arrays for identifying asynchronous messagesc     dimension msid(kxb), mrid(kyb)c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition l      dimension edges(idps,nblok)c nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.      dimension nyp(nblok), noff(nblok)c npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition l      dimension npp(nblok), nps(nblok)c sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processor      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)c rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processor      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)c ihole = location of holes left in particle arrays      dimension ihole(ntmax,nblok)c jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition l      dimension jsl(idps,nblok), jsr(idps,nblok)c jss(idps,l) = scratch array for particle partition lc scr(j,ngds,l) = complex scratch array for particle partition l      dimension jss(idps,nblok), scr(3*nxe,ngds,nblok)      dimension scs(3*nxe,nblok)c nn = scratch address array for vectorized charge depositionc amxy = scratch weight array for vectorized charge deposition      dimension nn(n27,npd,nblok), amxy(n27,npd,nblok)c wtot = total energy      dimension wtot(6), work(6)c sorting arrays      dimension pt(npmax,nblok), ip(npmax,nblok), npic(nypm1,nblok)c scratch data for moving field partitions      dimension g(nxe,nypmx,kblok), h(3,nxe,nypmx,kblok)      dimension nyps(nblok), noffs(nblok), nypd(nblok), noffd(nblok)c scratch data for repartitioning      dimension edg(nypm1,nblok), eds(nypm1,nblok)      dimension eg(idps,nblok), es(idps,nblok), et2(2*idps,nblok)c non-uniform plasma      dimension qi(nxe,nypmx*kbmin,kblok)      real FGDISTR1      external FGDISTR1      character*12 labelc debugc     dimension nypg(nvp), noffg(nvp)c end debug  991 format (5h t = ,i7)  992 format (19h * * * q.e.d. * * *)  993 format (34h field, kinetic, total energies = ,3e14.7)  994 format (36h electric(l,t), magnetic energies = ,3e14.7)c vtx/vtz = thermal velocity of electrons in x/z directionc qme = charge on electron, in units of ec vdx/vdz = drift velocity of beam electrons in x/z directionc vtdx/vtdz = thermal velocity of beam electrons in x/z direction      data vtx,vtz,qme,vdx,vdz,vtdx,vtdz /1.,1.,-1.,0.,0.,.5,.5/c ax/ay = half-width of particle in x/y direction      data ax,ay /.866667,.866667/c omx/omy/omz = magnetic field electron cyclotron frequency in x/y/z c direction      data omx,omy,omz /0.2,0.5,1.0/c ntpose = (0,1) = (no,yes) input, output data are transposed in pfft2r      data ntpose /1/c debugc     data nypg /48,16,16,48/, noffg /0,48,64,80/c end debugc initialize for parallel processing      call ppinit(idproc,nvp)      kstrt = idproc + 1      if (kstrt.eq.1) then         open(unit=6,file='output2',form='formatted',status='unknown')      endifc initialize timer      call timera(-1,'init    ',time)c initialize constants      itime = 0      nypmin = kyp      nypmax = kyp      nterf = 0      nterg = 1      if (nypmin.eq.1) nterg = 0      aipav = 1.0/real(npav)      qbme = qme      affp = float(nx*ny)/float(np)      dth = .5*dtc     ci = 1.0      ci = 0.1      zero = 0.c calculate partition variables      call dcomp2(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c debugc     vtz = 0.c     vdz = 0.c     vtdz = 0.c initialize transverse electromagnetic fields      do 170 l = 1, jblok      do 160 j = 1, kxp      do 150 k = 1, ny      do 140 i = 1, 3      bxyz(i,k,j,l) = 0.      exyz(i,k,j,l) = 0.  140 continue  150 continue  160 continue  170 continuec prepare fft tables      isign = 0c     call PFFT2R(qe,qt,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrt,nxec    1h,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX(qe,qt,isign,ntpose,mixup,sct,indx,indy,kstrt,nxeh,nyv     1,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c calculate form factors      call PPOIS23(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,j     1blok,nyh)c     call PPOIS22(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,jc    1blok,nyh)c initialize density profile and velocity distributionc background electrons      do 180 l = 1, nblok      nps(l) = 1      npp(l) = 0  180 continuec     if (npxy.gt.0) call PISTR2 (part,edges,npp,nps,vtx,vty,zero,zero,nc    1px,npy,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)c     if (npxy.gt.0) call PISTR2H(part,edges,npp,nps,vtx,vty,vtz,zero,zec    1ro,zero,npx,npy,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)      ang = 10000.0      wi = 0.1      y0 = real(nyh)      if (npxy.gt.0) then         call PFDISTR2(part,nps,FGDISTR1,zero,zero,zero,FGDISTR1,ang,wi,     1y0,npx,npy,nx,ny,idimp,npmax,nblok,kstrt,nvp,ipbc,ierr)         if (ierr.ne.0) then            call ppexit            stop         endif         call PVDISTR2H(part,npp,nps,vtx,vty,vtz,zero,zero,zero,npx,npy,     1idimp,npmax,nblok,kstrt,nvp,ierr)         if (ierr.ne.0) then            call ppexit            stop         endif      endifc beam electrons      do 190 l = 1, nblok      nps(l) = npp(l) + 1  190 continuec     if (npxyb.gt.0) call PISTR2 (part,edges,npp,nps,vtdx,vtdy,vdx,vdy,c    1npxb,npyb,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)c     if (npxyb.gt.0) call PISTR2H(part,edges,npp,nps,vtdx,vtdy,vtdz,vdxc    1,vdy,vdz,npxb,npyb,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)      if (npxyb.gt.0) then         call PFDISTR2(part,nps,FGDISTR1,zero,zero,zero,FGDISTR1,ang,wi,     1y0,npxb,npyb,nx,ny,idimp,npmax,nblok,kstrt,nvp,ipbc,ierr)         if (ierr.ne.0) then            call ppexit            stop         endif         call PVDISTR2H(part,npp,nps,vtdx,vtdy,vtdz,vdx,vdy,vdz,npxb,npy     1b,idimp,npmax,nblok,kstrt,nvp,ierr)         if (ierr.ne.0) then            call ppexit            stop         endif      endifc find new partition analytically      call FEDGES2(edges,noff,nyp,FGDISTR1,ang,wi,y0,ny,nypmin,nypmax,ks     1trt,nvp,nblok,idps,ipbc)      if ((nypmin.ge.1).and.((nypmax+3).le.nypmx)) then         nterg = 1         if (nypmin.eq.1) nterg = 0      else         write (6,*) 'Field size error: nypmin,nypmax=',nypmin,nypmax         call ppexit         stop      endifc move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc initialize external magnetic fields      do 130 l = 1, nblok      nyp3 = nyp(l) + 3      do 120 k = 1, nyp3      do 110 j = 1, nx      do 100 i = 1, 3      bxye(i,j+1,k,l) = 0.  100 continue      bze(j+1,k,l) = 0.  110 continue  120 continue  130 continue      call PBGUARD2X(bxye,nyp,nx,nxe,nypmx,nblok)      call PDGUARD2X(bze,nyp,nx,nxe,nypmx,nblok)c calculate actual coordinates from guiding centersc     call PBDISTR2L(part,bx,by,bz,npp,noff,qbme,nx,ny,idimp,npmax,nblokc    1,nxv,nypmx)c     call PGBDISTR2L(part,bxye(1,2,2,1),npp,noff,qbme,nx,ny,idimp,npmaxc    1,nblok,nxe,nypmx,ipbc)c     call PGBZDISTR2L(part,bze(2,2,1),npp,noff,qbme,nx,ny,idimp,npmax,nc    1blok,nxe,nypmx,ipbc)c     call PRETARD2(part,npp,dth,nx,ny,idimp,npmax,nblok,ipbc)      call PGRBDISTR2L(part,bxye(1,2,2,1),npp,noff,qbme,ci,nx,ny,idimp,n     1pmax,nblok,nxe,nypmx,ipbc)c     call PGRBZDISTR2L(part,bze(2,2,1),npp,noff,qbme,ci,nx,ny,idimp,npmc    1ax,nblok,nxe,nypmx,ipbc)      call PRRETARD2(part,npp,dth,ci,nx,ny,idimp,npmax,nblok,ipbc)c     call PRRETARD22(part,npp,dth,ci,nx,ny,idimp,npmax,nblok,ipbc)c debug: redefine spatial regionsc     ks = kstrt - 2c     do 200 l = 1, nblokc     nyp(l) = nypg((l+ks)+1)c     noff(l) = noffg((l+ks)+1)c     edges(1,l) = noff(l)c     edges(2,l) = noff(l) + nyp(l)c 200 continuec move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc count the number of particles per cellc     isign = 1c     call PCOUNT2YL(part,isign,npic,npp,noff,nyp,idimp,npmax,nblok,nypmc    11)c determine repartitioning boundariesc     call REPARTD2(edges,edg,eds,eg,es,et2,npic,noff,nyp,npav,nypmin,nyc    1pmax,kstrt,nvp,nblok,idps,nypm1)c     if ((nypmin.ge.1).and.((nypmax+3).le.nypmx)) thenc        nterg = 1c        if (nypmin.eq.1) nterg = 0c     elsec        write (6,*) 'Field size error: nypmin,nypmax=',nypmin,nypmaxc        call ppexitc        stopc     endifc move particles into appropriate spatial regionsc     call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,jc    1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c     if (ierr.ne.0) thenc        call ppexitc        stopc     endifc initialize current density to background      call PSCGUARD2(cu,nyp,zero,zero,zero,nx,nxe,nypmx,nblok)c     call PSCGUARD22(cu,nyp,zero,zero,nx,nxe,nypmx,nblok)c deposit currentc     call PGJPOST2(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxec    1,nypmx,ipbc)c     call PGJPOST22(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxc    1e,nypmx,ipbc)      call PGRJPOST2(part,cu,npp,noff,qme,dth,ci,nx,ny,idimp,npmax,nblok     1,nxe,nypmx,ipbc)c     call PGRJPOST22(part,cu,npp,noff,qme,dth,ci,nx,ny,idimp,npmax,nbloc    1k,nxe,nypmx,ipbc)c move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc merge current arrays      call PACGUARD2X(cu,nyp,nx,nxe,nypmx,nblok)c     call PACGUARD22X(cu,nyp,nx,nxe,nypmx,nblok)c copy data from particle to field partition, and add up guard cellsc     call PACGUARD2(cu,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)c     call PACGUARD22(cu,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)      call PNACGUARD2(cu,scr,scs,nyp,kstrt,nvp,nx,nxe,nypmx,nblok,ngds,n     1terg)c     call PNACGUARD22(cu,scr,scs,nyp,kstrt,nvp,nx,nxe,nypmx,nblok,ngds,c    1nterg)c initialize charge density to background      qi0 = -qme/affpc     call PSGUARD2(qe,nyp,qi0,nx,nxe,nypmx,nblok)c set ion background      call PSGUARD2(qi,nyp,zero,nx,nxe,nypmx,nblok)      call PGPOST2(part,qi,npp,noff,abs(qme),idimp,npmax,nblok,nxe,nypmx     1)      call PAGUARD2X(qi,nyp,nx,nxe,nypmx,nblok)      call PNAGUARD2(qi,scr,scs,nyp,kstrt,nvp,nx,nxe,nypmx,nblok,ngds,nt     1erg)      call PSGUARD2(qi,nyp,qi0,nx,nxe,nypmx,nblok)c initialize charge density to background      do 330 l = 1, kblok      do 320 k = 1, nypmx      do 310 j = 1, nxe      qe(j,k,l) = qi(j,k,l)  310 continue  320 continue  330 continuec deposit charge for initial distributionc     call PDOST2(part,q,npp,noff,qme,nx,idimp,npmax,nblok,nxv,nypmx)      call PGPOST2(part,qe,npp,noff,qme,idimp,npmax,nblok,nxe,nypmx)c add guard cells      call PAGUARD2X(qe,nyp,nx,nxe,nypmx,nblok)      call timera(1,'init    ',time)      call timera(-1,'main    ',time)cc * * * start main iteration loop * * *c  500 if (nloop.le.itime) go to 2000      if (kstrt.eq.1) write (6,991) itime      write (label,991) itime      call LOGNAME(label)c copy data from particle to field partition, and add up guard cellsc     call PAGUARD2(qe,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)      call PNAGUARD2(qe,scr,scs,nyp,kstrt,nvp,nx,nxe,nypmx,nblok,ngds,nt     1erg)c move charge to uniform field partition.      isign = -1      call PFMOVE2(qe(2,2,1),g,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsl,is     1ign,kyp,kstrt,nvp,nxe,nypmx,nblok,idps,nterf,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc transform charge to fourier space      isign = -1c     call PFFT2R(qe(2,2,1),qt,bs,br,isign,ntpose,mixup,sct,indx,indy,ksc    1trt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX(qe(2,2,1),qt,isign,ntpose,mixup,sct,indx,indy,kstrt,n     1xeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c calculate force/charge in fourier space      isign = -1      call PPOIS23(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,j     1blok,nyh)c     call PPOIS22(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,jc    1blok,nyh)c move current to uniform field partition.      isign = -1      call PFMOVE2(cu(1,2,2,1),h,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsl,     1isign,kyp,kstrt,nvp,3*nxe,nypmx,nblok,idps,nterf,ierr)c     call PFMOVE2(cu(1,2,2,1),h,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsl,c    1isign,kyp,kstrt,nvp,2*nxe,nypmx,nblok,idps,nterf,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc transform current to fourier space      isign = -1c     call PFFT2R3(cu(1,2,2,1),cut,bs,br,isign,ntpose,mixup,sct,indx,indc    1y,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX3(cu(1,2,2,1),cut,isign,ntpose,mixup,sct,indx,indy,kst     1rt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2R2(cu(1,2,2,1),cut,bs,br,isign,ntpose,mixup,sct,indx,indc    1y,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2RX2(cu(1,2,2,1),cut,isign,ntpose,mixup,sct,indx,indy,kstc    1rt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c take transverse part of current      call PCUPERP2(cut,nx,ny,kstrt,nyv,kxp,jblok)c     call PCUPERP22(cut,nx,ny,kstrt,nyv,kxp,jblok)c calculate magnetic field in fourier spacec     isign = -1c     call PBPOISP23(cut,bxyt,isign,ffc,ax,ay,affp,ci,wm,nx,ny,kstrt,nyvc    1,kxp,jblok,nyh)c     call PBPOISP22(cut,bxyt,bzt,isign,ffc,ax,ay,affp,ci,wm,nx,ny,kstrtc    1,nyv,kxp,jblok,nyh)c     wf = 0.      if (itime.eq.0) then         call IPBPOISP23(cut,bxyz,ffc,ci,wm,nx,ny,kstrt,nyv,kxp,jblok,ny     1h)         wf = 0.      else         call PMAXWEL2(exyz,bxyz,cut,ffc,affp,ci,dt,wf,wm,nx,ny,kstrt,ny     1v,kxp,jblok,nyh)      endifc add longitudinal and transverse electric fields      isign = 1      call PEMFIELD2(fxyt,exyz,ffc,isign,nx,ny,kstrt,nyv,kxp,jblok,nyh)c copy magnetic field      isign = -1      call PEMFIELD2(bxyt,bxyz,ffc,isign,nx,ny,kstrt,nyv,kxp,jblok,nyh)c transform force/charge to real space      isign = 1c     call PFFT2R3(fxyze(1,2,2,1),fxyt,bs,br,isign,ntpose,mixup,sct,indxc    1,indy,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX3(fxyze(1,2,2,1),fxyt,isign,ntpose,mixup,sct,indx,indy     1,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2R2(fxye(1,2,2,1),fxyt,bs,br,isign,ntpose,mixup,sct,indx,c    1indy,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2RX2(fxye(1,2,2,1),fxyt,isign,ntpose,mixup,sct,indx,indy,c    1kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c move force/charge to non-uniform field partition.      isign = 1      call PFMOVE2(fxyze(1,2,2,1),h,noff,nyp,noffs,nyps,noffd,nypd,jsr,j     1sl,isign,kyp,kstrt,nvp,3*nxe,nypmx,nblok,idps,nterf,ierr)c     call PFMOVE2(fxye(1,2,2,1),h,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsc    1l,isign,kyp,kstrt,nvp,2*nxe,nypmx,nblok,idps,nterf,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc copy data from field to particle partition, and copy to guard cellsc     call pcguard2(fxyze,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok)      call PNCGUARD2(fxyze,scs,nyp,kstrt,nvp,3*nxe,nypmx,nblok,nterg)      call PBGUARD2X(fxyze,nyp,nx,nxe,nypmx,nblok)c     call pcguard2(fxye,kstrt,nvp,2*nxe,nypmx,nblok,kyp,kblok)c     call PNCGUARD2(fxye,scs,nyp,kstrt,nvp,2*nxe,nypmx,nblok,nterg)c     call PCGUARD2X(fxye,nyp,nx,nxe,nypmx,nblok)c transform magnetic field to real space      isign = 1c     call PFFT2R3(bxye(1,2,2,1),bxyt,bs,br,isign,ntpose,mixup,sct,indx,c    1indy,kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX3(bxye(1,2,2,1),bxyt,isign,ntpose,mixup,sct,indx,indy,     1kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2R(bze(2,2,1),bzt,bs,br,isign,ntpose,mixup,sct,indx,indy,c    1kstrt,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c     call PFFT2RX(bze(2,2,1),bzt,isign,ntpose,mixup,sct,indx,indy,kstrtc    1,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c move magnetic field to non-uniform field partition.      isign = 1      call PFMOVE2(bxye(1,2,2,1),h,noff,nyp,noffs,nyps,noffd,nypd,jsr,js     1l,isign,kyp,kstrt,nvp,3*nxe,nypmx,nblok,idps,nterf,ierr)c     call PFMOVE2(bze,2,2,1),g,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsl,ic    1sign,kyp,kstrt,nvp,nxe,nypmx,nblok,idps,nterf,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc copy data from field to particle partition, and copy to guard cellsc     call pcguard2(bxye,kstrt,nvp,3*nxe,nypmx,nblok,kyp,kblok)      call PNCGUARD2(bxye,scs,nyp,kstrt,nvp,3*nxe,nypmx,nblok,nterg)      call PBGUARD2X(bxye,nyp,nx,nxe,nypmx,nblok)c     call pcguard2(bze,kstrt,nvp,nxe,nypmx,nblok,kyp,kblok)c     call PNCGUARD2(bze,scs,nyp,kstrt,nvp,nxe,nypmx,nblok,nterg)c     call PDGUARD2X(bze,nyp,nx,nxe,nypmx,nblok)c particle push and charge density updatec     call timera(-1,'push    ',time)c push particles      wke = 0.c     call PGBPUSH23(part,fxyze,bxye,npp,noff,qbme,dt,dth,wke,nx,ny,idimc    1p,npmax,nblok,nxe,nypmx,ipbc)c     call PGSBPUSH23(part,fxyze,bxye,npp,noff,qbme,dt,dth,wke,nx,ny,idic    1mp,npmax,nblok,nxe,nxeyp,ipbc)c     call PGBPUSH22(part,fxye,bze,npp,noff,qbme,dt,dth,wke,nx,ny,idimp,c    1npmax,nblok,nxe,nypmx,ipbc)c     call PGSBPUSH22(part,fxye,bze,npp,noff,qbme,dt,dth,wke,nx,ny,idimpc    1,npmax,nblok,nxe,nxeyp,ipbc)      call PGRBPUSH23(part,fxyze,bxye,npp,noff,qbme,dt,dth,ci,wke,nx,ny,     1idimp,npmax,nblok,nxe,nypmx,ipbc)c     call PGSRBPUSH23(part,fxyze,bxye,npp,noff,qbme,dt,dth,ci,wke,nx,nyc    1,idimp,npmax,nblok,nxe,nxeyp,ipbc)c     call PGRBPUSH22(part,fxye,bze,npp,noff,qbme,dt,dth,ci,wke,nx,ny,idc    1imp,npmax,nblok,nxe,nypmx,ipbc)c     call PGSRBPUSH22(part,fxye,bze,npp,noff,qbme,dt,dth,ci,wke,nx,ny,ic    1dimp,npmax,nblok,nxe,nxeyp,ipbc)c move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c     call pxmov2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,c    1jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifcc begin reparititioning      nppmx = 0      do 610 l = 1, nblok      nppmx = max0(nppmx,abs(npp(l)-npav))  610 continue      call PIMAX(nppmx,it1,1,1)      at1 = real(nppmx)*aipav      write (6,*) 'maximum percent imbalance =',at1      if (at1.lt.(.05)) go to 1000c count the number of particles per cell      isign = 1      call PCOUNT2YL(part,isign,npic,npp,noff,nyp,idimp,npmax,nblok,nypm     11)c save old repartitioning boundaries      do 620 l = 1, nblok      noffs(l) = noff(l)      nyps(l) = nyp(l)  620 continuec determine new repartitioning boundaries      call REPARTD2(edges,edg,eds,eg,es,et2,npic,noff,nyp,npav,nypmin,ny     1pmax,kstrt,nvp,nblok,idps,nypm1)      if ((nypmin.ge.1).and.((nypmax+3).le.nypmx)) then         nterg = 1         if (nypmin.eq.1) nterg = 0      else         write (6,*) 'Field size error: nypmin,nypmax=',nypmin,nypmax         call ppexit         stop      endifc move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c     call pxmov2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,c    1jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc move static data to new field partition.      do 630 l = 1, nblok      noffd(l) = noff(l)      nypd(l) = nyp(l)  630 continue      isign = 0      nterf = 0      call PFMOVE2(qi(2,2,1),g,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsl,is     1ign,kyp,kstrt,nvp,nxe,nypmx,nblok,idps,nterf,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc zero out guard cells      call PZGUARD2(qi,nyp,nx,nxe,nypmx,kblok)      nterf = 0      write (6,*) 'repartitioning complete' 1000 continuec end reparititioningcc initialize current density to background      call PSCGUARD2(cu,nyp,zero,zero,zero,nx,nxe,nypmx,nblok)c     call PSCGUARD22(cu,nyp,zero,zero,nx,nxe,nypmx,nblok)c deposit currentc     call PGJPOST2(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxec    1,nypmx,ipbc)c     call PGSJPOST2(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxc    1e,nxeyp,ipbc)c     call PGSJOST2X(part,cu,npp,noff,nn,amxy,qme,dth,nx,ny,idimp,npmax,c    1nblok,nxe,nxeyp,npd,n27,ipbc)c     call PGJPOST22(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nxc    1e,nypmx,ipbc)c     call PGSJPOST22(part,cu,npp,noff,qme,dth,nx,ny,idimp,npmax,nblok,nc    1xe,nxeyp,ipbc)c     call PGSJOST22X(part,cu,npp,noff,nn,amxy,qme,dth,nx,ny,idimp,npmaxc    1,nblok,nxe,nxeyp,npd,n18,ipbc)      call PGRJPOST2(part,cu,npp,noff,qme,dth,ci,nx,ny,idimp,npmax,nblok     1,nxe,nypmx,ipbc)c     call PGSRJPOST2(part,cu,npp,noff,qme,dth,ci,nx,ny,idimp,npmax,nbloc    1k,nxe,nxeyp,ipbc)c     call PGSRJOST2X(part,cu,npp,noff,nn,amxy,qme,dth,ci,nx,ny,idimp,npc    1max,nblok,nxe,nxeyp,npd,n27,ipbc)c     call PGRJPOST22(part,cu,npp,noff,qme,dth,ci,nx,ny,idimp,npmax,nbloc    1k,nxe,nypmx,ipbc)c     call PGSRJPOST22(part,cu,npp,noff,qme,dth,ci,nx,ny,idimp,npmax,nblc    1ok,nxe,nxeyp,ipbc)c     call PGSRJOST22X(part,cu,npp,noff,nn,amxy,qme,dth,ci,nx,ny,idimp,nc    1pmax,nblok,nxe,nxeyp,npd,n18,ipbc)c move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c     call pxmov2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,c    1jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc merge current arrays      call PACGUARD2X(cu,nyp,nx,nxe,nypmx,nblok)c     call PACGUARD22X(cu,nyp,nx,nxe,nypmx,nblok)c copy data from particle to field partition, and add up guard cellsc     call PACGUARD2(cu,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)c     call PACGUARD22(cu,scr,kstrt,nvp,nx,nxe,nypmx,kyp,kblok,ngds)      call PNACGUARD2(cu,scr,scs,nyp,kstrt,nvp,nx,nxe,nypmx,nblok,ngds,n     1terg)c     call PNACGUARD22(cu,scr,scs,nyp,kstrt,nvp,nx,nxe,nypmx,nblok,ngds,c    1nterg)c initialize charge density to backgroundc     call PSGUARD2(qe,nyp,qi0,nx,nxe,nypmx,nblok)c set ion background      do 1350 l = 1, kblok      do 1340 k = 1, nypmx      do 1330 j = 1, nxe      qe(j,k,l) = qi(j,k,l) 1330 continue 1340 continue 1350 continuec deposit chargec     call PDOST2(part,q,npp,noff,qme,nx,idimp,npmax,nblok,nxv,nypmx)c     call PGPOST2(part,qe,npp,noff,qme,idimp,npmax,nblok,nxe,nypmx)      call PGSPOST2(part,qe,npp,noff,qme,idimp,npmax,nblok,nxe,nxeyp)c     call PSOST2X(part,q,npp,noff,nn,amxy,qme,nx,idimp,npmax,nblok,nxv,c    1nxvyp,npd,nine)c     call PGSOST2X(part,qe,npp,noff,nn,amxy,qme,idimp,npmax,nblok,nxe,nc    1xeyp,npd,nine)c merge density arrays      call PAGUARD2X(qe,nyp,nx,nxe,nypmx,nblok)c     call timera(1,'push    ',time)c sort particles      if (mod(itime,50).eq.0) then         call PSORTP2Y(part,pt,ip,npic,npp,noff,nyp,idimp,npmax,nblok,ny     1pm1)      endif c energy diagnostic      wef = we + wf + wm      wtot(1) = wef      wtot(2) = wke      wtot(3) = wef + wke      wtot(4) = we      wtot(5) = wf      wtot(6) = wm      call PSUM(wtot,work,6,1)      if (kstrt.eq.1) write (6,993) wtot(1), wtot(2), wtot(3)      if (kstrt.eq.1) write (6,994) wtot(4), wtot(5), wtot(6)      itime = itime + 1      go to 500 2000 continuecc * * * end main iteration loop * * *c      if (kstrt.eq.1) write (6,992)      call timera(1,'main    ',time)      call ppexit      stop      endc-----------------------------------------------------------------------      subroutine ppinit(idproc,nvp)c this subroutine initializes parallel processingc input: nvp, output: idprocc idproc = processor idc nvp = number of real or virtual processors requested      implicit none      integer idproc, nvpc get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for realsc mint = default datatype for integersc mcplx = default datatype for complex type      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ierror, ndprec      save /PPARMS/c ndprec = (0,1) = (no,yes) use (normal,autodouble) precision      data ndprec /1/c this segment is used for shared memory computersc     nproc = nvpc     idproc = 0c this segment is used for mpi computers      if (MPI_STATUS_SIZE.gt.lstat) then         write (2,*) ' status size too small, actual/required = ', lstat     1, MPI_STATUS_SIZE         stop      endifc initialize the MPI execution environment      call MPI_INIT(ierror)      if (ierror.ne.0) stop      lgrp = MPI_COMM_WORLDc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lgrp,idproc,ierror)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lgrp,nproc,ierror)c set default datatypes         mint = MPI_INTEGERc single precision      if (ndprec.eq.0) then         mreal = MPI_REAL         mcplx = MPI_COMPLEXc double precision      else         mreal = MPI_DOUBLE_PRECISION         mcplx = MPI_DOUBLE_COMPLEX      endifc requested number of processors not obtained      if (nproc.ne.nvp) then         write (2,*) ' processor number error: nvp, nproc=', nvp, nproc         call ppexit         stop      endif      return      endc-----------------------------------------------------------------------      subroutine ppexitc this subroutine terminates parallel processing      implicit nonec common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc lgrp = current communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworld      integer ierrorc synchronize processes      call MPI_BARRIER(lgrp,ierror)c terminate MPI execution environment      call MPI_FINALIZE(ierror)      return      endc-----------------------------------------------------------------------      subroutine dcomp2(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c this subroutine determines spatial boundaries for particlec decomposition, calculates number of grid points in each spatialc region, and the offset of these grid points from the global addressc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.c ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idps = number of partition boundariesc nblok = number of particle partitions.      implicit none      real edges      integer nyp, noff, ny, kstrt, nvp, idps, nblok      dimension edges(idps,nblok)      dimension nyp(nblok), noff(nblok)c local data      integer ks, kb, kr, l      real at1      ks = kstrt - 2      at1 = float(ny)/float(nvp)      do 10 l = 1, nblok      kb = l + ks      edges(1,l) = at1*float(kb)      noff(l) = edges(1,l) + .5      edges(2,l) = at1*float(kb + 1)      kr = edges(2,l) + .5      nyp(l) = kr - noff(l)   10 continue      return      endc-----------------------------------------------------------------------      subroutine PISTR2H(part,edges,npp,nps,vtx,vty,vtz,vdx,vdy,vdz,npx,     1npy,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)c for 2-1/2d code, this subroutine calculates initial particlec co-ordinates and velocities with uniform density and maxwellianc velocity with drift for distributed data.c part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc part(5,n,l) = velocity vz of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition lc vtx/vty/vtz = thermal velocity of electrons in x/y/z directionc vdx/vdy/vdz = drift velocity of beam electrons in x/y/z directionc npx/npy = initial number of particles distributed in x/y directionc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)c ierr = (0,1) = (no,yes) error condition existsc ranorm = gaussian random number with zero mean and unit variancec with spatial decomposition      double precision ranorm      double precision sum0, sum1, sum2      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok), nps(nblok)      dimension sum3(3), isum2(2)      dimension work3(3), iwork2(2)      ierr = 0c set boundary values      edgelx = 0.      edgely = 0.      at1 = float(nx)/float(npx)      at2 = float(ny)/float(npy)      if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         at1 = float(nx-2)/float(npx)         at2 = float(ny-2)/float(npy)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         at1 = float(nx-2)/float(npx)         at2 = float(ny)/float(npy)      endif      do 30 k = 1, npy      yt = edgely + at2*(float(k) - .5)      do 20 j = 1, npxc uniform density profile      xt = edgelx + at1*(float(j) - .5)c maxwellian velocity distribution      vxt = vtx*ranorm()      vyt = vty*ranorm()      vzt = vtz*ranorm()      do 10 l = 1, nblok      if ((yt.ge.edges(1,l)).and.(yt.lt.edges(2,l))) then         npt = npp(l) + 1         if (npt.le.npmax) then            part(1,npt,l) = xt            part(2,npt,l) = yt            part(3,npt,l) = vxt            part(4,npt,l) = vyt            part(5,npt,l) = vzt            npp(l) = npt         else            ierr = ierr + 1         endif      endif   10 continue   20 continue   30 continue      npxy = 0c add correct drift      sum3(1) = 0.      sum3(2) = 0.      sum3(3) = 0.      do 50 l = 1, nblok      sum0 = 0.0d0      sum1 = 0.0d0      sum2 = 0.0d0      do 40 j = nps(l), npp(l)      npxy = npxy + 1      sum0 = sum0 + part(3,j,l)      sum1 = sum1 + part(4,j,l)      sum2 = sum2 + part(5,j,l)   40 continue      sum3(1) = sum3(1) + sum0      sum3(2) = sum3(2) + sum1      sum3(3) = sum3(3) + sum2   50 continue      isum2(1) = ierr      isum2(2) = npxy      call PISUM(isum2,iwork2,2,1)      ierr = isum2(1)      npxy = isum2(2)      call PSUM(sum3,work3,3,1)      at1 = 1./float(npxy)      sum3(1) = at1*sum3(1) - vdx      sum3(2) = at1*sum3(2) - vdy      sum3(3) = at1*sum3(3) - vdz      do 70 l = 1, nblok      do 60 j = nps(l), npp(l)      part(3,j,l) = part(3,j,l) - sum3(1)      part(4,j,l) = part(4,j,l) - sum3(2)      part(5,j,l) = part(5,j,l) - sum3(3)   60 continue   70 continuec process errors      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr      else if (npxy.ne.(npx*npy)) then         write (2,*) 'particle distribution truncated, np = ', npxy      endif      return      endc-----------------------------------------------------------------------      subroutine PBDISTR2L(part,bx,by,bz,npp,noff,qbm,nx,ny,idimp,npmax,     1nblok,nxv,nypmx)c for 2-1/2d code, this subroutine reinterprets curent particlec positions as positions of guiding centers, and calculates the actualc particle positions, with periodic boundary conditionsc for distributed datac in converting from guiding center to actual co-ordinates,c the following equations are used:c       x(t) = xg(t) - (vy(t)*omz - vz(t)*omy)/om**2c       y(t) = yg(t) - (vz(t)*omx - vx(t)*omz)/om**2c where omx = (q/m)*bx(xg(t),yg(t)),c       omy = (q/m)*by(xg(t),yg(t)),c and   omz = (q/m)*bz(xg(t),yg(t)),c bx(x(t),y(t)), by(x(t),y(t)), and bz(x(t),y(t))c are approximated by interpolation from the nearest grid points:c bx(x,y) = (1-dy)*((1-dx)*bx(n,m)+dx*bx(n+1,m)) + dy*((1-dx)*bx(n,m+1)c    + dx*bx(n+1.m+1))c where n,m = leftmost grid points and dx = x-n, dy = y-mc similarly for by(x,y), bz(x,y)c part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc part(5,n,l) = velocity vz of particle n in partition lc bx(j,k,l) = x component of magnetic field at grid (j,kk)c by(j,k,l) = y component of magnetic field at grid (j,kk)c bz(j,k),l = z component of magnetic field at grid (j,kk)c that is, the convolution of magnetic field over particle shapec where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c qbm = particle charge/mass ratioc nx/ny = system length in x/y directionc idimp = size of phase space = 5c npmax = maximum number of particles in each partitionc nblok = number of particle partitionsc nxv = first dimension of field arrays, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)      dimension part(idimp,npmax,nblok)      dimension bx(nxv,nypmx,nblok), by(nxv,nypmx,nblok)      dimension bz(nxv,nypmx,nblok)      dimension npp(nblok), noff(nblok)      zero = 0.      anx = float(nx)      any = float(ny)c calculate actual position from guiding center      do 20 l = 1, nblok      mnoff = noff(l) - 1      do 10 j = 1, npp(l)c find interpolation weights      nn = part(1,j,l)      mm = part(2,j,l)      dxp = qbm*(part(1,j,l) - float(nn))      dyp = part(2,j,l) - float(mm)      nn = nn + 1      mm = mm - mnoff      amx = qbm - dxp      np = nn + 1      if (np.gt.nx) np = np - nx      amy = 1. - dyp      mp = mm + 1c find magnetic field      omx = amy*(amx*bx(nn,mm,l) + dxp*bx(np,mm,l)) + dyp*(amx*bx(nn,mp,     1l) + dxp*bx(np,mp,l))      omy = amy*(amx*by(nn,mm,l) + dxp*by(np,mm,l)) + dyp*(amx*by(nn,mp,     1l) + dxp*by(np,mp,l))      omz = amy*(amx*bz(nn,mm,l) + dxp*bz(np,mm,l)) + dyp*(amx*bz(nn,mp,     1l) + dxp*bz(np,mp,l))      at3 = sqrt(omx*omx + omy*omy + omz*omz)      if (at3.ne.0.) at3 = 1./at3      at3 = at3*at3      omxt = omx*at3      omyt = omy*at3      omzt = omz*at3c correct position      dx = part(1,j,l) - (part(4,j,l)*omzt - part(5,j,l)*omyt)      dy = part(2,j,l) - (part(5,j,l)*omxt - part(3,j,l)*omzt)c periodic boundary conditions      n = abs(dx)/anx      if (dx.lt.zero) dx = dx + float(n + 1)*anx      if (dx.ge.anx) dx = dx - float(n)*anx      part(1,j,l) = dx      m = abs(dy)/any      if (dy.lt.zero) dy = dy + float(m + 1)*any      if (dy.ge.any) dy = dy - float(m)*any      part(2,j,l) = dy   10 continue   20 continue      return      endc-----------------------------------------------------------------------      subroutine PGBDISTR2L(part,bxy,npp,noff,qbm,nx,ny,idimp,npmax,nblo     1k,nxv,nypmx,ipbc)c for 2-1/2d code, this subroutine reinterprets curent particlec positions as positions of guiding centers, and calculates the actualc particle positions for distributed datac in converting from guiding center to actual co-ordinates,c the following equations are used:c       x(t) = xg(t) - (vy(t)*omz - vz(t)*omy)/om**2c       y(t) = yg(t) - (vz(t)*omx - vx(t)*omz)/om**2c where omx = (q/m)*bxyz(1,xg(t),yg(t)),c       omy = (q/m)*bxyz(2,xg(t),yg(t)),c and   omz = (q/m)*bxyz(2,xg(t),yg(t)),c and the magnetic field components bxyz(i,x(t),y(t)) are approximatedc by interpolation from the nearest grid points:c bxy(i,x,y) = (1-dy)*((1-dx)*bxy(i,n,m)+dx*bxy(i,n+1,m)) + c               dy*((1-dx)*bxy(i,n,m+1) + dx*bxy(i,n+1.m+1))c where n,m = leftmost grid points and dx = x-n, dy = y-mc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc part(5,n,l) = velocity vz of particle n in partition lc bxyz(i,1,j,k,l) = i component of magnetic field at grid (j,kk)c that is, the convolution of magnetic field over particle shapec where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c qbm = particle charge/mass ratioc nx/ny = system length in x/y directionc idimp = size of phase space = 5c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nxv = first dimension of field arrays, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)      dimension part(idimp,npmax,nblok)      dimension bxy(3,nxv,nypmx,nblok)      dimension npp(nblok), noff(nblok)c set boundary values      if (ipbc.eq.1) then         edgelx = 0.         edgely = 0.         edgerx = float(nx)         edgery = float(ny)      else if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         edgerx = float(nx-1)         edgery = float(ny-1)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         edgerx = float(nx-1)         edgery = float(ny)      endifc calculate actual position from guiding center      do 20 l = 1, nblok      mnoff = noff(l) - 1      do 10 j = 1, npp(l)c find interpolation weights      nn = part(1,j,l)      mm = part(2,j,l)      dxp = qbm*(part(1,j,l) - float(nn))      dyp = part(2,j,l) - float(mm)      nn = nn + 1      mm = mm - mnoff      amx = qbm - dxp      mp = mm + 1      amy = 1. - dyp      np = nn + 1c find magnetic field      omx = dyp*(dxp*bxy(1,np,mp,l) + amx*bxy(1,nn,mp,l)) + amy*(dxp*bxy     1(1,np,mm,l) + amx*bxy(1,nn,mm,l))      omy = dyp*(dxp*bxy(2,np,mp,l) + amx*bxy(2,nn,mp,l)) + amy*(dxp*bxy     1(2,np,mm,l) + amx*bxy(2,nn,mm,l))      omz = dyp*(dxp*bxy(3,np,mp,l) + amx*bxy(3,nn,mp,l)) + amy*(dxp*bxy     1(3,np,mm,l) + amx*bxy(3,nn,mm,l))      at3 = sqrt(omx*omx + omy*omy + omz*omz)      if (at3.ne.0.) at3 = 1./at3      at3 = at3*at3      omxt = omx*at3      omyt = omy*at3      omzt = omz*at3c correct position      dx = part(1,j,l) - (part(4,j,l)*omzt - part(5,j,l)*omyt)      dy = part(2,j,l) - (part(5,j,l)*omxt - part(3,j,l)*omzt)c periodic boundary conditions      if (ipbc.eq.1) then         n = abs(dx)/edgerx         if (dx.lt.edgelx) dx = dx + float(n + 1)*edgerx         if (dx.ge.edgerx) dx = dx - float(n)*edgerx         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgeryc reflecting boundary conditions      else if (ipbc.eq.2) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(dy.g     1e.edgery)) then            if ((dy.ge.edgely).and.(dy.lt.edgery)) thenc if x co-ordinate only is out of bounds, try switching vy               dx = part(1,j,l) + (part(4,j,l)*omzt + part(5,j,l)*omyt)               if ((dx.ge.edgelx).and.(dx.lt.edgerx)) then                  part(4,j,l) = -part(4,j,l)               elsec otherwise, try switching both vy and vz                  dx = part(1,j,l) + (part(4,j,l)*omzt - part(5,j,l)*omy     1t)                  dy = part(2,j,l) + (part(5,j,l)*omxt + part(3,j,l)*omz     1t)                  if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgel     1y).and.(dy.lt.edgery)) then                     part(4,j,l) = -part(4,j,l)                     part(5,j,l) = -part(5,j,l)                  endif               endif            else if ((dx.ge.edgelx).and.(dx.lt.edgerx)) thenc if y co-ordinate only is out of bounds, try switching vx               dy = part(2,j,l) - (part(5,j,l)*omxt + part(3,j,l)*omzt)               if ((dy.ge.edgely).and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)               elsec otherwise, try switching both vx and vz                  dx = part(1,j,l) - (part(4,j,l)*omzt + part(5,j,l)*omy     1t)                  dy = part(2,j,l) + (part(5,j,l)*omxt - part(3,j,l)*omz     1t)                  if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgel     1y).and.(dy.lt.edgery)) then                     part(3,j,l) = -part(3,j,l)                     part(5,j,l) = -part(5,j,l)                  endif               endif            endifc if both co-ordinates are out of bounds, try switching vx, vy, vz            if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(d     1y.ge.edgery)) then               dx = part(1,j,l) + (part(4,j,l)*omzt - part(5,j,l)*omyt)               dy = part(2,j,l) + (part(5,j,l)*omxt - part(3,j,l)*omzt)               if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgely).     1and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)                  part(4,j,l) = -part(4,j,l)                  part(5,j,l) = -part(5,j,l)               elsec give up if larmor radius is too large                  dx = part(1,j,l)                  dy = part(2,j,l)               endif            endif         endifc mixed reflecting/periodic boundary conditions      else if (ipbc.eq.3) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx)) thenc rotate particle position by reversing velocity in y and z            dx = part(1,j,l) + (part(4,j,l)*omzt - part(5,j,l)*omyt)            dy = part(2,j,l) + (part(5,j,l)*omxt + part(3,j,l)*omzt)c give up if larmor radius is too large            if ((dx.lt.edgelx).or.(dx.ge.edgerx)) then               dx = part(1,j,l)               dy = part(2,j,l)            else               part(4,j,l) = -part(4,j,l)               part(5,j,l) = -part(5,j,l)            endif         endif         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgery      endifc set new position      part(1,j,l) = dx      part(2,j,l) = dy   10 continue   20 continue      return      endc-----------------------------------------------------------------------      subroutine PGBZDISTR2L(part,bz,npp,noff,qbm,nx,ny,idimp,npmax,nblo     1k,nxv,nypmx,ipbc)c for 2d code, this subroutine reinterprets curent particlec positions as positions of guiding centers, and calculates the actualc particle positions for distributed datac in converting from guiding center to actual co-ordinates,c the following equations are used:c       x(t) = xg(t) - (vy(t)*omz)/om**2c       y(t) = yg(t) + (vx(t)*omz)/om**2c where omz = (q/m)*bz(xg(t),yg(t)),c and the magnetic field component bz(x(t),y(t)) is approximatedc by interpolation from the nearest grid points:c bz(x,y) = (1-dy)*((1-dx)*bz(n,m)+dx*bz(n+1,m)) + c               dy*((1-dx)*bz(n,m+1) + dx*bz(n+1.m+1))c where n,m = leftmost grid points and dx = x-n, dy = y-mc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc bz(1,j,k,l) = z component of magnetic field at grid (j,kk)c that is, the convolution of magnetic field over particle shapec where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c qbm = particle charge/mass ratioc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nxv = first dimension of field arrays, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)      dimension part(idimp,npmax,nblok)      dimension bz(nxv,nypmx,nblok)      dimension npp(nblok), noff(nblok)c set boundary values      if (ipbc.eq.1) then         edgelx = 0.         edgely = 0.         edgerx = float(nx)         edgery = float(ny)      else if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         edgerx = float(nx-1)         edgery = float(ny-1)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         edgerx = float(nx-1)         edgery = float(ny)      endifc calculate actual position from guiding center      do 20 l = 1, nblok      mnoff = noff(l) - 1      do 10 j = 1, npp(l)c find interpolation weights      nn = part(1,j,l)      mm = part(2,j,l)      dxp = qbm*(part(1,j,l) - float(nn))      dyp = part(2,j,l) - float(mm)      nn = nn + 1      mm = mm - mnoff      amx = qbm - dxp      mp = mm + 1      amy = 1. - dyp      np = nn + 1c find magnetic field      omz = dyp*(dxp*bz(np,mp,l) + amx*bz(nn,mp,l)) + amy*(dxp*bz(np,mm,     1l) + amx*bz(nn,mm,l))      at3 = abs(omz)      if (at3.ne.0.) at3 = 1./at3      at3 = at3*at3      omzt = omz*at3c correct position      dx = part(1,j,l) - part(4,j,l)*omzt      dy = part(2,j,l) + part(3,j,l)*omztc periodic boundary conditions      if (ipbc.eq.1) then         n = abs(dx)/edgerx         if (dx.lt.edgelx) dx = dx + float(n + 1)*edgerx         if (dx.ge.edgerx) dx = dx - float(n)*edgerx         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgeryc reflecting boundary conditions      else if (ipbc.eq.2) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(dy.g     1e.edgery)) then            if ((dy.ge.edgely).and.(dy.lt.edgery)) thenc if x co-ordinate only is out of bounds, try switching vy               dx = part(1,j,l) + part(4,j,l)*omzt               if ((dx.ge.edgelx).and.(dx.lt.edgerx)) then                  part(4,j,l) = -part(4,j,l)               endif            else if ((dx.ge.edgelx).and.(dx.lt.edgerx)) thenc if y co-ordinate only is out of bounds, try switching vx               dy = part(2,j,l) - part(3,j,l)*omzt               if ((dy.ge.edgely).and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)               endif            endifc if both co-ordinates are out of bounds, try switching vx, vy            if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(d     1y.ge.edgery)) then               dx = part(1,j,l) + part(4,j,l)*omzt               dy = part(2,j,l) - part(3,j,l)*omzt               if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgely).     1and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)                  part(4,j,l) = -part(4,j,l)               elsec give up if larmor radius is too large                  dx = part(1,j,l)                  dy = part(2,j,l)               endif            endif         endifc mixed reflecting/periodic boundary conditions      else if (ipbc.eq.3) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx)) thenc rotate particle position by reversing velocity in y            dx = part(1,j,l) + part(4,j,l)*omzt            dy = part(2,j,l) + part(3,j,l)*omztc give up if larmor radius is too large            if ((dx.lt.edgelx).or.(dx.ge.edgerx)) then               dx = part(1,j,l)               dy = part(2,j,l)            else               part(4,j,l) = -part(4,j,l)            endif         endif         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgery      endifc set new position      part(1,j,l) = dx      part(2,j,l) = dy   10 continue   20 continue      return      endc-----------------------------------------------------------------------      subroutine PGRBDISTR2L(part,bxy,npp,noff,qbm,ci,nx,ny,idimp,npmax,     1nblok,nxv,nypmx,ipbc)c for 2-1/2d code, this subroutine reinterprets curent particlec positions as positions of guiding centers, and calculates the actualc particle positions for relativistic particles for distributed datac in converting from guiding center to actual co-ordinates,c the following equations are used:c       x(t) = xg(t) - gami*(py(t)*omz - pz(t)*omy)/om**2c       y(t) = yg(t) - gami*(pz(t)*omx - px(t)*omz)/om**2c where gami = 1./sqrt(1.+(px(t)*px(t)+py(t)*py(t)+pz(t)*pz(t))*ci*ci)c where omx = (q/m)*bxyz(1,xg(t),yg(t)),c       omy = (q/m)*bxyz(2,xg(t),yg(t)),c and   omz = (q/m)*bxyz(2,xg(t),yg(t)),c and the magnetic field components bxyz(i,x(t),y(t)) are approximatedc by interpolation from the nearest grid points:c bxy(i,x,y) = (1-dy)*((1-dx)*bxy(i,n,m)+dx*bxy(i,n+1,m)) + c               dy*((1-dx)*bxy(i,n,m+1) + dx*bxy(i,n+1.m+1))c where n,m = leftmost grid points and dx = x-n, dy = y-mc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = momentum px of particle n in partition lc part(4,n,l) = momentum py of particle n in partition lc part(5,n,l) = momentum pz of particle n in partition lc bxyz(i,1,j,k,l) = i component of magnetic field at grid (j,kk)c that is, the convolution of magnetic field over particle shapec where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c qbm = particle charge/mass ratioc ci = reciprical of velocity of lightc nx/ny = system length in x/y directionc idimp = size of phase space = 5c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nxv = first dimension of field arrays, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)      dimension part(idimp,npmax,nblok)      dimension bxy(3,nxv,nypmx,nblok)      dimension npp(nblok), noff(nblok)      ci2 = ci*cic set boundary values      if (ipbc.eq.1) then         edgelx = 0.         edgely = 0.         edgerx = float(nx)         edgery = float(ny)      else if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         edgerx = float(nx-1)         edgery = float(ny-1)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         edgerx = float(nx-1)         edgery = float(ny)      endifc calculate actual position from guiding center      do 20 l = 1, nblok      mnoff = noff(l) - 1      do 10 j = 1, npp(l)c find interpolation weights      nn = part(1,j,l)      mm = part(2,j,l)      dxp = qbm*(part(1,j,l) - float(nn))      dyp = part(2,j,l) - float(mm)      nn = nn + 1      mm = mm - mnoff      amx = qbm - dxp      mp = mm + 1      amy = 1. - dyp      np = nn + 1c find inverse gamma      vx = part(3,j,l)      vy = part(4,j,l)      vz = part(5,j,l)      p2 = vx*vx + vy*vy + vz*vz      gami = 1.0/sqrt(1.0 + p2*ci2)c find magnetic field      omx = dyp*(dxp*bxy(1,np,mp,l) + amx*bxy(1,nn,mp,l)) + amy*(dxp*bxy     1(1,np,mm,l) + amx*bxy(1,nn,mm,l))      omy = dyp*(dxp*bxy(2,np,mp,l) + amx*bxy(2,nn,mp,l)) + amy*(dxp*bxy     1(2,np,mm,l) + amx*bxy(2,nn,mm,l))      omz = dyp*(dxp*bxy(3,np,mp,l) + amx*bxy(3,nn,mp,l)) + amy*(dxp*bxy     1(3,np,mm,l) + amx*bxy(3,nn,mm,l))      at3 = sqrt(omx*omx + omy*omy + omz*omz)      if (at3.ne.0.) at3 = 1./at3      at3 = at3*at3*gami      omxt = omx*at3      omyt = omy*at3      omzt = omz*at3c correct position      dx = part(1,j,l) - (part(4,j,l)*omzt - part(5,j,l)*omyt)      dy = part(2,j,l) - (part(5,j,l)*omxt - part(3,j,l)*omzt)c periodic boundary conditions      if (ipbc.eq.1) then         n = abs(dx)/edgerx         if (dx.lt.edgelx) dx = dx + float(n + 1)*edgerx         if (dx.ge.edgerx) dx = dx - float(n)*edgerx         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgeryc reflecting boundary conditions      else if (ipbc.eq.2) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(dy.g     1e.edgery)) then            if ((dy.ge.edgely).and.(dy.lt.edgery)) thenc if x co-ordinate only is out of bounds, try switching vy               dx = part(1,j,l) + (part(4,j,l)*omzt + part(5,j,l)*omyt)               if ((dx.ge.edgelx).and.(dx.lt.edgerx)) then                  part(4,j,l) = -part(4,j,l)               elsec otherwise, try switching both vy and vz                  dx = part(1,j,l) + (part(4,j,l)*omzt - part(5,j,l)*omy     1t)                  dy = part(2,j,l) + (part(5,j,l)*omxt + part(3,j,l)*omz     1t)                  if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgel     1y).and.(dy.lt.edgery)) then                     part(4,j,l) = -part(4,j,l)                     part(5,j,l) = -part(5,j,l)                  endif               endif            else if ((dx.ge.edgelx).and.(dx.lt.edgerx)) thenc if y co-ordinate only is out of bounds, try switching vx               dy = part(2,j,l) - (part(5,j,l)*omxt + part(3,j,l)*omzt)               if ((dy.ge.edgely).and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)               elsec otherwise, try switching both vx and vz                  dx = part(1,j,l) - (part(4,j,l)*omzt + part(5,j,l)*omy     1t)                  dy = part(2,j,l) + (part(5,j,l)*omxt - part(3,j,l)*omz     1t)                  if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgel     1y).and.(dy.lt.edgery)) then                     part(3,j,l) = -part(3,j,l)                     part(5,j,l) = -part(5,j,l)                  endif               endif            endifc if both co-ordinates are out of bounds, try switching vx, vy, vz            if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(d     1y.ge.edgery)) then               dx = part(1,j,l) + (part(4,j,l)*omzt - part(5,j,l)*omyt)               dy = part(2,j,l) + (part(5,j,l)*omxt - part(3,j,l)*omzt)               if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgely).     1and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)                  part(4,j,l) = -part(4,j,l)                  part(5,j,l) = -part(5,j,l)               elsec give up if larmor radius is too large                  dx = part(1,j,l)                  dy = part(2,j,l)               endif            endif         endifc mixed reflecting/periodic boundary conditions      else if (ipbc.eq.3) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx)) thenc rotate particle position by reversing velocity in y and z            dx = part(1,j,l) + (part(4,j,l)*omzt - part(5,j,l)*omyt)            dy = part(2,j,l) + (part(5,j,l)*omxt + part(3,j,l)*omzt)c give up if larmor radius is too large            if ((dx.lt.edgelx).or.(dx.ge.edgerx)) then               dx = part(1,j,l)               dy = part(2,j,l)            else               part(4,j,l) = -part(4,j,l)               part(5,j,l) = -part(5,j,l)            endif         endif         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgery      endifc set new position      part(1,j,l) = dx      part(2,j,l) = dy   10 continue   20 continue      return      endc-----------------------------------------------------------------------      subroutine PGRBZDISTR2L(part,bz,npp,noff,qbm,ci,nx,ny,idimp,npmax,     1nblok,nxv,nypmx,ipbc)c for 2d code, this subroutine reinterprets curent particlec positions as positions of guiding centers, and calculates the actualc particle positions for relativistic particles for distributed datac in converting from guiding center to actual co-ordinates,c the following equations are used:c       x(t) = xg(t) - gami*(py(t)*omz)/om**2c       y(t) = yg(t) + gami*(px(t)*omz)/om**2c where gami = 1./sqrt(1.+(px(t)*px(t)+py(t)*py(t))*ci*ci)c and omz = (q/m)*bz(xg(t),yg(t)),c and the magnetic field component bz(x(t),y(t)) is approximatedc by interpolation from the nearest grid points:c bz(x,y) = (1-dy)*((1-dx)*bz(n,m)+dx*bz(n+1,m)) + c               dy*((1-dx)*bz(n,m+1) + dx*bz(n+1.m+1))c where n,m = leftmost grid points and dx = x-n, dy = y-mc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = momentum px of particle n in partition lc part(4,n,l) = momentum py of particle n in partition lc bz(1,j,k,l) = z component of magnetic field at grid (j,kk)c that is, the convolution of magnetic field over particle shapec where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c qbm = particle charge/mass ratioc ci = reciprical of velocity of lightc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nxv = first dimension of field arrays, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)      dimension part(idimp,npmax,nblok)      dimension bz(nxv,nypmx,nblok)      dimension npp(nblok), noff(nblok)      ci2 = ci*cic set boundary values      if (ipbc.eq.1) then         edgelx = 0.         edgely = 0.         edgerx = float(nx)         edgery = float(ny)      else if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         edgerx = float(nx-1)         edgery = float(ny-1)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         edgerx = float(nx-1)         edgery = float(ny)      endifc calculate actual position from guiding center      do 20 l = 1, nblok      mnoff = noff(l) - 1      do 10 j = 1, npp(l)c find interpolation weights      nn = part(1,j,l)      mm = part(2,j,l)      dxp = qbm*(part(1,j,l) - float(nn))      dyp = part(2,j,l) - float(mm)      nn = nn + 1      mm = mm - mnoff      amx = qbm - dxp      mp = mm + 1      amy = 1. - dyp      np = nn + 1c find inverse gamma      vx = part(3,j,l)      vy = part(4,j,l)      p2 = vx*vx + vy*vy      gami = 1.0/sqrt(1.0 + p2*ci2)c find magnetic field      omz = dyp*(dxp*bz(np,mp,l) + amx*bz(nn,mp,l)) + amy*(dxp*bz(np,mm,     1l) + amx*bz(nn,mm,l))      at3 = abs(omz)      if (at3.ne.0.) at3 = 1./at3      at3 = at3*at3*gami      omzt = omz*at3c correct position      dx = part(1,j,l) - part(4,j,l)*omzt      dy = part(2,j,l) + part(3,j,l)*omztc periodic boundary conditions      if (ipbc.eq.1) then         n = abs(dx)/edgerx         if (dx.lt.edgelx) dx = dx + float(n + 1)*edgerx         if (dx.ge.edgerx) dx = dx - float(n)*edgerx         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgeryc reflecting boundary conditions      else if (ipbc.eq.2) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(dy.g     1e.edgery)) then            if ((dy.ge.edgely).and.(dy.lt.edgery)) thenc if x co-ordinate only is out of bounds, try switching vy               dx = part(1,j,l) + part(4,j,l)*omzt               if ((dx.ge.edgelx).and.(dx.lt.edgerx)) then                  part(4,j,l) = -part(4,j,l)               endif            else if ((dx.ge.edgelx).and.(dx.lt.edgerx)) thenc if y co-ordinate only is out of bounds, try switching vx               dy = part(2,j,l) - part(3,j,l)*omzt               if ((dy.ge.edgely).and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)               endif            endifc if both co-ordinates are out of bounds, try switching vx, vy            if ((dx.lt.edgelx).or.(dx.ge.edgerx).or.(dy.lt.edgely).or.(d     1y.ge.edgery)) then               dx = part(1,j,l) + part(4,j,l)*omzt               dy = part(2,j,l) - part(3,j,l)*omzt               if ((dx.ge.edgelx).and.(dx.lt.edgerx).and.(dy.ge.edgely).     1and.(dy.lt.edgery)) then                  part(3,j,l) = -part(3,j,l)                  part(4,j,l) = -part(4,j,l)               elsec give up if larmor radius is too large                  dx = part(1,j,l)                  dy = part(2,j,l)               endif            endif         endifc mixed reflecting/periodic boundary conditions      else if (ipbc.eq.3) then         if ((dx.lt.edgelx).or.(dx.ge.edgerx)) thenc rotate particle position by reversing velocity in y            dx = part(1,j,l) + part(4,j,l)*omzt            dy = part(2,j,l) + part(3,j,l)*omztc give up if larmor radius is too large            if ((dx.lt.edgelx).or.(dx.ge.edgerx)) then               dx = part(1,j,l)               dy = part(2,j,l)            else               part(4,j,l) = -part(4,j,l)            endif         endif         m = abs(dy)/edgery         if (dy.lt.edgely) dy = dy + float(m + 1)*edgery         if (dy.ge.edgery) dy = dy - float(m)*edgery      endifc set new position      part(1,j,l) = dx      part(2,j,l) = dy   10 continue   20 continue      return      endc-----------------------------------------------------------------------      subroutine PISTR2(part,edges,npp,nps,vtx,vty,vdx,vdy,npx,npy,nx,ny     1,idimp,npmax,nblok,idps,ipbc,ierr)c for 2d code, this subroutine calculates initial particle co-ordinatesc and velocities with uniform density and maxwellian velocity with driftc for distributed data.c part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition lc vtx/vty = thermal velocity of electrons in x/y directionc vdx/vdy = drift velocity of beam electrons in x/y directionc npx/npy = initial number of particles distributed in x/y directionc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)c ierr = (0,1) = (no,yes) error condition existsc ranorm = gaussian random number with zero mean and unit variancec with spatial decomposition      double precision ranorm      double precision sum0, sum1      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok), nps(nblok)      dimension sum2(2), isum2(2)      dimension work2(2), iwork2(2)      ierr = 0c set boundary values      if (ipbc.eq.1) then         edgelx = 0.         edgely = 0.         at1 = float(nx)/float(npx)         at2 = float(ny)/float(npy)      else if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         at1 = float(nx-2)/float(npx)         at2 = float(ny-2)/float(npy)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         at1 = float(nx-2)/float(npx)         at2 = float(ny)/float(npy)      endif      do 30 k = 1, npy      yt = edgely + at2*(float(k) - .5)      do 20 j = 1, npxc uniform density profile      xt = edgelx + at1*(float(j) - .5)c maxwellian velocity distribution      vxt = vtx*ranorm()      vyt = vty*ranorm()      do 10 l = 1, nblok      if ((yt.ge.edges(1,l)).and.(yt.lt.edges(2,l))) then         npt = npp(l) + 1         if (npt.le.npmax) then            part(1,npt,l) = xt            part(2,npt,l) = yt            part(3,npt,l) = vxt            part(4,npt,l) = vyt            npp(l) = npt         else            ierr = ierr + 1         endif      endif   10 continue   20 continue   30 continue      npxy = 0c add correct drift      sum2(1) = 0.      sum2(2) = 0.      do 50 l = 1, nblok      sum0 = 0.0d0      sum1 = 0.0d0      do 40 j = nps(l), npp(l)      npxy = npxy + 1      sum0 = sum0 + part(3,j,l)      sum1 = sum1 + part(4,j,l)   40 continue      sum2(1) = sum2(1) + sum0      sum2(2) = sum2(2) + sum1   50 continue      isum2(1) = ierr      isum2(2) = npxy      call PISUM(isum2,iwork2,2,1)      ierr = isum2(1)      npxy = isum2(2)      call PSUM(sum2,work2,2,1)      at1 = 1./float(npxy)      sum2(1) = at1*sum2(1) - vdx      sum2(2) = at1*sum2(2) - vdy      do 70 l = 1, nblok      do 60 j = nps(l), npp(l)      part(3,j,l) = part(3,j,l) - sum2(1)      part(4,j,l) = part(4,j,l) - sum2(2)   60 continue   70 continuec process errors      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr      else if (npxy.ne.(npx*npy)) then         write (2,*) 'particle distribution truncated, np = ', npxy      endif      return      endc-----------------------------------------------------------------------      subroutine pcguard2(f,kstrt,nvp,nxv,nypmx,nblok,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c nblok = number of particle partitions, assumed equal to kblok.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c quadratic interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nxv, nypmx, nblok, kyp, kblok      dimension f(nxv,nypmx,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 30 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computersc     do 10 j = 1, nxvc     f(j,1,l) = f(j,kyp+1,kl)c     f(j,kyp+2,l) = f(j,2,kr)c     f(j,kyp+3,l) = f(j,ngc+1,krr)c  10 continuec this segment is used for mpi computers      call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+3,lgrp,msid,ierr)      call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+3,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+4,lgrp,msid,ie     1rr)      call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+6,lgrp,msi     1d,ierr)         call MPI_SEND(f(1,2,l),ngc*nxv,mreal,kll-1,moff+6,lgrp,ierr)         call MPI_WAIT(msid,istatus,ierr)      endif   30 continue      return      endc-----------------------------------------------------------------------      subroutine PACGUARD2(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok,ngds)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c f(3,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c scr(3,j,ngds,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok, ngds      dimension f(3,nxv,nypmx,kblok), scr(3,nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l, m      dimension istatus(lstat)      nx3 = nx + 3c special case for one processor      if (nvp.eq.1) then         do 30 l = 1, kblok         do 20 j = 1, nx3         do 10 m = 1, 3         f(m,j,2,l) = f(m,j,2,l) + f(m,j,kyp+2,l)         f(m,j,3,l) = f(m,j,3,l) + f(m,j,kyp+3,l)         f(m,j,kyp+1,l) = f(m,j,kyp+1,l) + f(m,j,1,l)   10    continue   20    continue   30    continue         return      endif      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 80 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computersc     do 50 j = 1, nx3c     do 40 m = 1, 3c     scr(m,j,1,l) = f(m,j,kyp+2,kl)c     scr(m,j,2,l) = f(m,j,kyp+3,kll)c     scr(m,j,3,l) = f(m,j,1,kr)c  40 continuec  50 continuec this segment is used for mpi computers      call MPI_IRECV(scr,3*ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,1,kyp+2,l),3*ngc*nxv,mreal,kr-1,moff+1,lgrp,ierr     1)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scr(1,1,3,l),3*nxv,mreal,kr-1,moff+2,lgrp,msid,ierr     1)      call MPI_SEND(f(1,1,1,l),3*nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(scr(1,1,2,l),3*ngc*nxv,mreal,kll-1,moff+5,lgrp,m     1sid,ierr)         call MPI_SEND(f(1,1,kyp+3,l),3*ngc*nxv,mreal,krr-1,moff+5,lgrp,     1ierr)         call MPI_WAIT(msid,istatus,ierr)      endifc add up the guard cells      do 70 j = 1, nx3      do 60 m = 1, 3      f(m,j,2,l) = f(m,j,2,l) + scr(m,j,1,l)      f(m,j,ngc+1,l) = f(m,j,ngc+1,l) + scr(m,j,2,l)      f(m,j,kyp+1,l) = f(m,j,kyp+1,l) + scr(m,j,3,l)   60 continue   70 continue   80 continue      return      endc-----------------------------------------------------------------------      subroutine PACGUARD22(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok,ngds)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c f(2,j,k,l) = real data for grid j,k in particle partition l. number ofc grids per partition is uniform and includes three extra guard cells.c scr(2,j,ngds,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok, ngds      dimension f(2,nxv,nypmx,kblok), scr(2,nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l, m      dimension istatus(lstat)      nx3 = nx + 3c special case for one processor      if (nvp.eq.1) then         do 30 l = 1, kblok         do 20 j = 1, nx3         do 10 m = 1, 2         f(m,j,2,l) = f(m,j,2,l) + f(m,j,kyp+2,l)         f(m,j,3,l) = f(m,j,3,l) + f(m,j,kyp+3,l)         f(m,j,kyp+1,l) = f(m,j,kyp+1,l) + f(m,j,1,l)   10    continue   20    continue   30    continue         return      endif      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 80 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computersc     do 50 j = 1, nx3c     do 40 m = 1, 2c     scr(m,j,1,l) = f(m,j,kyp+2,kl)c     scr(m,j,2,l) = f(m,j,kyp+3,kll)c     scr(m,j,3,l) = f(m,j,1,kr)c  40 continuec  50 continuec this segment is used for mpi computers      call MPI_IRECV(scr,2*ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,1,kyp+2,l),2*ngc*nxv,mreal,kr-1,moff+1,lgrp,ierr     1)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scr(1,1,3,l),2*nxv,mreal,kr-1,moff+2,lgrp,msid,ierr     1)      call MPI_SEND(f(1,1,1,l),2*nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(scr(1,1,2,l),2*ngc*nxv,mreal,kll-1,moff+5,lgrp,m     1sid,ierr)         call MPI_SEND(f(1,1,kyp+3,l),2*ngc*nxv,mreal,krr-1,moff+5,lgrp,     1ierr)         call MPI_WAIT(msid,istatus,ierr)      endifc add up the guard cells      do 70 j = 1, nx3      do 60 m = 1, 2      f(m,j,2,l) = f(m,j,2,l) + scr(m,j,1,l)      f(m,j,ngc+1,l) = f(m,j,ngc+1,l) + scr(m,j,2,l)      f(m,j,kyp+1,l) = f(m,j,kyp+1,l) + scr(m,j,3,l)   60 continue   70 continue   80 continue      return      endc-----------------------------------------------------------------------      subroutine PAGUARD2(f,scr,kstrt,nvp,nx,nxv,nypmx,kyp,kblok,ngds)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c scr(j,ngds,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c ngds = number of guard cellsc quadratic interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, kyp, kblok, ngds      dimension f(nxv,nypmx,kblok), scr(nxv,ngds,kblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx3 = nx + 3c special case for one processor      if (nvp.eq.1) then         do 20 l = 1, kblok         do 10 j = 1, nx3         f(j,2,l) = f(j,2,l) + f(j,kyp+2,l)         f(j,3,l) = f(j,3,l) + f(j,kyp+3,l)         f(j,kyp+1,l) = f(j,kyp+1,l) + f(j,1,l)   10    continue   20    continue         return      endif      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 50 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) krr = krr - nvp         kll = kll - 1         if (kll.lt.1) kll = kll + nvp         ngc = 1      endifc this segment is used for shared memory computersc     do 30 j = 1, nx3c     scr(j,1,l) = f(j,kyp+2,kl)c     scr(j,2,l) = f(j,kyp+3,kll)c     scr(j,3,l) = f(j,1,kr)c  30 continuec this segment is used for mpi computers      call MPI_IRECV(scr,ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+1,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scr(1,3,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)      call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(scr(1,2,l),ngc*nxv,mreal,kll-1,moff+5,lgrp,msid,     1ierr)         call MPI_SEND(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+5,lgrp,ierr     1)         call MPI_WAIT(msid,istatus,ierr)      endifc add up the guard cells      do 40 j = 1, nx3      f(j,2,l) = f(j,2,l) + scr(j,1,l)      f(j,ngc+1,l) = f(j,ngc+1,l) + scr(j,2,l)      f(j,kyp+1,l) = f(j,kyp+1,l) + scr(j,3,l)   40 continue   50 continue      return      endc-----------------------------------------------------------------------      subroutine pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc ierr = (0,1) = (no,yes) error condition exists      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer nbsize, nter, msid, istatus      real any, yt      dimension msid(4), istatus(lstat), ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 50 l = 1, nblok      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 30 j = 1, npp(l)      yt = part(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = part(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,j,l)            sbufr(4,jsr(1,l),l) = part(4,j,l)            sbufr(5,jsr(1,l),l) = part(5,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endifc particles going down      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = part(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,j,l)            sbufl(4,jsl(1,l),l) = part(4,j,l)            sbufl(5,jsl(1,l),l) = part(5,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endif      endif   30 continue   40 jss(1,l) = jsl(1,l) + jsr(1,l)   50 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computersc     jsl(2,l) = jsr(1,kl)c     do 110 j = 1, jsl(2,l)c     rbufl(1,j,l) = sbufr(1,j,kl)c     rbufl(2,j,l) = sbufr(2,j,kl)c     rbufl(3,j,l) = sbufr(3,j,kl)c     rbufl(4,j,l) = sbufr(4,j,kl)c     rbufl(5,j,l) = sbufr(5,j,kl)c 110 continuec     jsr(2,l) = jsl(1,kr)c     do 120 j = 1, jsr(2,l)c     rbufr(1,j,l) = sbufl(1,j,kr)c     rbufr(2,j,l) = sbufl(2,j,kr)c     rbufr(3,j,l) = sbufl(3,j,kr)c     rbufr(4,j,l) = sbufl(4,j,kr)c     rbufr(5,j,l) = sbufl(5,j,kr)c 120 continuec this segment is used for mpi computersc post receive      call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)      call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particles      call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)     1,ierr)      call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),i     1err)c wait for particles to arrive      call MPI_WAIT(msid(1),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsl(2,l) = nps/idimp      call MPI_WAIT(msid(2),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (6,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (6,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sent      call MPI_WAIT(msid(3),istatus,ierr)      call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)         sbufl(5,jsl(1,l),l) = rbufr(5,j,l)c particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufr(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufr(3,j,l)         sbufr(4,jsr(1,l),l) = rbufr(4,j,l)         sbufr(5,jsr(1,l),l) = rbufr(5,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)         rbufr(5,jss(2,l),l) = rbufr(5,j,l)      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = rbufl(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = rbufl(3,j,l)            sbufr(4,jsr(1,l),l) = rbufl(4,j,l)            sbufr(5,jsr(1,l),l) = rbufl(5,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = rbufl(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = rbufl(3,j,l)            sbufl(4,jsl(1,l),l) = rbufl(4,j,l)            sbufl(5,jsl(1,l),l) = rbufl(5,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)         rbufl(5,jss(2,l),l) = rbufl(5,j,l)      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (6,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)      part(5,ihole(j,l),l) = rbufl(5,j,l)  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)         part(5,ihole(j+jsl(2,l),l),l) = rbufr(5,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)         part(5,j+npp(l),l) = rbufl(5,j+jss(1,l),l)      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)            part(5,ihole(j2,l),l) = part(5,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)         part(5,j+npp(l),l) = rbufr(5,j+jss(1,l),l)      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (6,*) 'Info: particles being passed further = ', ibflg(2)         if (ibflg(3).gt.0) ibflg(3) = 1         go to 100      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after movec     nps = 0c     do 270 l = 1, nblokc     nps = nps + npp(l)c 270 continuec     ibflg(2) = npsc     ibflg(1) = nprc     call PISUM(ibflg,iwork,2,1)c     if (ibflg(1).ne.ibflg(2)) thenc        write (6,*) 'particle number error, old/new=',ibflg(1),ibflg(2)c        ierr = 1c     endifc information      if (nter.gt.0) then         write (6,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine pxmov2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ier     2r)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc maskp = scratch array for particle addressesc ierr = (0,1) = (no,yes) error condition existsc optimized for vector processor      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, maskp, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok), maskp(npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer nbsize, nter, msid, istatus      real any, yt      dimension msid(4), istatus(lstat), ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 80 l = 1, nblok      jss(1,l) = 0      jss(2,l) = 0c find mask function for particles out of bounds      do 30 j = 1, npp(l)      yt = part(2,j,l)      if ((yt.ge.edges(2,l)).or.(yt.lt.edges(1,l))) then         jss(1,l) = jss(1,l) + 1         maskp(j,l) = 1      else         maskp(j,l) = 0      endif   30 continuec set flag if hole buffer would overflow      if (jss(1,l).gt.ntmax) then         jss(1,l) = ntmax         jss(2,l) = 1      endifc accumulate location of holes      do 40 j = 2, npp(l)      maskp(j,l) = maskp(j,l) + maskp(j-1,l)   40 continuec store addresses of particles out of bounds      do 50 j = 2, npp(l)      if ((maskp(j,l).gt.maskp(j-1,l)).and.(maskp(j,l).le.ntmax)) then         ihole(maskp(j,l),l) = j      endif   50 continue      if (maskp(1,l).gt.0) ihole(1,l) = 1      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0c load particle buffers      do 60 j = 1, jss(1,l)      yt = part(2,ihole(j,l),l)c particles going up      if (yt.ge.edges(2,l)) then         if ((kb+1).eq.nvp) yt = yt - any         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            sbufr(1,jsr(1,l),l) = part(1,ihole(j,l),l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,ihole(j,l),l)            sbufr(4,jsr(1,l),l) = part(4,ihole(j,l),l)            sbufr(5,jsr(1,l),l) = part(5,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endifc particles going down      else         if (kb.eq.0) yt = yt + any         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            sbufl(1,jsl(1,l),l) = part(1,ihole(j,l),l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,ihole(j,l),l)            sbufl(4,jsl(1,l),l) = part(4,ihole(j,l),l)            sbufl(5,jsl(1,l),l) = part(5,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endif      endif   60 continue   70 jss(1,l) = jsl(1,l) + jsr(1,l)   80 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computersc     jsl(2,l) = jsr(1,kl)c     do 110 j = 1, jsl(2,l)c     rbufl(1,j,l) = sbufr(1,j,kl)c     rbufl(2,j,l) = sbufr(2,j,kl)c     rbufl(3,j,l) = sbufr(3,j,kl)c     rbufl(4,j,l) = sbufr(4,j,kl)c     rbufl(5,j,l) = sbufr(5,j,kl)c 110 continuec     jsr(2,l) = jsl(1,kr)c     do 120 j = 1, jsr(2,l)c     rbufr(1,j,l) = sbufl(1,j,kr)c     rbufr(2,j,l) = sbufl(2,j,kr)c     rbufr(3,j,l) = sbufl(3,j,kr)c     rbufr(4,j,l) = sbufl(4,j,kr)c     rbufr(5,j,l) = sbufl(5,j,kr)c 120 continuec this segment is used for mpi computersc post receive      call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)      call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particles      call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)     1,ierr)      call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),i     1err)c wait for particles to arrive      call MPI_WAIT(msid(1),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsl(2,l) = nps/idimp      call MPI_WAIT(msid(2),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (6,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (6,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sent      call MPI_WAIT(msid(3),istatus,ierr)      call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)         sbufl(5,jsl(1,l),l) = rbufr(5,j,l)c particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufr(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufr(3,j,l)         sbufr(4,jsr(1,l),l) = rbufr(4,j,l)         sbufr(5,jsr(1,l),l) = rbufr(5,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)         rbufr(5,jss(2,l),l) = rbufr(5,j,l)      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = rbufl(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = rbufl(3,j,l)            sbufr(4,jsr(1,l),l) = rbufl(4,j,l)            sbufr(5,jsr(1,l),l) = rbufl(5,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = rbufl(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = rbufl(3,j,l)            sbufl(4,jsl(1,l),l) = rbufl(4,j,l)            sbufl(5,jsl(1,l),l) = rbufl(5,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)         rbufl(5,jss(2,l),l) = rbufl(5,j,l)      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (6,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)      part(5,ihole(j,l),l) = rbufl(5,j,l)  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)         part(5,ihole(j+jsl(2,l),l),l) = rbufr(5,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)         part(5,j+npp(l),l) = rbufl(5,j+jss(1,l),l)      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)            part(5,ihole(j2,l),l) = part(5,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)         part(5,j+npp(l),l) = rbufr(5,j+jss(1,l),l)      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (6,*) 'Info: particles being passed further = ', ibflg(2)         if (ibflg(3).gt.0) ibflg(3) = 1         go to 100      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after move      nps = 0      do 270 l = 1, nblok      nps = nps + npp(l)  270 continue      ibflg(2) = nps      ibflg(1) = npr      call PISUM(ibflg,iwork,2,1)      if (ibflg(1).ne.ibflg(2)) then         write (6,*) 'particle number error, old/new=',ibflg(1),ibflg(2)         ierr = 1      endifc information      if (nter.gt.0) then         write (6,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine ptpose(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jb     1lok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(nxv,kypd,kblok), g(nyv,kxpd,jblok)      dimension s(kxp,kyp,kblok), t(kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff,j,l) = f(j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,l),kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,msid,     1ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(j,k,l) = f(j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,l),kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,ierr     1)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(k+koff,j,l) = t(j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine p2tpose(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(2,nxv,kypd,kblok), g(2,nyv,kxpd,jblok)      dimension s(2,kxp,kyp,kblok), t(2,kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1,k+koff,j,l) = f(1,j+joff,k,i)c     g(2,k+koff,j,l) = f(2,j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec     returnc this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,1,l),2*kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,m     1sid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(1,j,k,l) = f(1,j+joff,k,l)         s(2,j,k,l) = f(2,j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,1,l),2*kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,     1ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(1,k+koff,j,l) = t(1,j,k,l)         g(2,k+koff,j,l) = t(2,j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine p3tpose(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:3,k+kyp*(m-1),j,l) = f(1:3,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/y      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(3,nxv,kypd,kblok), g(3,nyv,kxpd,jblok)      dimension s(3,kxp,kyp,kblok), t(3,kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb      integer jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1,k+koff,j,l) = f(1,j+joff,k,i)c     g(2,k+koff,j,l) = f(2,j+joff,k,i)c     g(3,k+koff,j,l) = f(3,j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,1,l),3*kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,m     1sid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(1,j,k,l) = f(1,j+joff,k,l)         s(2,j,k,l) = f(2,j+joff,k,l)         s(3,j,k,l) = f(3,j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,1,l),3*kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,     1ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(1,k+koff,j,l) = t(1,j,k,l)         g(2,k+koff,j,l) = t(2,j,k,l)         g(3,k+koff,j,l) = t(3,j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine PTPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblok     1,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(nxv*kypd*kblok), g(nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff+nyv*(j-1+kxpd*(l-1))) = f(j+joff+nxv*(k-1+kypd*(i-1)))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, kxp         g(j+kxp*(k+is)) = f(j+joff+nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continuec exchange data      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+kxp*kyp*(ir+koff)),kxp*kyp,mcplx,ir-1,ir+kxy     1m+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+kxp*kyp*(is+ioff)),kxp*kyp,mcplx,is-1,l+ks+kx     1ym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(k+koff+nyv*(j+joff)) = f(j+kxp*(k+ir))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      subroutine P2TPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(2*nxv*kypd*kblok), g(2*nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks) - 1c     do 30 i = 1, kybc     koff = kyp*(i - 1) - 1c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(1+2*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(2+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(2+2*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = 2*kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, 2*kxp         g(j+2*kxp*(k+is)) = f(j+joff+2*nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continue      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+2*kxp*kyp*(ir+koff)),2*kxp*kyp,mcplx,ir-1,ir     1+kxym+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+2*kxp*kyp*(is+ioff)),2*kxp*kyp,mcplx,is-1,l+k     1s+kxym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(2*(k+koff+nyv*(j+joff))-1) = f(2*(j+kxp*(k+ir))-1)         g(2*(k+koff+nyv*(j+joff))) = f(2*(j+kxp*(k+ir)))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      subroutine P3TPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:3,k+kyp*(m-1),j,l) = f(1:3,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(3*nxv*kypd*kblok), g(3*nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks) - 1c     do 30 i = 1, kybc     koff = kyp*(i - 1) - 1c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1+3*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(1+3*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(2+3*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(2+3*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(3+3*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(3+3*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = 3*kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, 3*kxp         g(j+3*kxp*(k+is)) = f(j+joff+3*nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continue      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+3*kxp*kyp*(ir+koff)),3*kxp*kyp,mcplx,ir-1,ir     1+kxym+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+3*kxp*kyp*(is+ioff)),3*kxp*kyp,mcplx,is-1,l+k     1s+kxym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(3*(k+koff+nyv*(j+joff))-2) = f(3*(j+kxp*(k+ir))-2)         g(3*(k+koff+nyv*(j+joff))-1) = f(3*(j+kxp*(k+ir))-1)         g(3*(k+koff+nyv*(j+joff))) = f(3*(j+kxp*(k+ir)))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      function ranorm()c this program calculates a random number y from a gaussian distributionc with zero mean and unit variance, according to the method ofc mueller and box:c    y(k) = (-2*ln(x(k)))**1/2*sin(2*pi*x(k+1))c    y(k+1) = (-2*ln(x(k)))**1/2*cos(2*pi*x(k+1)),c where x is a random number uniformly distributed on (0,1).c written for the ibm by viktor k. decyk, ucla      integer r1,r2,r4,r5      double precision ranorm,h1l,h1u,h2l,r0,r3,asc,bsc,temp      save iflg,r1,r2,r4,r5,h1l,h1u,h2l,r0      data r1,r2,r4,r5 /885098780,1824280461,1396483093,55318673/      data h1l,h1u,h2l /65531.0d0,32767.0d0,65525.0d0/      data iflg,r0 /0,0.0d0/      if (iflg.eq.0) go to 10      ranorm = r0      r0 = 0.0d0      iflg = 0      return   10 isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r1 - (r1/isc)*isc      r3 = h1l*dble(r1) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r2/isc      isc = r2 - i1*isc      r0 = h1l*dble(r2) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r2 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r1 = r3 - dble(isc)*bsc      temp = dsqrt(-2.0d0*dlog((dble(r1) + dble(r2)*asc)*asc))      isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r4 - (r4/isc)*isc      r3 = h2l*dble(r4) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r5/isc      isc = r5 - i1*isc      r0 = h2l*dble(r5) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r5 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r4 = r3 - dble(isc)*bsc      r0 = 6.28318530717959d0*((dble(r4) + dble(r5)*asc)*asc)      ranorm = temp*dsin(r0)      r0 = temp*dcos(r0)      iflg = 1      return      endc-----------------------------------------------------------------------      subroutine timera(icntrl,chr,time)c this subroutine performs timingc input: icntrl, chrc icntrl = (-1,0,1) = (initialize,ignore,read) clockc clock should be initialized before it is read!c chr = character variable for labeling timingsc time = elapsed time in secondsc written for mpi      implicit none      integer icntrl      character*8 chr      real timec get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer idproc, ierr      real nclock, mclock      double precision jclock      save jclock   91 format (1x,a8,1x,'max/min real time = ',e14.7,1x,e14.7,1x,'sec')      data jclock /0.0d0/      if (icntrl.eq.0) return      if (icntrl.eq.1) go to 10c initialize clock      call MPI_BARRIER(lgrp,ierr)      jclock = MPI_WTIME()      returnc read clock and write time difference from last clock initialization   10 nclock = real(MPI_WTIME() - jclock)      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MIN,lgrp,ierr)      mclock = time      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MAX,lgrp,ierr)      call MPI_COMM_RANK(lgrp,idproc,ierr)      if (idproc.eq.0) write (6,91) chr, time, mclock      return      endc-----------------------------------------------------------------------      subroutine PSUM (f,g,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c f(j,k) = sum over k of f(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus      integer idproc, ierr, kstrt, ks, l, kxs, k, kb, lb, msid, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = f(j,kb+kxs)c     elsec        g(j,k) = f(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      f(j,k) = f(j,k) + g(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PISUM (if,ig,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c if(j,k) = sum over k of if(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c if = input and output integer datac ig = scratch integer arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      integer if, ig, nxp, nblok      dimension if(nxp,nblok), ig(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mint = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus      integer idproc, ierr, kstrt, ks, l, kxs, k, kb, lb, nsid, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        ig(j,k) = if(j,kb+kxs)c     elsec        ig(j,k) = if(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_ISEND(if,nxp,mint,kb+kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb+kxs-1,l+nxp,lgrp,istatus,ierr)      else         call MPI_ISEND(if,nxp,mint,kb-kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb-kxs-1,l+nxp,lgrp,istatus,ierr)      endif      call MPI_WAIT(nsid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      if(j,k) = if(j,k) + ig(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PIMAX(if,ig,nxp,nblok)c this subroutine finds parallel maximum for each element of a vectorc that is, if(j,k) = maximum as a function of k of if(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c if = input and output integer datac ig = scratch integer arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      integer if, ig      integer nxp, nblok      dimension if(nxp,nblok), ig(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        ig(j,k) = if(j,kb+kxs)c     elsec        ig(j,k) = if(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(ig,nxp,mint,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(if,nxp,mint,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(ig,nxp,mint,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(if,nxp,mint,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec find maximum      do 50 k = 1, nblok      do 40 j = 1, nxp      if(j,k) = max0(if(j,k),ig(j,k))   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PFMOVE2(f,g,noff,nyp,noffs,nyps,noffd,nypd,jsr,jsl,isig     1n,kyp,kstrt,nvp,nxv,nypmx,nblok,idps,mter,ierr)c this subroutine moves fields into appropriate spatial regions,c between non-uniform and uniform partitionsc f(j,k,l) = real data for grid j,k in field partition l.c the grid is non-uniform and includes extra guard cells.c g(j,k,l) = scratch data for grid j,k in field partition l.c noff(l) = lowermost global gridpoint in field partition lc nyp(l) = number of primary gridpoints in field partition lc noffs(l)/nyps(l) = source or scratch arrays for field partition lc noffd(l)/nypd(l) = destination or scratch arrays for field partition lc jsl(idps,l) = number of particles going down in field partition lc jsr(idps,l) = number of particles going up in field partition lc isign = -1, move from non-uniform (noff/nyp) to uniform (kyp) fieldsc isign = 1, move from uniform (kyp) to non-uniform (noff/nyp) fieldsc if isign = 0, the noffs/nyps contains the source partition, noffd/nypdc    contains the destination partition, and noff/nyp, kyp are not used.c    the source partition noffs/nyps is modified.c kyp = number of complex grids in each uniform field partition.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c nblok = number of field partitions.c idps = number of partition boundariesc mter = number of shifts requiredc if mter = 0, then number of shifts is determined and returnedc ierr = (0,1) = (no,yes) error condition exists      implicit none      real f, g      integer noff, nyp, noffs, nyps, noffd, nypd, jsr, jsl      integer isign, kyp, kstrt, nvp, nxv, nypmx, nblok, idps, mter      integer ierr      dimension f(nxv,nypmx,nblok), g(nxv,nypmx,nblok)      dimension noff(nblok), nyp(nblok)      dimension noffs(nblok), nyps(nblok), noffd(nblok), nypd(nblok)      dimension jsl(idps,nblok), jsr(idps,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer j, k, l      integer nbsize, ks, iter, npr, nps, nter, koff, kl, kr, kk      integer msid, istatus      integer ibflg, iwork      dimension istatus(lstat)      dimension ibflg(2), iwork(2)c exit if certain flags are set      if (mter.lt.0) return      ks = kstrt - 2      nbsize = nxv*nypmx      iter = 2      ierr = 0c move from non-uniform to uniform fields      if (isign.lt.0) thenc copy non-uniform partition parameters         do 10 l = 1, nblok         noffs(l) = noff(l)         nyps(l) = nyp(l)         koff = kyp*(l + ks)         noffd(l) = koff         nypd(l) = kyp   10    continuec move from uniform to non-uniform fields      else if (isign.gt.0) thenc set uniform partition parameters         do 20 l = 1, nblok         koff = kyp*(l + ks)         noffs(l) = koff         nyps(l) = kyp         noffd(l) = noff(l)         nypd(l) = nyp(l)   20    continue      endifc determine number of outgoing grids   30 do 50 l = 1, nblok      kl = noffd(l)      kr = kl + nypd(l)      jsl(1,l) = 0      jsr(1,l) = 0      do 40 k = 1, nyps(l)      kk = k + noffs(l)c fields going up      if (kk.gt.kr) then         jsr(1,l) = jsr(1,l) + 1c fields going down      else if (kk.le.kl) then         jsl(1,l) = jsl(1,l) + 1      endif   40 continue   50 continuec copy fields      iter = iter + 2      npr = 0      nter = 0c get fields from below      do 80 l = 1, nblok      kr = l + ks + 2      kl = l + ks      jsl(2,l) = 0      jsr(2,l) = 0c this segment is used for shared memory computersc     if (noffs(l).gt.noffd(l)) then     c        jsl(2,l) = jsr(1,kl)c        do 70 k = 1, jsl(2,l)c        do 60 j = 1, nxvc        g(j,k,l) = f(j,k+nyps(kl)-jsr(1,kl),kl)c  60    continuec  70    continuec     endifc this segment is used for mpi computersc post receive from left      if (noffs(l).gt.noffd(l)) then               call MPI_IRECV(g,nbsize,mreal,kl-1,iter-1,lgrp,msid,ierr)      endifc send fields to right      if (jsr(1,l).gt.0) then         call MPI_SEND(f(1,nyps(l)-jsr(1,l)+1,l),nxv*jsr(1,l),mreal,kr-1     1,iter-1,lgrp,ierr)      endifc wait for fields to arrive      if (noffs(l).gt.noffd(l)) then          call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nps,ierr)         jsl(2,l) = nps/nxv      endif   80 continuec adjust field      do 150 l = 1, nblokc adjust field size      nyps(l) = nyps(l) - jsr(1,l)c do not allow move to overflow field array      jsr(1,l) = max0((nyps(l)+jsl(2,l)-nypmx),0)      nyps(l) = nyps(l) - jsr(1,l)      if (jsr(1,l).gt.0) then         npr = max0(npr,jsr(1,l))c save whatever is possible into end of g         kk = min0(jsr(1,l),nypmx-jsl(2,l))         do 100 k = 1, kk         do  90 j = 1, nxv         g(j,nypmx-kk+k,l) = f(j,nyps(l)+k,l)   90    continue  100    continue      endifc shift data which is staying, if necessary      if ((nyps(l).gt.0).and.(jsl(2,l).gt.0)) then         do 120 k = 1, nyps(l)         kk = nyps(l) - k + 1         do 110 j = 1, nxv         f(j,kk+jsl(2,l),l) = f(j,kk,l)  110    continue  120    continue      endifc insert data coming from left      do 140 k = 1, jsl(2,l)      do 130 j = 1, nxv      f(j,k,l) = g(j,k,l)  130 continue  140 continuec adjust field size and offset      nyps(l) = nyps(l) + jsl(2,l)      noffs(l) = noffs(l) - jsl(2,l)  150 continuec get fields from above      do 180 l = 1, nblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if ((noffs(l)+nyps(l)).lt.(noffd(l)+nypd(l))) then c        jsr(2,l) = jsl(1,kr)c        do 170 k = 1, jsr(2,l)c        do 160 j = 1, nxvc        g(j,k,l) =  f(j,k,kr)c 160    continuec 170    continuec     endifc this segment is used for mpi computersc post receive from right      if ((noffs(l)+nyps(l)).lt.(noffd(l)+nypd(l))) then              call MPI_IRECV(g,nbsize,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send fields to left      if (jsl(1,l).gt.0) then         call MPI_SEND(f(1,1,l),nxv*jsl(1,l),mreal,kl-1,iter,lgrp,ierr)      endifc wait for fields to arrive      if ((noffs(l)+nyps(l)).lt.(noffd(l)+nypd(l))) then           call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nps,ierr)         jsr(2,l) = nps/nxv      endif  180 continuec adjust field      do 240 l = 1, nblokc adjust field size      nyps(l) = nyps(l) - jsl(1,l)      noffs(l) = noffs(l) + jsl(1,l)c shift data which is staying, if necessary      if ((nyps(l).gt.0).and.(jsl(1,l).gt.0)) then         do 200 k = 1, nyps(l)         do 190 j = 1, nxv         f(j,k,l) = f(j,k+jsl(1,l),l)  190    continue  200    continue      endifc do not allow move to overflow field array      jsl(1,l) = max0((nyps(l)+jsr(2,l)-nypmx),0)      if (jsl(1,l).gt.0) then         npr = max0(npr,jsl(1,l))         jsr(2,l) = jsr(2,l) - jsl(1,l)c do not process if prior error      else if (jsr(1,l).gt.0) then         go to 230      endifc insert data coming from right      do 220 k = 1, jsr(2,l)      do 210 j = 1, nxv      f(j,k+nyps(l),l) = g(j,k,l)  210 continue  220 continuec adjust field size and offset      nyps(l) = nyps(l) + jsr(2,l)c check if new partition is uniform  230 nter = nter + abs(nyps(l)-nypd(l)) + abs(noffs(l)-noffd(l))  240 continuec calculate number of iterations      nps = iter/2 - 1      if (nps.le.mter) thenc process errors         if (npr.ne.0) then            ierr = npr            write (2,*) 'local field overflow error, ierr = ', ierr            return         endif         if (nps.lt.mter) go to 30         return      endifc process errors      ibflg(1) = npr      ibflg(2) = nter      call PIMAX(ibflg,iwork,2,1)c field overflow error      if (ibflg(1).ne.0) then         ierr = ibflg(1)         write (2,*) 'global field overflow error, ierr = ', ierr         return      endifc check if any fields have to be passed further      if (ibflg(2).gt.0) then         write (2,*) 'Info: fields being passed further = ', ibflg(2)         go to 30      endif      mter = nps      return      endc-----------------------------------------------------------------------      subroutine PNCGUARD2(f,scs,nyp,kstrt,nvp,nxv,nypmx,nblok,mter)c this subroutine copies data to guard cells in non-uniform partitionsc f(j,k,l) = real data for grid j,k in field partition l.c the grid is non-uniform and includes three extra guard cells.c scs(j,l) = scratch array for field partition lc nyp(l) = number of primary gridpoints in field partition lc it is assumed the nyp(l) > 0.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of field partition, including guard cells.c nblok = number of field partitions.c mter = (0,1) = (no,yes) pass data to next processor onlyc quadratic interpolation, for distributed data      implicit none      real f, scs      integer nyp      integer kstrt, nvp, nxv, nypmx, nblok, mter      dimension f(nxv,nypmx,nblok), scs(nxv,nblok)      dimension nyp(nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer ks, moff, kr, krr, kl, kll, ngc, nps, lc     integer j      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 30 l = 1, nblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr + 1      if (krr.gt.nvp) krr = krr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl - 1      if (kll.lt.1) kll = kll + nvp      ngc = 0c special case of only one grid per processor      if (nyp(l).eq.1) ngc = 1c this segment is used for shared memory computersc     if (nyp(kr).eq.1) thenc        do 10 j = 1, nxvc        f(j,1,l) = f(j,nyp(kl)+1,kl)c        f(j,nyp(l)+2,l) = f(j,2,kr)c        f(j,nyp(l)+3,l) = f(j,2,krr)c  10    continuec     elsec        do 20 j = 1, nxvc        f(j,1,l) = f(j,nyp(kl)+1,kl)c        f(j,nyp(l)+2,l) = f(j,2,kr)c        f(j,nyp(l)+3,l) = f(j,3,kr)c  20    continuec     endifc this segment is used for mpi computers      call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+3,lgrp,msid,ierr)      call MPI_SEND(f(1,nyp(l)+1,l),nxv,mreal,kr-1,moff+3,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(f(1,nyp(l)+2,l),2*nxv,mreal,kr-1,moff+4,lgrp,msid,i     1err)      call MPI_SEND(f(1,2,l),(2-ngc)*nxv,mreal,kl-1,moff+4,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)c special case of only one grid per processor      if (mter.ge.1) go to 30      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      if (nps.eq.nxv) then         call MPI_IRECV(f(1,nyp(l)+3,l),nxv,mreal,krr-1,moff+6,lgrp,msid     1,ierr)      else         call MPI_IRECV(scs,nxv,mreal,krr-1,moff+6,lgrp,msid,ierr)      endif      call MPI_SEND(f(1,2,l),nxv,mreal,kll-1,moff+6,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)   30 continue      return      endc-----------------------------------------------------------------------      subroutine PNACGUARD2(f,scr,scs,nyp,kstrt,nvp,nx,nxv,nypmx,nblok,n     1gds,mter)c this subroutine adds data from guard cells in non-uniform partitionsc f(3,j,k,l) = real data for grid j,k in field partition l.c the grid is non-uniform and includes three extra guard cells.c nyp(l) = number of primary gridpoints in field partition lc it is assumed the nyp(l) > 0.c scr(3,j,ngds,l) = scratch array for field partition lc scs(3,j,l) = scratch array for field partition lc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of field partition, including guard cells.c nblok = number of field partitions.c ngds = number of guard cellsc mter = (0,1) = (no,yes) pass data to next processor onlyc quadratic interpolation, for distributed data      implicit none      real f, scr, scs      integer nyp      integer kstrt, nvp, nx, nxv, nypmx, nblok, ngds, mter      dimension f(3,nxv,nypmx,nblok), scr(3,nxv,ngds,nblok)      dimension scs(3,nxv,nblok)      dimension nyp(nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l, m      dimension istatus(lstat)      nx3 = nx + 3c special case for one processor      if (nvp.eq.1) then         do 30 l = 1, nblok         do 20 j = 1, nx3         do 10 m = 1, 3         f(m,j,2,l) = f(m,j,2,l) + f(m,j,nyp(l)+2,l)         f(m,j,3,l) = f(m,j,3,l) + f(m,j,nyp(l)+3,l)         f(m,j,nyp(l)+1,l) = f(m,j,nyp(l)+1,l) + f(m,j,1,l)         f(m,j,1,l) = 0.         f(m,j,nyp(l)+2,l) = 0.         f(m,j,nyp(l)+3,l) = 0.   10    continue   20    continue   30    continue         return      endif      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 110 l = 1, nblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr + 1      if (krr.gt.nvp) krr = krr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl - 1      if (kll.lt.1) kll = kll + nvpc this segment is used for shared memory computersc     if (nyp(kl).eq.1) thenc        do 50 j = 1, nx3c        do 40 m = 1, 3c        scr(m,j,1,l) = f(m,j,nyp(kl)+2,kl) + f(m,j,nyp(kll)+2,kll)c        scr(m,j,2,l) = f(m,j,nyp(kl)+3,kl)c        scr(m,j,3,l) = f(m,j,1,kr)c  40    continuec  50    continuec     elsec        do 70 j = 1, nx3c        do 60 m = 1, 3c        scr(m,j,1,l) = f(m,j,nyp(kl)+2,kl)c        scr(m,j,2,l) = f(m,j,nyp(kl)+3,kl)c        scr(m,j,3,l) = f(m,j,1,kr)c  60    continuec  70    continuec     endifc this segment is used for mpi computers      call MPI_IRECV(scr,6*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,1,nyp(l)+2,l),6*nxv,mreal,kr-1,moff+1,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scr(1,1,3,l),3*nxv,mreal,kr-1,moff+2,lgrp,msid,ierr     1)      call MPI_SEND(f(1,1,1,l),3*nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)c special case of only one grid per processor      if (mter.ge.1) go to 80      call MPI_IRECV(ngc,1,mint,kl-1,moff+3,lgrp,msid,ierr)      call MPI_SEND(nyp(l),1,mint,kr-1,moff+3,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scs,3*nxv,mreal,kll-1,moff+5,lgrp,msid,ierr)      call MPI_SEND(f(1,1,nyp(l)+3,l),3*nxv,mreal,krr-1,moff+5,lgrp,ierr     1)      call MPI_WAIT(msid,istatus,ierr)      if (ngc.eq.1) then         do 50 j = 1, nx3         do 40 m = 1, 3         scr(m,j,1,l) = scr(m,j,1,l) + scs(m,j,l)   40    continue   50    continue      endifc add up the guard cells   80 do 100 j = 1, nx3      do 90 m = 1, 3      f(m,j,2,l) = f(m,j,2,l) + scr(m,j,1,l)      f(m,j,3,l) = f(m,j,3,l) + scr(m,j,2,l)      f(m,j,nyp(l)+1,l) = f(m,j,nyp(l)+1,l) + scr(m,j,3,l)      f(m,j,1,l) = 0.      f(m,j,nyp(l)+2,l) = 0.      f(m,j,nyp(l)+3,l) = 0.   90 continue  100 continue  110 continue      return      endc-----------------------------------------------------------------------      subroutine PNACGUARD22(f,scr,scs,nyp,kstrt,nvp,nx,nxv,nypmx,nblok,     1ngds,mter)c this subroutine adds data from guard cells in non-uniform partitionsc f(2,j,k,l) = real data for grid j,k in field partition l.c the grid is non-uniform and includes three extra guard cells.c nyp(l) = number of primary gridpoints in field partition lc it is assumed the nyp(l) > 0.c scr(2,j,ngds,l) = scratch array for field partition lc scs(2,j,l) = scratch array for field partition lc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of field partition, including guard cells.c nblok = number of field partitions.c ngds = number of guard cellsc mter = (0,1) = (no,yes) pass data to next processor onlyc quadratic interpolation, for distributed data      implicit none      real f, scr, scs      integer nyp      integer kstrt, nvp, nx, nxv, nypmx, nblok, ngds, mter      dimension f(2,nxv,nypmx,nblok), scr(2,nxv,ngds,nblok)      dimension scs(2,nxv,nblok)      dimension nyp(nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l, m      dimension istatus(lstat)      nx3 = nx + 3c special case for one processor      if (nvp.eq.1) then         do 30 l = 1, nblok         do 20 j = 1, nx3         do 10 m = 1, 2         f(m,j,2,l) = f(m,j,2,l) + f(m,j,nyp(l)+2,l)         f(m,j,3,l) = f(m,j,3,l) + f(m,j,nyp(l)+3,l)         f(m,j,nyp(l)+1,l) = f(m,j,nyp(l)+1,l) + f(m,j,1,l)         f(m,j,1,l) = 0.         f(m,j,nyp(l)+2,l) = 0.         f(m,j,nyp(l)+3,l) = 0.   10    continue   20    continue   30    continue         return      endif      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 110 l = 1, nblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr + 1      if (krr.gt.nvp) krr = krr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl - 1      if (kll.lt.1) kll = kll + nvpc this segment is used for shared memory computersc     if (nyp(kl).eq.1) thenc        do 50 j = 1, nx3c        do 40 m = 1, 2c        scr(m,j,1,l) = f(m,j,nyp(kl)+2,kl) + f(m,j,nyp(kll)+2,kll)c        scr(m,j,2,l) = f(m,j,nyp(kl)+3,kl)c        scr(m,j,3,l) = f(m,j,1,kr)c  40    continuec  50    continuec     elsec        do 70 j = 1, nx3c        do 60 m = 1, 2c        scr(m,j,1,l) = f(m,j,nyp(kl)+2,kl)c        scr(m,j,2,l) = f(m,j,nyp(kl)+3,kl)c        scr(m,j,3,l) = f(m,j,1,kr)c  60    continuec  70    continuec     endifc this segment is used for mpi computers      call MPI_IRECV(scr,4*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,1,nyp(l)+2,l),4*nxv,mreal,kr-1,moff+1,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scr(1,1,3,l),2*nxv,mreal,kr-1,moff+2,lgrp,msid,ierr     1)      call MPI_SEND(f(1,1,1,l),2*nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)c special case of only one grid per processor      if (mter.ge.1) go to 80      call MPI_IRECV(ngc,1,mint,kl-1,moff+3,lgrp,msid,ierr)      call MPI_SEND(nyp(l),1,mint,kr-1,moff+3,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scs,2*nxv,mreal,kll-1,moff+5,lgrp,msid,ierr)      call MPI_SEND(f(1,1,nyp(l)+3,l),2*nxv,mreal,krr-1,moff+5,lgrp,ierr     1)      call MPI_WAIT(msid,istatus,ierr)      if (ngc.eq.1) then         do 50 j = 1, nx3         do 40 m = 1, 2         scr(m,j,1,l) = scr(m,j,1,l) + scs(m,j,l)   40    continue   50    continue      endifc add up the guard cells   80 do 100 j = 1, nx3      do 90 m = 1, 2      f(m,j,2,l) = f(m,j,2,l) + scr(m,j,1,l)      f(m,j,3,l) = f(m,j,3,l) + scr(m,j,2,l)      f(m,j,nyp(l)+1,l) = f(m,j,nyp(l)+1,l) + scr(m,j,3,l)      f(m,j,1,l) = 0.      f(m,j,nyp(l)+2,l) = 0.      f(m,j,nyp(l)+3,l) = 0.   90 continue  100 continue  110 continue      return      endc-----------------------------------------------------------------------      subroutine PNAGUARD2(f,scr,scs,nyp,kstrt,nvp,nx,nxv,nypmx,nblok,ng     1ds,mter)c this subroutine adds data from guard cells in non-uniform partitionsc f(j,k,l) = real data for grid j,k in field partition l.c the grid is non-uniform and includes three extra guard cells.c nyp(l) = number of primary gridpoints in field partition lc it is assumed the nyp(l) > 0.c scr(j,ngds,l) = scratch array for field partition lc scs(j,l) = scratch array for field partition lc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of field partition, including guard cells.c nblok = number of field partitions.c ngds = number of guard cellsc mter = (0,1) = (no,yes) pass data to next processor onlyc quadratic interpolation, for distributed data      implicit none      real f, scr, scs      integer nyp      integer kstrt, nvp, nx, nxv, nypmx, nblok, ngds, mter      dimension f(nxv,nypmx,nblok), scr(nxv,ngds,nblok)      dimension scs(nxv,nblok)      dimension nyp(nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, msid, ierr      integer nx3, ks, moff, kr, krr, kl, kll, ngc, j, l      dimension istatus(lstat)      nx3 = nx + 3c special case for one processor      if (nvp.eq.1) then         do 20 l = 1, nblok         do 10 j = 1, nx3         f(j,2,l) = f(j,2,l) + f(j,nyp(l)+2,l)         f(j,3,l) = f(j,3,l) + f(j,nyp(l)+3,l)         f(j,nyp(l)+1,l) = f(j,nyp(l)+1,l) + f(j,1,l)         f(j,1,l) = 0.         f(j,nyp(l)+2,l) = 0.         f(j,nyp(l)+3,l) = 0.   10    continue   20    continue         return      endif      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 70 l = 1, nblok      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      krr = kr + 1      if (krr.gt.nvp) krr = krr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvp      kll = kl - 1      if (kll.lt.1) kll = kll + nvp      ngc = 0c this segment is used for shared memory computersc     if (nyp(kl).eq.1) thenc        do 30 j = 1, nx3c        scr(j,1,l) = f(j,nyp(kl)+2,kl)c        scr(j,2,l) = f(j,nyp(kll)+2,kll)c        scr(j,3,l) = f(j,1,kr)c  30    continuec     elsec        do 40 j = 1, nx3c        scr(j,1,l) = f(j,nyp(kl)+2,kl)c        scr(j,2,l) = f(j,nyp(kl)+3,kl)c        scr(j,3,l) = f(j,1,kr)c  40    continuec     endifc this segment is used for mpi computers      call MPI_IRECV(scr,2*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,nyp(l)+2,l),2*nxv,mreal,kr-1,moff+1,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scr(1,3,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)      call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)c special case of only one grid per processor      if (mter.ge.1) go to 50      call MPI_IRECV(ngc,1,mint,kl-1,moff+3,lgrp,msid,ierr)      call MPI_SEND(nyp(l),1,mint,kr-1,moff+3,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      call MPI_IRECV(scs,nxv,mreal,kll-1,moff+5,lgrp,msid,ierr)      call MPI_SEND(f(1,nyp(l)+3,l),nxv,mreal,krr-1,moff+5,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (ngc.eq.1) then         do 30 j = 1, nx3         scr(j,1,l) = scr(j,1,l) + scs(j,l)   30    continue      endifc add up the guard cells   50 do 60 j = 1, nx3      f(j,2,l) = f(j,2,l) + scr(j,1,l)      f(j,3,l) = f(j,3,l) + scr(j,2,l)      f(j,nyp(l)+1,l) = f(j,nyp(l)+1,l) + scr(j,3,l)      f(j,1,l) = 0.      f(j,nyp(l)+2,l) = 0.      f(j,nyp(l)+3,l) = 0.   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine REPARTD2(edges,edg,eds,eg,es,et2,npic,noff,nyp,npav,nyp     1min,nypmax,kstrt,nvp,nblok,idps,nypm)c this subroutines finds new partitions boundaries (edges,noff,nyp)c from old partition information (npic,nyp).c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc edg/eds/eg/es/et2 = scratch arraysc npic(l) = number of particles per grid in partition lc noff(l) = lowermost global gridpoint in particle partition lc nyp(l) = number of primary gridpoints in particle partition lc npav = average number of particles per partition desiredc nypmin/nypmax = minimum/maximum value of nyp in new partitionc kstrt = starting data block numberc nvp = number of real or virtual processorsc nblok = number of field partitions.c idps = number of partition boundariesc nypm = maximum size of particle partition      implicit none      real edges, edg, eds, eg, es, et2      integer npic, noff, nyp      integer npav, nypmin, nypmax, kstrt, nvp, nblok, idps, nypm      dimension edges(idps,nblok)      dimension edg(nypm,nblok), eds(nypm,nblok)      dimension eg(idps,nblok), es(idps,nblok), et2(2*idps,nblok)      dimension npic(nypm,nblok), noff(nblok), nyp(nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, iter, nter, nyp1, k1, kl, kr, k, l, ierr      real sum1, at1, at2, anpav, apav, anpl, anpr      integer msid, istatus      integer ibflg, iwork      dimension istatus(lstat)      dimension ibflg(2), iwork(2)c exit if flag is set      ks = kstrt - 2      iter = 2      anpav = real(npav)c copy number of particles and grid in current partition      do 20 l = 1, nblok      sum1 = 0.      do 10 k = 1, nyp(l)      at1 = npic(k,l)      sum1 = sum1 + at1      eds(k,l) = at1   10 continue      edges(1,l) = sum1      edges(2,l) = nyp(l)      et2(1,l) = edges(1,l)      et2(2,l) = edges(2,l)   20 continuec perform running sum      call PSCAN(edges,eg,es,idps,nblok)      do 30 l = 1, nblok      es(1,l) = et2(1,l)      es(2,l) = et2(2,l)      et2(1,l) = edges(1,l)      et2(2,l) = edges(2,l)      et2(3,l) = et2(1,l)      et2(4,l) = et2(2,l)      eg(1,l) = 0.      eg(2,l) = 0.      edges(2,l) = 1.0   30 continuec move partitions   40 iter = iter + 2c get partition from left      do 60 l = 1, nblok      kr = l + ks + 2      kl = l + ksc apav = desired number of particles on processor to left      apav = real(kl)*anpavc anpl = deficit of particles on processor to left      anpl = apav - et2(1,l) + es(1,l)c anpr = excess of particles on current processor      anpr = et2(1,l) - apav - anpavc this segment is used for shared memory computersc     if (anpl.lt.0.) thenc        nyp1 = es(2,kl)c        do 50 k = 1, nyp1c        edg(k,l) = eds(k,kl)c  50    continuec        eg(1,l) = es(1,kl)c        eg(2,l) = es(2,kl)c     endifc this segment is used for mpi computersc post receive from left      if (anpl.lt.0.) then         call MPI_IRECV(edg,nypm,mreal,kl-1,iter-1,lgrp,msid,ierr)      endifc send partition to right      if (anpr.gt.0.) then         nyp1 = es(2,l)         call MPI_SEND(eds,nyp1,mreal,kr-1,iter-1,lgrp,ierr)      endifc wait for partition to arrive      if (anpl.lt.0.) then         call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nyp1,ierr)         eg(2,l) = nyp1         sum1 = 0.         do 50 k = 1, nyp1         sum1 = sum1 + edg(k,l)   50    continue         eg(1,l) = sum1      endif   60 continuec find new partitions      nter = 0      do 100 l = 1, nblok      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et2(1,l) + es(1,l)      anpr = et2(1,l) - apav - anpavc left boundary is on the left      if (anpl.lt.0.) then         if ((anpl+eg(1,l)).ge.0.) then            nyp1 = eg(2,l)            k1 = nyp1            sum1 = 0.   70       at1 = sum1            sum1 = sum1 - edg(k1,l)            k1 = k1 - 1            if ((sum1.gt.anpl).and.(k1.gt.0)) go to 70            at1 = real(nyp1 - k1 - 1) + (anpl - at1)/(sum1 - at1)            edges(1,l) = (et2(2,l) - es(2,l)) - at1c left boundary is even further to left         else            nter = nter + 1         endifc left boundary is inside      else if (et2(1,l).ge.apav) then         nyp1 = es(2,l)         k1 = 1         sum1 = 0.   80    at1 = sum1         sum1 = sum1 + eds(k1,l)         k1 = k1 + 1         if ((sum1.lt.anpl).and.(k1.le.nyp1)) go to 80         at2 = real(k1 - 2)         if (sum1.gt.at1) at2 = at2 + (anpl - at1)/(sum1 - at1)         edges(1,l) = (et2(2,l) - es(2,l)) + at2      endifc right going data will need to be sent      if (anpr.gt.es(1,l)) nter = nter + 1      if (kl.gt.0) then         nyp1 = eg(2,l)         do 90 k = 1, nyp1         eds(k,l) = edg(k,l)   90    continue         et2(1,l) = et2(1,l) - es(1,l)         et2(2,l) = et2(2,l) - es(2,l)         es(1,l) = eg(1,l)         es(2,l) = eg(2,l)      endif  100 continuec get more data from left      if (nter.gt.0) go to 40      iter = nvp + 2c restore partition data      do 120 l = 1, nblok      sum1 = 0.      do 110 k = 1, nyp(l)      at1 = npic(k,l)      sum1 = sum1 + at1      eds(k,l) = at1  110 continue      et2(1,l) = et2(3,l)      et2(2,l) = et2(4,l)      es(1,l) = sum1      es(2,l) = nyp(l)      eg(1,l) = 0.      eg(2,l) = 0.  120 continuec continue moving partitions  130 iter = iter + 2c get partition from right      do 150 l = 1, nblok      kr = l + ks + 2      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et2(1,l) + es(1,l)c this segment is used for shared memory computersc     if (et2(1,l).lt.apav) thenc        nyp1 = es(2,kr)c        do 140 k = 1, nyp1c        edg(k,l) = eds(k,kr)c 140    continuec        eg(1,l) = es(1,kr)c        eg(2,l) = es(2,kr)c     endifc this segment is used for mpi computersc post receive from right      if (et2(1,l).lt.apav) then         call MPI_IRECV(edg,nypm,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send partition to left      if (anpl.gt.anpav) then         nyp1 = es(2,l)         call MPI_SEND(eds,nyp1,mreal,kl-1,iter,lgrp,ierr)      endifc wait for partition to arrive      if (et2(1,l).lt.apav) then         call MPI_WAIT(msid,istatus,ierr)         call MPI_GET_COUNT(istatus,mreal,nyp1,ierr)         eg(2,l) = nyp1         sum1 = 0.         do 140 k = 1, nyp1         sum1 = sum1 + edg(k,l)  140    continue         eg(1,l) = sum1      endif  150 continuec find new partitions      nter = 0      do 180 l = 1, nblok      kr = l + ks + 2      kl = l + ks      apav = real(kl)*anpav      anpl = apav - et2(1,l) + es(1,l)      anpr = et2(1,l) - apav - anpavc left boundary is on the right      if (et2(1,l).lt.apav) then         if ((et2(1,l)+eg(1,l)).ge.apav) then            nyp1 = eg(2,l)            k1 = 1            sum1 = 0.            at2 = - (anpr + anpav)  160       at1 = sum1            sum1 = sum1 + edg(k1,l)            k1 = k1 + 1            if ((sum1.lt.at2).and.(k1.le.nyp1)) go to 160            at1 = real(k1 - 2) + (at2 - at1)/(sum1 - at1)            edges(1,l) = et2(2,l) + at1c left boundary is even further to right         else            nter = nter + 1         endif      endifc left going data will need to be sent      if ((anpl-es(1,l)).gt.anpav) nter = nter + 1      if (kr.le.nvp) then         nyp1 = eg(2,l)         do 170 k = 1, nyp1         eds(k,l) = edg(k,l)  170    continue         et2(1,l) = et2(1,l) + eg(1,l)         et2(2,l) = et2(2,l) + eg(2,l)         es(1,l) = eg(1,l)         es(2,l) = eg(2,l)      endif  180 continuec get more data from right      if (nter.gt.0) go to 130c send left edge to processor on right      iter = 2      do 190 l = 1, nblok      kr = l + ks + 2      kl = l + ksc this segment is used for shared memory computersc     if (kr.le.nvp) thenc        edges(2,l) = edges(1,kr)c     elsec        edges(2,l) = et2(4,l)c     endifc this segment is used for mpi computersc post receive from right      if (kr.le.nvp) then         call MPI_IRECV(edges(2,l),1,mreal,kr-1,iter,lgrp,msid,ierr)      endifc send left edge to left      if (kl.gt.0) then         call MPI_SEND(edges(1,l),1,mreal,kl-1,iter,lgrp,ierr)      endifc wait for edge to arrive      if (kr.le.nvp) then         call MPI_WAIT(msid,istatus,ierr)      else         edges(2,l) = et2(4,l)      endif  190 continuec calculate number of grids and offsets in new partitions      do 200 l = 1, nblok      kl = edges(1,l) + .5      noff(l) = kl      kr = edges(2,l) + .5      nyp(l) = kr - kl      edges(1,l) = real(kl)      edges(2,l) = real(kr)  200 continuec find minimum and maximum partition size      nypmin = nyp(1)      nypmax = nyp(1)      do 210 l = 1, nblok      nypmin = min0(nypmin,nyp(l))      nypmax = max0(nypmax,nyp(l))  210 continue      ibflg(1) = -nypmin      ibflg(2) = nypmax      call PIMAX(ibflg,iwork,2,1)      nypmin = -ibflg(1)      nypmax = ibflg(2)      return      endc-----------------------------------------------------------------------      subroutine PSCAN(f,g,s,nxp,nblok)c this subroutine performs a parallel prefix reduction of a vector,c that is: f(j,k) = sum over k of f(j,k), where the sum is over k valuesc less than idproc.c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g, s = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g, s      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok), s(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus, ierr, msid      integer idproc, kstrt, ks, l, kxs, k, kb, lb, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c initialize global sum      do 20 k = 1, nblok      do 10 j = 1, nxp      s(j,k) = f(j,k)   10 continue   20 continuec main iteration loop   30 if (kxs.ge.nproc) go to 90c shift data      do 60 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 40 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = s(j,kb+kxs)c     elsec        g(j,k) = s(j,kb-kxs)c     endifc  40 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(s,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(s,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)c perform prefix scan      if (lb.ne.0) then         do 50 j = 1, nxp         f(j,k) = f(j,k) + g(j,k)   50    continue      endif   60 continuec perform sum      do 80 k = 1, nblok      do 70 j = 1, nxp      s(j,k) = s(j,k) + g(j,k)   70 continue   80 continue      l = l + 1      kxs = kxs + kxs      go to 30   90 return      endc-----------------------------------------------------------------------      subroutine PFDISTR2(part,nps,fnx,argx1,argx2,argx3,fny,argy1,argy2     1,argy3,npx,npy,nx,ny,idimp,npmax,nblok,kstrt,nvp,ipbc,ierr)c for 2d code, this subroutine calculates initial particle co-ordinatesc with general density profile n(x,y) = n(x)*n(y), c where density in x is given by n(x) = fnx(x,argx1,argx2,argx3,0)c and integral of the density is given by = fnx(x,argx1,argx2,argx3,1)c and where density in y is given by n(y) = fny(y,argy1,argy2,argy3,0)c and integral of the density is given by = fny(y,argy1,argy2,argy3,1)c for distributed data.c particles are not necessarily in the correct processor.c part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc nps(l) = starting address of particles in partition lc fnx/fny = density and density integral function in x/y directionc argx1,argx2,argx3 = arguments to fnxc argy1,argy2,argy3 = arguments to fnyc npx/npy = initial number of particles distributed in x/y directionc nx/ny = system length in x/y directionc idimp = size of phase space = 4 or 5c npmax = maximum number of particles in each partitionc nblok = number of particle partitionsc kstrt = starting data block numberc nvp = number of real or virtual processorsc ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)c ierr = (0,1) = (no,yes) error condition existsc with spatial decomposition      implicit none      integer npx, npy, nx, ny, idimp, npmax, nblok, kstrt, nvp, ipbc      integer ierr, nps      real argx1, argx2, argx3, argy1, argy2, argy3      real part      dimension part(idimp,npmax,nblok)      dimension nps(nblok)      real fnx, fny      external fnx, fnyc local data      integer ks, nppv, npxy, kc, jc, i, j, k, l, n, nn, koff, noff      integer imax, iwork      real edgelx, edgely, anx, any, bnx, bny, xt, yt, xt0, yt0, x0, y0      real xn, yn, eps, big, f, fp      ierr = 0c particle distribution constants      ks = kstrt - 2      nppv = min((npx*npy)/nvp,npmax)      npxy = nppv*nvpc eps = convergence criterion      imax = 20      eps = 0.0001      big = 0.5c check for errors      if (npxy.ne.(npx*npy)) ierr = 1      call PISUM(ierr,iwork,1,1)      if (ierr.gt.0) then         ierr = npxy - npx*npy         write (2,*) 'particle distribution truncated, np = ', npxy         return      endifc set boundary values      edgelx = 0.      edgely = 0.      if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.      endifc find normalization for function      anx = float(nx) - edgelx      any = float(ny) - edgely      x0 = fnx(edgelx,argx1,argx2,argx3,1)      y0 = fny(edgely,argy1,argy2,argy3,1)      bnx = float(npx)/(fnx(anx,argx1,argx2,argx3,1) - x0)      bny = float(npy)/(fny(any,argy1,argy2,argy3,1) - y0)      x0 = bnx*x0 - .5      y0 = bny*y0 - .5c uniform density profile      do 90 l = 1, nblok      koff = nppv*(l + ks)      noff = nps(l) - 1c integrate to find starting point in y      kc = koff/npx      yt0 = edgely      yt = yt0 + 0.5/(bny*fny(yt0,argy1,argy2,argy3,0))      do 20 k = 1, kc      yn = float(k) + y0c guess next value for yt      if (k.gt.1) yt = yt + 1.0/(bny*fny(yt,argy1,argy2,argy3,0))      yt = max(edgely,min(yt,any))      i = 0   10 f = bny*fny(yt,argy1,argy2,argy3,1) - ync find improved value for yt      if (abs(f).ge.eps) thenc newton's method         if (abs(f).lt.big) then            fp = bny*fny(yt,argy1,argy2,argy3,0)            yt0 = yt            yt = yt - f/fp            yt = max(edgely,min(yt,any))c bisection method         else if (f.gt.0.) then            fp = .5*(yt - yt0)            yt = yt0 + fp         else            fp = yt - yt0c           yt0 = yt            yt = yt + fp         endif         i = i + 1         if (i.lt.imax) go to 10         write (2,*) 'newton iteration max exceeded, yt = ', yt         ierr = ierr + 1      endif      yt0 = yt   20 continuec quit if error      if (ierr.ne.0) returnc integrate to find starting point in x      jc = koff - npx*kc      xt0 = edgelx      xt = xt0 + 0.5/(bnx*fnx(xt0,argx1,argx2,argx3,0))      do 40 j = 1, jc      xn = float(j) + x0c guess next value for xt      if (j.gt.1) xt = xt + 1.0/(bnx*fnx(xt,argx1,argx2,argx3,0))      xt = max(edgelx,min(xt,anx))      i = 0   30 f = bnx*fnx(xt,argx1,argx2,argx3,1) - xnc find improved value for xt      if (abs(f).ge.eps) thenc newton's method         if (abs(f).lt.big) then            fp = bnx*fnx(xt,argx1,argx2,argx3,0)            xt0 = xt            xt = xt - f/fp            xt = max(edgelx,min(xt,anx))c bisection method         else if (f.gt.0.) then            fp = .5*(xt - xt0)            xt = xt0 + fp         else            fp = xt - xt0c           xt0 = xt            xt = xt + fp         endif         i = i + 1         if (i.lt.imax) go to 30         write (2,*) 'newton iteration max exceeded, xt = ', xt         ierr = ierr + 1      endif      xt0 = xt   40 continuec quit if error      if (ierr.ne.0) returnc density profile in x      kc = kc + 1      do 60 n = 1, min(npx,nppv)c j, k = used to determine spatial location of this particle      nn = n + koff      k = (nn - 1)/npx + 1      j = nn - npx*(k - 1)      xn = float(j) + x0c guess next value for xt      if (j.eq.1) then         xt0 = edgelx         xt = xt0 + 0.5/(bnx*fnx(xt0,argx1,argx2,argx3,0))      else         xt = xt + 1.0/(bnx*fnx(xt,argx1,argx2,argx3,0))      endif      xt = max(edgelx,min(xt,anx))      i = 0   50 f = bnx*fnx(xt,argx1,argx2,argx3,1) - xnc find improved value for xt      if (abs(f).ge.eps) thenc newton's method         if (abs(f).lt.big) then            fp = bnx*fnx(xt,argx1,argx2,argx3,0)            xt0 = xt            xt = xt - f/fp            xt = max(edgelx,min(xt,anx))c bisection method         else if (f.gt.0.) then            fp = .5*(xt - xt0)            xt = xt0 + fp         else            fp = xt - xt0c           xt0 = xt            xt = xt + fp         endif         i = i + 1         if (i.lt.imax) go to 50         write (2,*) 'newton iteration max exceeded, xt = ', xt         ierr = ierr + 1      endif      part(1,n+noff,l) = xt      xt0 = xt   60 continuec quit if error      if (ierr.ne.0) returnc density profile in y      do 80 n = 1, nppvc j, k = used to determine spatial location of this particle      nn = n + koff      k = (nn - 1)/npx + 1      j = nn - npx*(k - 1)      nn = n - ((n - 1)/npx)*npx      if (k.eq.kc) then         yn = float(k) + y0c guess next value for yt         if (k.gt.1) yt = yt + 1.0/(bny*fny(yt,argy1,argy2,argy3,0))         yt = max(edgely,min(yt,any))         i = 0   70    f = bny*fny(yt,argy1,argy2,argy3,1) - ync find improved value for yt         if (abs(f).ge.eps) thenc newton's method            if (abs(f).lt.big) then               fp = bny*fny(yt,argy1,argy2,argy3,0)               yt0 = yt               yt = yt - f/fp               yt = max(edgely,min(yt,any))c bisection method            else if (f.gt.0.) then               fp = .5*(yt - yt0)               yt = yt0 + fp            else               fp = yt - yt0c              yt0 = yt               yt = yt + fp            endif            i = i + 1            if (i.lt.imax) go to 70            write (2,*) 'newton iteration max exceeded, yt = ', yt            ierr = ierr + 1         endif         kc = kc + 1         yt0 = yt      endifc store co-ordinates      part(1,n+noff,l) = part(1,nn+noff,l)      part(2,n+noff,l) = yt   80 continue   90 continue      return      endc-----------------------------------------------------------------------      subroutine PVDISTR2H(part,npp,nps,vtx,vty,vtz,vdx,vdy,vdz,npx,npy,     1idimp,npmax,nblok,kstrt,nvp,ierr)c for 2-1/2d code, this subroutine calculates initial particlec velocities with maxwellian velocity with drift for distributed data.c part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc part(5,n,l) = velocity vz of particle n in partition lc npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition lc vtx/vty/vtz = thermal velocity of electrons in x/y/z directionc vdx/vdy/vdz = drift velocity of beam electrons in x/y/z directionc npx/npy = initial number of particles distributed in x/y directionc idimp = size of phase space = 5c npmax = maximum number of particles in each partitionc nblok = number of particle partitionsc kstrt = starting data block numberc nvp = number of real or virtual processorsc ierr = (0,1) = (no,yes) error condition existsc ranorm = gaussian random number with zero mean and unit variancec with spatial decomposition      implicit none      integer npx, npy, idimp, npmax, nblok, kstrt, nvp, ierr      real vtx, vty, vtz, vdx, vdy, vdz      integer npp, nps      real part      dimension part(idimp,npmax,nblok)      dimension npp(nblok), nps(nblok)c local data      integer ks, nppv, npxy, i, j, k, l, joff, imin, npt, iwork      real vxt, vyt, vzt, at1      double precision ranorm      double precision dsum1, dsum2, dsum3      real sum3, work3      dimension sum3(3), work3(3)      ierr = 0c particle distribution constants      ks = kstrt - 2      nppv = min((npx*npy)/nvp,npmax)      npxy = nppv*nvpc maxwellian velocity distribution      do 30 k = 1, npy      joff = npx*(k - 1)      do 20 j = 1, npx      i = j + joffc maxwellian velocity distribution      vxt = vtx*ranorm()      vyt = vty*ranorm()      vzt = vtz*ranorm()      do 10 l = 1, nblok      imin = nppv*(l + ks) + 1      if ((i.ge.imin).and.(i.lt.(imin+nppv))) then         npt = npp(l) + 1         part(3,npt,l) = vxt         part(4,npt,l) = vyt         part(5,npt,l) = vzt         npp(l) = npt      endif   10 continue   20 continue   30 continue      npxy = 0c add correct drift      sum3(1) = 0.      sum3(2) = 0.      sum3(3) = 0.      do 50 l = 1, nblok      dsum1 = 0.0d0      dsum2 = 0.0d0      dsum3 = 0.0d0      do 40 j = nps(l), npp(l)      npxy = npxy + 1      dsum1 = dsum1 + part(3,j,l)      dsum2 = dsum2 + part(4,j,l)      dsum3 = dsum3 + part(5,j,l)   40 continue      sum3(1) = sum3(1) + dsum1      sum3(2) = sum3(2) + dsum2      sum3(3) = sum3(3) + dsum3   50 continue      call PISUM(npxy,iwork,1,1)      call PSUM(sum3,work3,3,1)      at1 = 1./float(npxy)      sum3(1) = at1*sum3(1) - vdx      sum3(2) = at1*sum3(2) - vdy      sum3(3) = at1*sum3(3) - vdz      do 70 l = 1, nblok      do 60 j = nps(l), npp(l)      part(3,j,l) = part(3,j,l) - sum3(1)      part(4,j,l) = part(4,j,l) - sum3(2)      part(5,j,l) = part(5,j,l) - sum3(3)   60 continue   70 continuec process errors      if (npxy.ne.(npx*npy)) then         ierr = npxy - npx*npy         write (2,*) 'velocity distribution truncated, np = ', npxy      endif      return      endc-----------------------------------------------------------------------      function FGDISTR1(x,ang,wi,x0,intg)c this function calculates either a density function or its integralc for a gaussian density profile.  Used in initializing particlec coordinates.c if intg = 0, n(x) = 1.0 + ang*exp(-((x-x0)*wi)**2/2.)c if intg = 1, n(x) = x + (ang*sqrt(pi/2)/wi)*c                         (erf((x-x0)*wi/sqrt(2)) + erf(x0*wi/sqrt(2)))      implicit none      integer intg      real x, ang, x0, wic local data      real FGDISTR1, f, sqrt2i, sqtpih, aw, t, erfn      external erfn      data sqrt2i, sqtpih /0.7071067811865476,1.253314137397325/      save sqrt2i, sqtpih      aw = wi*sqrt2i      t = (x - x0)*aw      if (intg.eq.0) then         if (abs(t).lt.8.) then            f = 1.0 + ang*exp(-t**2)         else            f = 1.0         endif      else if (intg.eq.1) then         if (wi.eq.0.) then            f = (1.0 + ang)*x         else            f = x + (ang*sqtpih/wi)*(erfn(t) + erfn(x0*aw))         endif      else         f = -1.0      endif      if (f.lt.0.) write (2,*) 'FGDISTR1 Error: f = ', f      FGDISTR1 = f      return      endc-----------------------------------------------------------------------      function erfn(x)c this function calculates the real error function, according to thec formulae given in Abramowitz and Stegun, Handbook of Mathematicalc Functions, p. 299.  Error is < 1.5 x 10-7.      implicit none      real xc local data      real erfn, p, a1, a2, a3, a4, a5, t, f      data p, a1, a2 /0.3275911,0.254829592,-0.284496736/      data a3, a4, a5 /1.421413741,-1.453152027,1.061405429/      save p, a1, a2, a3, a4, a5      f = abs(x)      t = 1.0/(1.0 + p*f)      if (f.le.8.) then         erfn = 1.0 - t*(a1 + t*(a2 + t*(a3 + t*(a4 + t*a5))))*exp(-x*x)      else         erfn = 1.0      endif      if (x.lt.0.) erfn = -erfn      return      endc-----------------------------------------------------------------------      subroutine FEDGES2(edges,noff,nyp,fny,arg1,arg2,arg3,ny,nypmin,nyp     1max,kstrt,nvp,nblok,idps,ipbc)c this subroutines finds new partitions boundaries (edges,noff,nyp)c from density integral given by = fny(y,arg1,arg2,arg3,1)c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc noff(l) = lowermost global gridpoint in particle partition lc nyp(l) = number of primary gridpoints in particle partition lc fny = density and density integral functionc arg1,arg2,arg3 = arguments to fnyc nypmin/nypmax = minimum/maximum value of nyp in new partitionc kstrt = starting data block numberc nvp = number of real or virtual processorsc nblok = number of field partitions.c idps = number of partition boundariesc ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)      implicit none      real edges      real arg1, arg2, arg3      integer noff, nyp      integer ny, nypmin, nypmax, kstrt, nvp, nblok, idps, ipbc      dimension edges(idps,nblok)      dimension noff(nblok), nyp(nblok)      real fny      external fnyc local data      integer ks, kl, kr, l      real edgely, any1, any, y0, y1, anpav, anpl, anpr, sum1, at1, at2      integer ibflg, iwork      dimension ibflg(2), iwork(2)c particle distribution constants      ks = kstrt - 2c set boundary values      edgely = 0.      if (ipbc.eq.2) then         edgely = 1.      else if (ipbc.eq.3) then         edgely = 0.      endifc find normalization for function      any = real(ny)      any1 = any - edgely      y0 = fny(edgely,arg1,arg2,arg3,1)c anpav = desired number of particles per processor      anpav = (fny(any1,arg1,arg2,arg3,1) - y0)/real(nvp)c search for boundaries      do 30 l = 1, nblok      kl = l + ks      anpl = real(kl)*anpav      anpr = real(kl+1)*anpav      y1 = edgely      sum1 = 0.c first find left boundary   10 at1 = sum1      sum1 = fny(y1,arg1,arg2,arg3,1) - y0      y1 = y1 + 1.0      if ((sum1.lt.anpl).and.(y1.le.any)) go to 10       if (sum1.gt.at1) then         at2 = (y1 - 2.0) + (anpl - at1)/(sum1 - at1)      else         at2 = y1 - 1.0      endif      edges(1,l) = at2c then find right boundary   20 at1 = sum1      sum1 = fny(y1,arg1,arg2,arg3,1) - y0      y1 = y1 + 1.0      if ((sum1.lt.anpr).and.(y1.le.any)) go to 20      at2 = (y1 - 2.0) + (anpr - at1)/(sum1 - at1)      edges(2,l) = at2   30 continuec calculate number of grids and offsets in new partitions      do 40 l = 1, nblok      kl = edges(1,l) + .5      noff(l) = kl      kr = edges(2,l) + .5      nyp(l) = kr - kl      edges(1,l) = real(kl)      edges(2,l) = real(kr)   40 continuec find minimum and maximum partition size      nypmin = nyp(1)      nypmax = nyp(1)      do 50 l = 1, nblok      nypmin = min0(nypmin,nyp(l))      nypmax = max0(nypmax,nyp(l))   50 continue      ibflg(1) = -nypmin      ibflg(2) = nypmax      call PIMAX(ibflg,iwork,2,1)      nypmin = -ibflg(1)      nypmax = ibflg(2)      return      end