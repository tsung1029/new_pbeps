c * * * periodic 2d electrostatic particle simulation kernel code * * *c this is a simple 2d skeleton particle-in-cell code designed forc exploring new computer architectures.  it contains the critical piecesc needed for depositing charge, advancing particles, and solving thec field.  the code moves only electrons, with periodic electrostaticc forces obtained by solving poisson's equation with fast fourierc transforms.  the only diagnostic is particle and field energy.c portable gcpic kernel code, using algorithm described in:c p. c. liewer and v. k. decyk, j. computational phys. 85, 302 (1989).c written by viktor k. decyk, uclac for mpi distributed memory computersc update: october 25, 2004      program beps2kc indx/indy = exponent which determines length in x/y direction,c where nx=2**indx, ny=2**indyc npx/npy = initial number of particles distributed in x/y directionc     parameter( indx =   5, indy =   6, npx =      96, npy =     192)      parameter( indx =   6, indy =   7, npx =     384, npy =     768)c     parameter( indx =   7, indy =   8, npx =    1280, npy =    2560)c npxb/npyb = initial number of particles in beam in x/y directionc     parameter( npxb =  32, npyb =  64)      parameter( npxb = 128, npyb = 256)c     parameter( npxb = 384, npyb = 768)c tend = time at end of simulation, in units of plasma frequencyc dt = time interval between successive calculations      parameter( tend =  65.000, dt = 0.2000000e+00)c vty = thermal velocity of electrons in y directionc vdy = drift velocity of beam electrons y directionc vtdy = thermal velocity of beam electrons in y direction      parameter( vty =   1.000, vdy =   5.000, vtdy =   0.500)c avdy = absolute value of drift velocity of beam electrons y direction      parameter( avdy =   5.000)c indnvp = exponent determining number of real or virtual processorsc indnvp must be <= indyc idps = number of partition boundariesc idimp = dimension of phase space = 4c mshare = (0,1) = (no,yes) architecture is shared memory      parameter( indnvp =   2, idps =    2, idimp =   4, mshare =   0)c np = total number of electrons in simulation      parameter(npxy=npx*npy,npxyb=npxb*npyb,np=npxy+npxyb)      parameter(nx=2**indx,ny=2**indy,nxh=nx/2,nyh=ny/2)      parameter(nxv=nx+2,nyv=ny+2,nxvh=nxv/2)      parameter(nxe=nx+2,nxeh=nxe/2)c nloop = number of time steps in simulation      parameter(nloop=tend/dt+.0001)c nvp = number of real or virtual processors, nvp = 2**indnvpc nblok = number of particle partitions      parameter(nvp=2**indnvp,nblok=1+mshare*(nvp-1))c npmax = maximum number of particles in each partitionc nypmx = maximum size of particle partition, including guard cells.      parameter(npmax=(np/nvp)*1.01+7000,nypmx=(ny-1)/nvp+2)      parameter(nxvyp=nxv*nypmx,nxeyp=nxe*nypmx)c ndvr = number of independent seeds in parallel random number generatorc nvrp = number of parallel seeds per processor      parameter(ndvr=256,nvrp=(ndvr-1)/nvp+1)c kyp = number of complex grids in each field partition in y directionc kxp = number of complex grids in each field partition in x direction      parameter(kyp=(ny-1)/nvp+1,kxp=(nxh-1)/nvp+1)c kyb = number of processors in yc kxb = number of processors in x      parameter(kyb=ny/kyp,kxb=nxh/kxp,jkmx=kxb*(kyb/kxb)+kyb*(kxb/kyb))c kxyb = maximum(kxb,kyb)      parameter(kxyb=jkmx/(2-kxb/jkmx-kyb/jkmx))c kblok = number of field partitions in y direction      parameter(kbmin=1+(1-mshare)*(kxyb/kxb-1))      parameter(kblok=1+mshare*(ny/kyp-1))c jblok = number of field partitions in x direction      parameter(jbmin=1+(1-mshare)*(kxyb/kyb-1))      parameter(jblok=1+mshare*(nxh/kxp-1))c nxyh = maximum(nx,ny)/2c nxhy = maximum(nx/2,ny)      parameter(nmx=nx*(ny/nx)+ny*(nx/ny),nxy=nmx/(2-nx/nmx-ny/nmx))      parameter(nxyh=nxy/2,nmxh=nxh*(ny/nxh)+ny*(nxh/ny))      parameter(nxhy=nmxh/(2-nxh/nmxh-ny/nmxh))c nbmax = size of buffer for passing particles between processors      parameter(nbmax=1+(2*(npxy*vty+npxyb*vtdy)+1.4*npxyb*avdy)*dt/ny)c ntmax = size of hole array for particles leaving processors      parameter(ntmax=2*nbmax)c npd = size of scratch buffers for vectorized charge deposition      parameter(npd=128,ifour=4)c dimensions for sorting arrays      parameter(nypm1=(ny-1)/nvp+2)c ipbc = particle boundary condition is 2d periodic      parameter(ipbc=1)      double precision vranx, vrany      complex qt, fxyt, ffc, bs, br, sct      common /large/ partc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition l      dimension part(idimp,npmax,nblok)c maskp = scratch array for particle addressesc     dimension maskp(npmax,nblok)c vranx, vrany = output arrays for parallel random numbers      dimension vranx(nvrp,nblok), vrany(nvrp,nblok)c in real space, qe(j1,k,l) = charge density at grid point (j,kk)c where kk = k + noff(l) - 1c in real space, fxye(i,j,k,l) = i component of force/charge at c grid point (j,kk)c in other words, fxye are the convolutions of the electric fieldc over the particle shape, where kk = k + noff(l) - 1      dimension qe(nxe,nypmx*kbmin,kblok)      dimension fxye(2,nxe,nypmx*kbmin,kblok)c qt(k,j,l) = complex charge density for fourier mode jj-1,k-1c fxt(k,j,l) = x component of force/charge for fourier mode jj-1,k-1c fyt(k,j,l) = y component of force/charge for fourier mode jj-1,k-1c where jj = j + kxp*(l - 1)      dimension qt(nyv,kxp,jblok)      dimension fxyt(2,nyv,kxp,jblok)c ffc = form factor array for poisson solver      dimension ffc(nyh,kxp,jblok)c mixup, sct = arrays for fft      dimension mixup(nxhy), sct(nxyh)c bs, br = complex buffer arrays for transposec     dimension bs(2,kxp,kyp,kblok), br(2,kxp,kyp,jblok)c msid, mrid = scratch arrays for identifying asynchronous messagesc     dimension msid(kxb), mrid(kyb)c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition l      dimension edges(idps,nblok)c nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.      dimension nyp(nblok), noff(nblok)c npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition l      dimension npp(nblok), nps(nblok)c sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processor      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)c rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processor      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)c ihole = location of holes left in particle arrays      dimension ihole(ntmax,nblok)c jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition l      dimension jsl(idps,nblok), jsr(idps,nblok)c jss(idps,l) = scratch array for particle partition lc scr(j,l) = complex scratch array for particle partition l      dimension jss(idps,nblok), scr(nxe,nblok)c nn = scratch address array for vectorized charge depositionc amxy = scratch weight array for vectorized charge depositionc     dimension nn(npd,ifour,nblok), amxy(npd,ifour,nblok)c wtot = total energy      dimension wtot(3), work(3)c sorting arrays      dimension pt(npmax,nblok), ip(npmax,nblok), npic(nypm1,nblok)      character*12 label  991 format (5h t = ,i7)  992 format (19h * * * q.e.d. * * *)  993 format (34h field, kinetic, total energies = ,3e14.7)c vtx = thermal velocity of electrons in x directionc qme = charge on electron, in units of ec vdx = drift velocity of beam electrons in x directionc vtdx = thermal velocity of beam electrons in x direction      data vtx,qme,vdx,vtdx /1.,-1.,0.,.5/c ax/ay = half-width of particle in x/y direction      data ax,ay /.912871,.912871/c ntpose = (0,1) = (no,yes) input, output data are transposed in pfft2r      data ntpose /1/c initialize for parallel processing      call ppinit(idproc,nvp)      kstrt = idproc + 1      if (kstrt.eq.1) then         open(unit=6,file='output2',form='formatted',status='unknown')      endifc initialize timer      call timera(-1,'init    ',time)c initialize constants      itime = 0      qbme = qme      qtme = qme*dt      affp = float(nx*ny)/float(np)      ci = 0.1      zero = 0.c calculate partition variables      call dcomp2l(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c prepare fft tables      isign = 0c     call PFFT2R(qe,qt,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrt,nxec    1h,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX(qe,qt,isign,ntpose,mixup,sct,indx,indy,kstrt,nxeh,nyv     1,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c calculate form factors      call PPOIS22(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,j     1blok,nyh)c initialize density profile and velocity distributionc background electrons      do 130 l = 1, nblok      nps(l) = 1      npp(l) = 0  130 continue      if (npxy.gt.0) call PISTR2 (part,edges,npp,nps,vtx,vty,zero,zero,n     1px,npy,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)c     if (npxy.gt.0) call PVISTR2 (part,npp,nps,vtx,vty,zero,zero,npx,npc    1y,nx,ny,idimp,npmax,nblok,ipbc,vranx,vrany,kstrt,nvp,nvrp,ierr)c beam electrons      do 140 l = 1, nblok      nps(l) = npp(l) + 1  140 continue      if (npxyb.gt.0) call PISTR2 (part,edges,npp,nps,vtdx,vtdy,vdx,vdy,     1npxb,npyb,nx,ny,idimp,npmax,nblok,idps,ipbc,ierr)c     if (npxyb.gt.0) call PVISTR2 (part,npp,nps,vtdx,vtdy,vdx,vdy,npxb,c    1npyb,nx,ny,idimp,npmax,nblok,ipbc,vranx,vrany,kstrt,nvp,nvrp,ierr)c initialize charge density to background      qi0 = -qme/affp      call PSGUARD2L(qe,nyp,qi0,nx,nxe,nypmx,nblok)c deposit charge for initial distributionc     call PDOST2L(part,q,npp,noff,qme,nx,idimp,npmax,nblok,nxv,nypmx)      call PGPOST2L(part,qe,npp,noff,qme,idimp,npmax,nblok,nxe,nypmx)c add guard cells      call PAGUARD2XL(qe,nyp,nx,nxe,nypmx,nblok)      call timera(1,'init    ',time)      call timera(-1,'main    ',time)cc * * * start main iteration loop * * *c  500 if (nloop.le.itime) go to 2000      if (kstrt.eq.1) write (6,991) itime      write (label,991) itime      call LOGNAME(label)c copy data from particle to field partition, and add up guard cells      call paguard2l(qe,scr,kstrt,nvp,nx,nxe,nypmx,nblok,kyp,kblok)c transform charge to fourier space      isign = -1c     call PFFT2R(qe,qt,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrt,nxec    1h,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX(qe,qt,isign,ntpose,mixup,sct,indx,indy,kstrt,nxeh,nyv     1,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c calculate force/charge in fourier space      isign = -1      call PPOIS22(qt,fxyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp,j     1blok,nyh)c transform force/charge to real space      isign = 1c     call PFFT2R2(fxye,fxyt,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrc    1t,nxeh,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)      call PFFT2RX2(fxye,fxyt,isign,ntpose,mixup,sct,indx,indy,kstrt,nxe     1h,nyv,kxp,kyp,nypmx,jblok,kblok,nxhy,nxyh)c copy data from field to particle partition, and copy to guard cells      call pcguard2l(fxye,kstrt,nvp,2*nxe,nypmx,nblok,kyp,kblok)      call PCGUARD2XL(fxye,nyp,nx,nxe,nypmx,nblok)c particle push and charge density updatec     call timera(-1,'push    ',time)      wke = 0.c push particlesc     call PPUSH2L(part,fx,fy,npp,noff,qbme,dt,wke,nx,idimp,npmax,nblok,c    1nxv,nypmx)      call PGPUSH2L(part,fxye,npp,noff,qbme,dt,wke,nx,ny,idimp,npmax,nbl     1ok,nxe,nypmx,ipbc)c     call PGSPUSH2L(part,fxye,npp,noff,qbme,dt,wke,nx,ny,idimp,npmax,nbc    1lok,nxe,nxeyp,ipbc)c     call PGRPUSH2L(part,fxye,npp,noff,qbme,dt,ci,wke,nx,ny,idimp,npmaxc    1,nblok,nxe,nypmx,ipbc)c     call PGSRPUSH2L(part,fxye,npp,noff,qbme,dt,ci,wke,nx,ny,idimp,npmac    1x,nblok,nxe,nxeyp,ipbc)c move particles into appropriate spatial regions      call pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,j     1ss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c     call pxmov2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,c    1jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc initialize charge density to background      call PSGUARD2L(qe,nyp,qi0,nx,nxe,nypmx,nblok)c deposit chargec     call PDOST2L(part,q,npp,noff,qme,nx,idimp,npmax,nblok,nxv,nypmx)c     call PGPOST2L(part,qe,npp,noff,qme,idimp,npmax,nblok,nxe,nypmx)      call PGSPOST2L(part,qe,npp,noff,qme,idimp,npmax,nblok,nxe,nxeyp)c     call PSOST2XL(part,q,npp,noff,nn,amxy,qme,nx,idimp,npmax,nblok,nxvc    1,nxvyp,npd,ifour)c     call PGSOST2XL(part,qe,npp,noff,nn,amxy,qme,idimp,npmax,nblok,nxe,c    1nxeyp,npd,ifour)c merge density arrays      call PAGUARD2XL(qe,nyp,nx,nxe,nypmx,nblok)c     call timera(1,'push    ',time)c sort particles      if (mod(itime,25).eq.0) then         call PSORTP2YL(part,pt,ip,npic,npp,noff,nyp,idimp,npmax,nblok,n     1ypm1)      endif c energy diagnostic      wtot(1) = we      wtot(2) = wke      wtot(3) = we + wke      call PSUM(wtot,work,3,1)      if (kstrt.eq.1) write (6,993) wtot(1), wtot(2), wtot(3)      itime = itime + 1      go to 500 2000 continuecc * * * end main iteration loop * * *c      if (kstrt.eq.1) write (6,992)      call timera(1,'main    ',time)      call ppexit      stop      endc-----------------------------------------------------------------------      subroutine ppinit(idproc,nvp)c this subroutine initializes parallel processingc input: nvp, output: idprocc idproc = processor idc nvp = number of real or virtual processors requested      implicit none      integer idproc, nvpc get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for realsc mint = default datatype for integersc mcplx = default datatype for complex type      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ierror, ndprec      save /pparms/c ndprec = (0,1) = (no,yes) use (normal,autodouble) precision      data ndprec /1/c this segment is used for shared memory computersc     nproc = nvpc     idproc = 0c this segment is used for mpi computers      if (MPI_STATUS_SIZE.gt.lstat) then         write (2,*) ' status size too small, actual/required = ', lstat     1, MPI_STATUS_SIZE         stop      endifc initialize the MPI execution environment      call MPI_INIT(ierror)      if (ierror.ne.0) stop      lgrp = MPI_COMM_WORLDc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lgrp,idproc,ierror)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lgrp,nproc,ierror)c set default datatypes         mint = MPI_INTEGERc single precision      if (ndprec.eq.0) then         mreal = MPI_REAL         mcplx = MPI_COMPLEXc double precision      else         mreal = MPI_DOUBLE_PRECISION         mcplx = MPI_DOUBLE_COMPLEX      endifc requested number of processors not obtained      if (nproc.ne.nvp) then         write (2,*) ' processor number error: nvp, nproc=', nvp, nproc         call ppexit         stop      endif      return      endc-----------------------------------------------------------------------      subroutine ppexitc this subroutine terminates parallel processing      implicit nonec common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc lgrp = current communicator      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworld      integer ierrorc synchronize processes      call MPI_BARRIER(lgrp,ierror)c terminate MPI execution environment      call MPI_FINALIZE(ierror)      return      endc-----------------------------------------------------------------------      subroutine dcomp2l(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c this subroutine determines spatial boundaries for particlec decomposition, calculates number of grid points in each spatialc region, and the offset of these grid points from the global addressc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.c ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idps = number of partition boundariesc nblok = number of particle partitions.      dimension edges(idps,nblok)      dimension nyp(nblok), noff(nblok)      ks = kstrt - 2      at1 = float(ny)/float(nvp)      do 10 l = 1, nblok      kb = l + ks      edges(1,l) = at1*float(kb)      noff(l) = edges(1,l)      edges(2,l) = at1*float(kb + 1)      kr = edges(2,l)      nyp(l) = kr - noff(l)   10 continue      return      endc-----------------------------------------------------------------------      subroutine PISTR2(part,edges,npp,nps,vtx,vty,vdx,vdy,npx,npy,nx,ny     1,idimp,npmax,nblok,idps,ipbc,ierr)c for 2d code, this subroutine calculates initial particle co-ordinatesc and velocities with uniform density and maxwellian velocity with driftc for distributed data.c part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition lc vtx/vty = thermal velocity of electrons in x/y directionc vdx/vdy = drift velocity of beam electrons in x/y directionc npx/npy = initial number of particles distributed in x/y directionc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)c ierr = (0,1) = (no,yes) error condition existsc ranorm = gaussian random number with zero mean and unit variancec with spatial decomposition      double precision ranorm      double precision sum0, sum1      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok), nps(nblok)      dimension sum2(2), isum2(2)      dimension work2(2), iwork2(2)      ierr = 0c set boundary values      if (ipbc.eq.1) then         edgelx = 0.         edgely = 0.         at1 = float(nx)/float(npx)         at2 = float(ny)/float(npy)      else if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         at1 = float(nx-2)/float(npx)         at2 = float(ny-2)/float(npy)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         at1 = float(nx-2)/float(npx)         at2 = float(ny)/float(npy)      endif      do 30 k = 1, npy      yt = edgely + at2*(float(k) - .5)      do 20 j = 1, npxc uniform density profile      xt = edgelx + at1*(float(j) - .5)c maxwellian velocity distribution      vxt = vtx*ranorm()      vyt = vty*ranorm()      do 10 l = 1, nblok      if ((yt.ge.edges(1,l)).and.(yt.lt.edges(2,l))) then         npt = npp(l) + 1         if (npt.le.npmax) then            part(1,npt,l) = xt            part(2,npt,l) = yt            part(3,npt,l) = vxt            part(4,npt,l) = vyt            npp(l) = npt         else            ierr = ierr + 1         endif      endif   10 continue   20 continue   30 continue      npxy = 0c add correct drift      sum2(1) = 0.      sum2(2) = 0.      do 50 l = 1, nblok      sum0 = 0.0d0      sum1 = 0.0d0      do 40 j = nps(l), npp(l)      npxy = npxy + 1      sum0 = sum0 + part(3,j,l)      sum1 = sum1 + part(4,j,l)   40 continue      sum2(1) = sum2(1) + sum0      sum2(2) = sum2(2) + sum1   50 continue      isum2(1) = ierr      isum2(2) = npxy      call PISUM(isum2,iwork2,2,1)      ierr = isum2(1)      npxy = isum2(2)      call PSUM(sum2,work2,2,1)      at1 = 1./float(npxy)      sum2(1) = at1*sum2(1) - vdx      sum2(2) = at1*sum2(2) - vdy      do 70 l = 1, nblok      do 60 j = nps(l), npp(l)      part(3,j,l) = part(3,j,l) - sum2(1)      part(4,j,l) = part(4,j,l) - sum2(2)   60 continue   70 continuec process errors      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr      else if (npxy.ne.(npx*npy)) then         write (2,*) 'particle distribution truncated, np = ', npxy      endif      return      endc-----------------------------------------------------------------------      subroutine PVISTR2(part,npp,nps,vtx,vty,vdx,vdy,npx,npy,nx,ny,idim     1p,npmax,nblok,ipbc,vranx,vrany,kstrt,nvp,nvrp,ierr)c for 2d code, this subroutine calculates initial particle co-ordinatesc and velocities with uniform density and maxwellian velocity with driftc for distributed data.  on input, the array npp contains the number ofc particles already stored in part, normally zero.  on output, containsc the number of particles stored in part.c part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition lc vtx/vty = thermal velocity of electrons in x/y directionc vdx/vdy = drift velocity of beam electrons in x/y directionc npx/npy = initial number of particles distributed in x/y directionc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c ipbc = particle boundary condition = (0,1,2,3) =c (none,2d periodic,2d reflecting,mixed reflecting/periodic)c vranx, vrany = output arrays for parallel gaussian random numbers,c with zero mean and unit variancec kstrt = starting data block numberc nvp = number of real or virtual processorsc nvrp = number of parallel seeds per processorc ierr = (0,1) = (no,yes) error condition existsc npx*npy should be a multiple of nvrp*nvpc with spatial decomposition      double precision vranx, vrany      double precision sum0, sum1      dimension part(idimp,npmax,nblok)      dimension npp(nblok), nps(nblok)      dimension vranx(nvrp,nblok), vrany(nvrp,nblok)      dimension sum2(2), isum2(2)      dimension work2(2), iwork2(2)      ierr = 0c particle distribution constants      ks = kstrt - 2      npxy = npx*npyc npd = total number of seeds used      npd = nvrp*nvpc ipp = number of particles per seed      ipp = npxy/npdc nppv = number of particles per processor      nppv = nvrp*ipp      if (npxy.ne.(ipp*npd)) then         ierr = 1         write (2,*) 'number of seeds not multiple of particle number',      1npd, npxy         return      endifc set boundary values      edgelx = 0.      edgely = 0.      at1 = float(nx)/float(npx)      at2 = float(ny)/float(npy)      if (ipbc.eq.2) then         edgelx = 1.         edgely = 1.         at1 = float(nx-2)/float(npx)         at2 = float(ny-2)/float(npy)      else if (ipbc.eq.3) then         edgelx = 1.         edgely = 0.         at1 = float(nx-2)/float(npx)         at2 = float(ny)/float(npy)      endifc particles in each block get random numbers from the same seed      do 30 n = 1, ipp      call pranorm(vranx,kstrt,nvp,nvrp,nd,nvrp,nblok)      call pranorm(vrany,kstrt,nvp,nvrp,nd,nvrp,nblok)c inner loop over independent seeds      do 20 l = 1, nblok      koff = nppv*(l + ks)      do 10 jj = 1, nvrpc i = local particle number belonging to jj seed      i = n + ipp*(jj - 1)c j, k = used to determine spatial location of this particle      nn = i + koff      k = (nn - 1)/npx + 1      j = nn - npx*(k - 1)      npt = nps(l) + i - 1      if (npt.le.npmax) thenc uniform density profile         part(1,npt,l) = edgelx + at1*(float(j) - .5)         part(2,npt,l) = edgely + at2*(float(k) - .5)c maxwellian velocity distribution         part(3,npt,l) = vtx*vranx(jj,l)         part(4,npt,l) = vty*vrany(jj,l)      else         ierr = ierr + 1      endif   10 continuec update particle number      npp(l) = npp(l) + nvrp   20 continue   30 continue      npxy = 0c add correct drift      sum2(1) = 0.      sum2(2) = 0.      do 50 l = 1, nblok      sum0 = 0.0d0      sum1 = 0.0d0      do 40 j = nps(l), npp(l)      npxy = npxy + 1      sum0 = sum0 + part(3,j,l)      sum1 = sum1 + part(4,j,l)   40 continue      sum2(1) = sum2(1) + sum0      sum2(2) = sum2(2) + sum1   50 continue      isum2(1) = ierr      isum2(2) = npxy      call PISUM(isum2,iwork2,2,1)      ierr = isum2(1)      npxy = isum2(2)      call PSUM(sum2,work2,2,1)      at1 = 1./float(npxy)      sum2(1) = at1*sum2(1) - vdx      sum2(2) = at1*sum2(2) - vdy      do 70 l = 1, nblok      do 60 j = nps(l), npp(l)      part(3,j,l) = part(3,j,l) - sum2(1)      part(4,j,l) = part(4,j,l) - sum2(2)   60 continue   70 continuec process errors      if (ierr.gt.0) then         write (2,*) 'particle overflow error, ierr = ', ierr      else if (npxy.ne.(npx*npy)) then         write (2,*) 'particle distribution truncated, np = ', npxy      endif      return      endc-----------------------------------------------------------------------      subroutine pcguard2l(f,kstrt,nvp,nxv,nypmx,nblok,kyp,kblok)c this subroutine copies data from field to particle partitions, copyingc data to guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes one extra guard cell.c kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c nblok = number of particle partitions, assumed equal to kblok.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c linear interpolation, for distributed data      implicit none      real f      integer kstrt, nvp, nxv, nypmx, nblok, kyp, kblok      dimension f(nxv,nypmx,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus      integer ks, moff, kl, kr, j, l, ierr, msid      dimension istatus(lstat)      ks = kstrt - 2      moff = nypmx*nvpc copy to guard cells      do 20 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) then         kr = kr - nvp      endif      kl = l + ks      if (kl.lt.1) then         kl = kl + nvp      endifc this loop is used for shared memory computersc     do 10 j = 1, nxvc     f(j,kyp+1,l) = f(j,1,kr)c  10 continuec this segment is used for mpi computers      call MPI_IRECV(f(1,kyp+1,l),nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)      call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)   20 continue      return      endc-----------------------------------------------------------------------      subroutine paguard2l(f,scr,kstrt,nvp,nx,nxv,nypmx,nblok,kyp,kblok)c this subroutine copies data from particle to field partitions, addingc data from guard cells, where the field and particle partitions are c assumed to be the same.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes one extra guard cell.c scr(j,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c nblok = number of particle partitions, assumed equal to kblok.c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c linear interpolation, for distributed data      implicit none      real f, scr      integer kstrt, nvp, nx, nxv, nypmx, nblok, kyp, kblok      dimension f(nxv,nypmx,nblok), scr(nxv,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus      integer nx1, ks, moff, kl, kr, j, l, ierr, msid      dimension istatus(lstat)      nx1 = nx + 1      ks = kstrt - 2      moff = nypmx*nvpc add guard cells      do 30 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) then         kr = kr - nvp      endif      kl = l + ks      if (kl.lt.1) then         kl = kl + nvp      endifc this segment is used for shared memory computersc     do 10 j = 1, nx1c     scr(j,l) = f(j,kyp+1,kl)c  10 continuec this segment is used for mpi computers      call MPI_IRECV(scr,nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,kyp+1,l),nxv,mreal,kr-1,moff+1,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      do 20 j = 1, nx1      f(j,1,l) = f(j,1,l) + scr(j,l)   20 continue   30 continue      return      endc-----------------------------------------------------------------------      subroutine pmove2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc ierr = (0,1) = (no,yes) error condition exists      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer nbsize, nter, msid, istatus      real any, yt      dimension msid(4), istatus(lstat), ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 50 l = 1, nblok      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 30 j = 1, npp(l)      yt = part(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = part(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,j,l)            sbufr(4,jsr(1,l),l) = part(4,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endifc particles going down      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = part(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,j,l)            sbufl(4,jsl(1,l),l) = part(4,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1            go to 40         endif      endif   30 continue   40 jss(1,l) = jsl(1,l) + jsr(1,l)   50 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computersc     jsl(2,l) = jsr(1,kl)c     do 110 j = 1, jsl(2,l)c     rbufl(1,j,l) = sbufr(1,j,kl)c     rbufl(2,j,l) = sbufr(2,j,kl)c     rbufl(3,j,l) = sbufr(3,j,kl)c     rbufl(4,j,l) = sbufr(4,j,kl)c 110 continuec     jsr(2,l) = jsl(1,kr)c     do 120 j = 1, jsr(2,l)c     rbufr(1,j,l) = sbufl(1,j,kr)c     rbufr(2,j,l) = sbufl(2,j,kr)c     rbufr(3,j,l) = sbufl(3,j,kr)c     rbufr(4,j,l) = sbufl(4,j,kr)c 120 continuec this segment is used for mpi computersc post receive      call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)      call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particles      call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)     1,ierr)      call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),i     1err)c wait for particles to arrive      call MPI_WAIT(msid(1),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsl(2,l) = nps/idimp      call MPI_WAIT(msid(2),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (6,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (6,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sent      call MPI_WAIT(msid(3),istatus,ierr)      call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)c particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufr(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufr(3,j,l)         sbufr(4,jsr(1,l),l) = rbufr(4,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = rbufl(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = rbufl(3,j,l)            sbufr(4,jsr(1,l),l) = rbufl(4,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = rbufl(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = rbufl(3,j,l)            sbufl(4,jsl(1,l),l) = rbufl(4,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (6,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (6,*) 'Info: particles being passed further = ', ibflg(2)         if (ibflg(3).gt.0) ibflg(3) = 1         go to 100      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after movec     nps = 0c     do 270 l = 1, nblokc     nps = nps + npp(l)c 270 continuec     ibflg(2) = npsc     ibflg(1) = nprc     call PISUM(ibflg,iwork,2,1)c     if (ibflg(1).ne.ibflg(2)) thenc        write (6,*) 'particle number error, old/new=',ibflg(1),ibflg(2)c        ierr = 1c     endifc information      if (nter.gt.0) then         write (6,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine pxmov2(part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr     1,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ier     2r)c this subroutine moves particles into appropriate spatial regionsc periodic boundary conditionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc maskp = scratch array for particle addressesc ierr = (0,1) = (no,yes) error condition existsc optimized for vector processor      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, maskp, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok), maskp(npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer l, ks, iter, npr, nps, ibflg, iwork, kb, kl, kr, j, j1, j2      integer nbsize, nter, msid, istatus      real any, yt      dimension msid(4), istatus(lstat), ibflg(3), iwork(3)      any = float(ny)      ks = kstrt - 2      nbsize = idimp*nbmax      iter = 2      nter = 0c debugging section: count total number of particles before move      npr = 0      do 10 l = 1, nblok      npr = npr + npp(l)   10 continuec buffer outgoing particles   20 do 80 l = 1, nblok      jss(1,l) = 0      jss(2,l) = 0c find mask function for particles out of bounds      do 30 j = 1, npp(l)      yt = part(2,j,l)      if ((yt.ge.edges(2,l)).or.(yt.lt.edges(1,l))) then         jss(1,l) = jss(1,l) + 1         maskp(j,l) = 1      else         maskp(j,l) = 0      endif   30 continuec set flag if hole buffer would overflow      if (jss(1,l).gt.ntmax) then         jss(1,l) = ntmax         jss(2,l) = 1      endifc accumulate location of holes      do 40 j = 2, npp(l)      maskp(j,l) = maskp(j,l) + maskp(j-1,l)   40 continuec store addresses of particles out of bounds      do 50 j = 2, npp(l)      if ((maskp(j,l).gt.maskp(j-1,l)).and.(maskp(j,l).le.ntmax)) then         ihole(maskp(j,l),l) = j      endif   50 continue      if (maskp(1,l).gt.0) ihole(1,l) = 1      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0c load particle buffers      do 60 j = 1, jss(1,l)      yt = part(2,ihole(j,l),l)c particles going up      if (yt.ge.edges(2,l)) then         if ((kb+1).eq.nvp) yt = yt - any         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            sbufr(1,jsr(1,l),l) = part(1,ihole(j,l),l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,ihole(j,l),l)            sbufr(4,jsr(1,l),l) = part(4,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endifc particles going down      else         if (kb.eq.0) yt = yt + any         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            sbufl(1,jsl(1,l),l) = part(1,ihole(j,l),l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,ihole(j,l),l)            sbufl(4,jsl(1,l),l) = part(4,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1c           go to 70         endif      endif   60 continue   70 jss(1,l) = jsl(1,l) + jsr(1,l)   80 continuec check for full buffer condition      nps = 0      do 90 l = 1, nblok      nps = nps + jss(2,l)   90 continue      ibflg(3) = npsc copy particle buffers  100 iter = iter + 2      do 130 l = 1, nblokc get particles from below and above      kr = l + ks + 2      if (kr.gt.nvp) kr = kr - nvp      kl = l + ks      if (kl.lt.1) kl = kl + nvpc this segment is used for shared memory computersc     jsl(2,l) = jsr(1,kl)c     do 110 j = 1, jsl(2,l)c     rbufl(1,j,l) = sbufr(1,j,kl)c     rbufl(2,j,l) = sbufr(2,j,kl)c     rbufl(3,j,l) = sbufr(3,j,kl)c     rbufl(4,j,l) = sbufr(4,j,kl)c 110 continuec     jsr(2,l) = jsl(1,kr)c     do 120 j = 1, jsr(2,l)c     rbufr(1,j,l) = sbufl(1,j,kr)c     rbufr(2,j,l) = sbufl(2,j,kr)c     rbufr(3,j,l) = sbufl(3,j,kr)c     rbufr(4,j,l) = sbufl(4,j,kr)c 120 continuec this segment is used for mpi computersc post receive      call MPI_IRECV(rbufl,nbsize,mreal,kl-1,iter-1,lgrp,msid(1),ierr)      call MPI_IRECV(rbufr,nbsize,mreal,kr-1,iter,lgrp,msid(2),ierr)c send particles      call MPI_ISEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-1,lgrp,msid(3)     1,ierr)      call MPI_ISEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,msid(4),i     1err)c wait for particles to arrive      call MPI_WAIT(msid(1),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsl(2,l) = nps/idimp      call MPI_WAIT(msid(2),istatus,ierr)      call MPI_GET_COUNT(istatus,mreal,nps,ierr)      jsr(2,l) = nps/idimp  130 continuec check if particles must be passed further      nps = 0      do 160 l = 1, nblokc check if any particles coming from above belong here      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 140 j = 1, jsr(2,l)      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = jsl(1,l) + 1      if (rbufr(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1  140 continue      if (jsr(1,l).ne.0) write (6,*) 'Info: particles returning up'c check if any particles coming from below belong here      do 150 j = 1, jsl(2,l)      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = jsr(1,l) + 1      if (rbufl(2,j,l).lt.edges(1,l)) jss(2,l) = jss(2,l) + 1  150 continue      if (jss(2,l).ne.0) write (6,*) 'Info: particles returning down'      jsl(1,l) = jsl(1,l) + jss(2,l)      nps = nps + (jsl(1,l) + jsr(1,l))  160 continue      ibflg(2) = npsc make sure sbufr and sbufl have been sent      call MPI_WAIT(msid(3),istatus,ierr)      call MPI_WAIT(msid(4),istatus,ierr)      if (nps.eq.0) go to 210c remove particles which do not belong here      do 200 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 170 j = 1, jsr(2,l)      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)c particles going up, should not happen      elseif (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufr(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufr(3,j,l)         sbufr(4,jsr(1,l),l) = rbufr(4,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)      endif  170 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jss(2,l) = 0      do 180 j = 1, jsl(2,l)      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = rbufl(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = rbufl(3,j,l)            sbufr(4,jsr(1,l),l) = rbufl(4,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles going down, should not happen      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = rbufl(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = rbufl(3,j,l)            sbufl(4,jsl(1,l),l) = rbufl(4,j,l)         else            jss(2,l) = 2*npmax            go to 190         endifc particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)      endif  180 continue  190 jsl(2,l) = jss(2,l)  200 continuec check if move would overflow particle array  210 nps = 0      do 220 l = 1, nblok      jss(2,l) = npp(l) + jsl(2,l) + jsr(2,l) - jss(1,l) - npmax      if (jss(2,l).le.0) jss(2,l) = 0      nps = nps + jss(2,l)  220 continue      ibflg(1) = nps      call PISUM(ibflg,iwork,3,1)      ierr = ibflg(1)      if (ierr.gt.0) then         write (6,*) 'particle overflow error, ierr = ', ierr         return      endif      do 260 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 230 j = 1, jss(2,l)      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)  230 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 240 j = 1, jss(2,l)c no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)      endif  240 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 250 j = 1, jsr(2,l)c holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)      endif  250 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  260 continuec check if any particles have to be passed further      if (ibflg(2).gt.0) then         write (6,*) 'Info: particles being passed further = ', ibflg(2)         if (ibflg(3).gt.0) ibflg(3) = 1         go to 100      endifc check if buffer overflowed and more particles remain to be checked      if (ibflg(3).gt.0) then         nter = nter + 1         go to 20      endifc debugging section: count total number of particles after move      nps = 0      do 270 l = 1, nblok      nps = nps + npp(l)  270 continue      ibflg(2) = nps      ibflg(1) = npr      call PISUM(ibflg,iwork,2,1)      if (ibflg(1).ne.ibflg(2)) then         write (6,*) 'particle number error, old/new=',ibflg(1),ibflg(2)         ierr = 1      endifc information      if (nter.gt.0) then         write (6,*) 'Info: ', nter, ' buffer overflows, nbmax=', nbmax      endif      return      endc-----------------------------------------------------------------------      subroutine ptpose(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jb     1lok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(nxv,kypd,kblok), g(nyv,kxpd,jblok)      dimension s(kxp,kyp,kblok), t(kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff,j,l) = f(j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,l),kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,msid,     1ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(j,k,l) = f(j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,l),kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,ierr     1)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(k+koff,j,l) = t(j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine p2tpose(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kypd/kxpd = second dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(2,nxv,kypd,kblok), g(2,nyv,kxpd,jblok)      dimension s(2,kxp,kyp,kblok), t(2,kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1,k+koff,j,l) = f(1,j+joff,k,i)c     g(2,k+koff,j,l) = f(2,j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec     returnc this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,1,l),2*kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,m     1sid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(1,j,k,l) = f(1,j+joff,k,l)         s(2,j,k,l) = f(2,j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,1,l),2*kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,     1ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(1,k+koff,j,l) = t(1,j,k,l)         g(2,k+koff,j,l) = t(2,j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine PTPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblok     1,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(nxv*kypd*kblok), g(nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff+nyv*(j-1+kxpd*(l-1))) = f(j+joff+nxv*(k-1+kypd*(i-1)))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, kxp         g(j+kxp*(k+is)) = f(j+joff+nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continuec exchange data      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+kxp*kyp*(ir+koff)),kxp*kyp,mcplx,ir-1,ir+kxy     1m+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+kxp*kyp*(is+ioff)),kxp*kyp,mcplx,is-1,l+ks+kx     1ym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(k+koff+nyv*(j+joff)) = f(j+kxp*(k+ir))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      subroutine P2TPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(1:2,k+kyp*(m-1),j,l) = f(1:2,j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives multiple asynchronous messages.c f = complex input arrayc g = complex output arrayc msid, mrid = scratch arrays for identifying asynchronous messagesc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc kxb/kyb = number of processors in x/yc kypd/kxpd = second dimension of f/gc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(2*nxv*kypd*kblok), g(2*nyv*kxpd*jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c lgrp = current communicatorc mcplx = default datatype for complex      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer ks, kxb, kyb, l, i, joff, koff, k, j      integer jkblok, kxym, mtr, ntr, mntr, msid      integer ir0, is0, ii, ir, is, ioff, ierr, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks) - 1c     do 30 i = 1, kybc     koff = kyp*(i - 1) - 1c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(1+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(1+2*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c     g(2+2*(k+koff+nyv*(j-1+kxpd*(l-1)))) = f(2+2*(j+joff+nxv*(k-1+kypdc    1*(i-1))))c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)c transpose local data      do 50 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kypd*(l - 1) - 1      do 40 i = 1, kxym      is0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 30 ii = 1, ntr      if (kstrt.le.ny) then         is = is0 + kxym*(ii - 1)         joff = 2*kxp*(is - 1)         is = kyp*(is + ioff) - 1         do 20 k = 1, kyp         do 10 j = 1, 2*kxp         g(j+2*kxp*(k+is)) = f(j+joff+2*nxv*(k+koff))   10    continue   20    continue      endif   30 continue   40 continue   50 continue      do 80 l = 1, jkblok      ioff = kxb*(l - 1) - 1      koff = kyb*(l - 1) - 1      do 70 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 60 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(f(1+2*kxp*kyp*(ir+koff)),2*kxp*kyp,mcplx,ir-1,ir     1+kxym+1,lgrp,msid,ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         call MPI_SEND(g(1+2*kxp*kyp*(is+ioff)),2*kxp*kyp,mcplx,is-1,l+k     1s+kxym+2,lgrp,ierr)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         call MPI_WAIT(msid,istatus,ierr)      endif   60 continue   70 continue   80 continuec transpose local data      do 130 l = 1, jkblok      ioff = kyb*(l - 1) - 1      joff = kxpd*(l - 1) - 1      do 120 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      do 110 ii = 1, mtr      if (kstrt.le.nx) then         ir = ir0 + kxym*(ii - 1)         koff = kyp*(ir - 1)         ir = kyp*(ir + ioff) - 1         do 100 k = 1, kyp         do 90 j = 1, kxp         g(2*(k+koff+nyv*(j+joff))-1) = f(2*(j+kxp*(k+ir))-1)         g(2*(k+koff+nyv*(j+joff))) = f(2*(j+kxp*(k+ir)))   90    continue  100    continue      endif  110 continue  120 continue  130 continue      return      endc-----------------------------------------------------------------------      function ranorm()c this program calculates a random number y from a gaussian distributionc with zero mean and unit variance, according to the method ofc mueller and box:c    y(k) = (-2*ln(x(k)))**1/2*sin(2*pi*x(k+1))c    y(k+1) = (-2*ln(x(k)))**1/2*cos(2*pi*x(k+1)),c where x is a random number uniformly distributed on (0,1).c written for the ibm by viktor k. decyk, ucla      integer r1,r2,r4,r5      double precision ranorm,h1l,h1u,h2l,r0,r3,asc,bsc,temp      save iflg,r1,r2,r4,r5,h1l,h1u,h2l,r0      data r1,r2,r4,r5 /885098780,1824280461,1396483093,55318673/      data h1l,h1u,h2l /65531.0d0,32767.0d0,65525.0d0/      data iflg,r0 /0,0.0d0/      if (iflg.eq.0) go to 10      ranorm = r0      r0 = 0.0d0      iflg = 0      return   10 isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r1 - (r1/isc)*isc      r3 = h1l*dble(r1) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r2/isc      isc = r2 - i1*isc      r0 = h1l*dble(r2) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r2 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r1 = r3 - dble(isc)*bsc      temp = dsqrt(-2.0d0*dlog((dble(r1) + dble(r2)*asc)*asc))      isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r4 - (r4/isc)*isc      r3 = h2l*dble(r4) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r5/isc      isc = r5 - i1*isc      r0 = h2l*dble(r5) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r5 = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r4 = r3 - dble(isc)*bsc      r0 = 6.28318530717959d0*((dble(r4) + dble(r5)*asc)*asc)      ranorm = temp*dsin(r0)      r0 = temp*dcos(r0)      iflg = 1      return      endc-----------------------------------------------------------------------      subroutine timera(icntrl,chr,time)c this subroutine performs timingc input: icntrl, chrc icntrl = (-1,0,1) = (initialize,ignore,read) clockc clock should be initialized before it is read!c chr = character variable for labeling timingsc time = elapsed time in secondsc written for mpi      implicit none      integer icntrl      character*8 chr      real timec get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer idproc, ierr      real nclock, mclock      double precision jclock      save jclock   91 format (1x,a8,1x,'max/min real time = ',e14.7,1x,e14.7,1x,'sec')      data jclock /0.0d0/      if (icntrl.eq.0) return      if (icntrl.eq.1) go to 10c initialize clock      call MPI_BARRIER(lgrp,ierr)      jclock = MPI_WTIME()      returnc read clock and write time difference from last clock initialization   10 nclock = real(MPI_WTIME() - jclock)      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MIN,lgrp,ierr)      mclock = time      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MAX,lgrp,ierr)      call MPI_COMM_RANK(lgrp,idproc,ierr)      if (idproc.eq.0) write (6,91) chr, time, mclock      return      endc-----------------------------------------------------------------------      subroutine pranorm(vran,kstrt,nvp,nvrp,ndp,nvrd,nblok)c this program calculates nvrp random numbers for nvp processors from a c gaussian distribution with zero mean and unit variance, according toc the method of mueller and box:c    y(k) = (-2*ln(x(k)))**1/2*sin(2*pi*x(k+1))c    y(k+1) = (-2*ln(x(k)))**1/2*cos(2*pi*x(k+1)),c where x is a random number uniformly distributed on (0,1).c written for the ibm by viktor k. decyk, uclac parallel versionc each random number is generated from a separate seed, which is justc the normal seed calculated every 100,000,000 million numbers apart.c thus if more than 100,000,000 arrays of numbers are requested, thec different numbers will no longer be unique. each processor usesc no more than (ndv-1)/nvp+1 seeds. if nvp > ndv, adjacent mdpc processors will share the same seed, where mdp = nvp/min0(nvp,ndv),c but return unique numbers by throwing away duplicate numbers amongstc the group. ndv is the maximum amount of parallelism one can obtain.c vran = output array of random numbersc kstrt = starting data block numberc nvp = number of real or virtual processorsc nvrp = number of random numbers requested per processor, <= ndv/nvpc ndp = number of random numbers returned = min((ndv-1)/nvp+1,nvrp)c nvrd = first dimension of vran arrayc nblok = number of data blocksc ndv = total maximum number of random seeds, currently 256      parameter(ndvb=64,ndv=4*ndvb)      integer r1,r2,r4,r5      integer r1a,r1b,r1c,r1d,r2a,r2b,r2c,r2d      integer r4a,r4b,r4c,r4d,r5a,r5b,r5c,r5d      double precision vran,vran0,h1l,h1u,h2l,r0,r3,asc,bsc,temp      dimension vran(nvrd,nblok)      dimension r1(ndv), r2(ndv), r4(ndv), r5(ndv), vran0(ndv)      dimension r1a(ndvb), r1b(ndvb), r1c(ndvb), r1d(ndvb)      dimension r2a(ndvb), r2b(ndvb), r2c(ndvb), r2d(ndvb)      dimension r4a(ndvb), r4b(ndvb), r4c(ndvb), r4d(ndvb)      dimension r5a(ndvb), r5b(ndvb), r5c(ndvb), r5d(ndvb)      equivalence (r1a(1),r1(1)), (r1b(1),r1(ndvb+1))      equivalence (r1c(1),r1(2*ndvb+1)), (r1d(1),r1(3*ndvb+1))      equivalence (r2a(1),r2(1)), (r2b(1),r2(ndvb+1))      equivalence (r2c(1),r2(2*ndvb+1)), (r2d(1),r2(3*ndvb+1))      equivalence (r4a(1),r4(1)), (r4b(1),r4(ndvb+1))      equivalence (r4c(1),r4(2*ndvb+1)), (r4d(1),r4(3*ndvb+1))      equivalence (r5a(1),r5(1)), (r5b(1),r5(ndvb+1))      equivalence (r5c(1),r5(2*ndvb+1)), (r5d(1),r5(3*ndvb+1))      save iflg,h1l,h1u,h2l      save r1,r2,r4,r5,vran0      data iflg /0/      data h1l,h1u,h2l /65531.0d0,32767.0d0,65525.0d0/      data r1a /359740401,579253173,631138885,301320988,616045397,197865     1899,320616057,1483498427,94225675,293599141,119335525,1401265604,4     238232031,258875505,1516636373,2083742725,1049217694,678455002,2039     3395477,737755465,2088659986,2037589420,402632816,1340891228,103903     41049,1282693042,1660116788,464751626,1886797,1266915258,880133036,     51015492382,111053996,868360335,1180991652,204559462,240784624,1406     692733,1701763304,1141745325,8877543,1168775120,605400900,805925055     7,632316507,334371796,229344900,1638112616,1487940890,2056688597,14     898710389,1659680628,968083694,320658079,486957793,1770449327,16311     976869,1007414014,8949141,161873637,319971082,1275092926,1886651485     a,1762615778/      data r1b /1902075624,1151890347,1362323973,1914314098,798243597,96     1731537,839057753,112122977,1165362756,2393148,2146479398,860350843     2,572224344,1756173944,953937974,1643196300,1592579351,1800565521,5     311044950,222837089,1923358284,1578824916,1765893970,1937519044,168     42760131,1948161675,2073215223,1229841282,2118406728,1887892227,344     5980751,1829747047,20529831,1502561544,53526120,430292544,146251102     60,730097511,1497253836,428186839,1181683234,197448938,508787720,20     742011641,614837975,2076223582,48029161,1020375667,163928726,200907     8920,320376889,234970895,51708971,251857994,1779848646,1946275994,1     9177370210,1283584601,1689751898,69259745,1677212679,875358602,1420     a772803,356371886/      data r1c /2075250663,281816231,864163276,1925828303,1843883212,993     1629822,1434862656,264732686,259532163,1298178521,484684781,9696164     241,1553247448,1430153606,747890590,1274723866,733959575,132229199,     3459407902,1645068415,1623986828,1953151234,652920379,189619762,534     4804100,923506282,1721538240,2106876417,7582793,738712402,212595263     52,1213150903,1078894248,582271624,1397557297,125560607,1988864109,     6276375255,994086582,380247559,1184293989,1137541898,860286163,1140     7532794,321783130,389804527,283551060,52554372,828399130,610283089,     8897047301,543356336,462348655,1994887932,1092898994,1939108492,611     9066406,1066474171,1172793639,320744419,704402188,120414300,2056136     a496,117017729/      data r1d /1217742571,1637785258,1840721305,2075238194,232681617,55     11071857,953335598,1969426114,735109832,132190333,857915067,4588492     25,733433950,2110708122,616214284,1878839215,556665886,1087030932,2     3038412013,2046203556,1562577107,420426295,1948814634,17606056,5508     485707,199759695,1631428751,1010026375,1356312656,246616232,1243148     5264,1811048205,671834288,970938638,669392386,224431878,1789222198,     62079181645,379743655,220719678,422295726,1282466353,136102308,2055     7455617,594945507,1594669190,1995807941,977339803,464385955,1450641     8689,429859287,968768344,1766727865,2004299444,358421412,569085829,     92082681841,1541809188,826594553,172674284,2135647255,632194101,155     a019427,737107546/      data r2a /28358029,1188633485,1412792717,1103488909,663375245,4951     104909,1001331085,437223309,1352918413,2003585933,644395405,1972967     2309,2096987533,1419109261,341985677,1415753613,748098957,889158541     3,94101901,913065869,1601219981,413733773,2048227725,464904077,3613     483309,2140318605,1909395853,71268237,1323556237,1773945741,1825089     5933,1879641997,192771469,1462098829,1795309965,1595058061,12639963     601,1204777869,1820055949,1365000077,242263437,1001982861,189932788     75,1189468045,1422540173,853713805,2033125773,1068461965,509859213,     8759970701,73965965,1001981837,1799187853,720753549,316815757,99002     97661,995558797,736062349,614191501,1032599437,246455693,805897101,     a966093197,1129697165/      data r2b /1699362189,930257805,1372520845,1281320845,1059310989,11     109144461,1833474445,1487470477,473785741,1342557069,201470349,1748     2146061,2090270093,1630495629,771475853,2063347597,1613796749,19729     360141,1396007309,285591437,1191849357,222466957,2075064717,7098448     477,824427917,673983373,661164429,1188624269,511532429,1180025741,1     5449273741,1721929613,253162893,1740594061,144425357,162277261,4931     69309,208204685,1041586573,804634509,2047485325,877824909,199327374     71,1501517709,1952693645,1601971085,852003213,105443213,1912427917,     8233159565,1912742285,911378317,1926688141,1066357645,880523661,177     91839373,1995474317,1954081677,2050314637,539342733,2118786445,7488     a48013,1127147917,1508855693/      data r2c / 149140877,1745623949,258507149,385410957,381504909,6494     142189,1591875981,1463975821,668394893,1755270029,832287117,4495829     289,1009810829,768140173,127224205,1637199757,1405752717,1983019917     3,1624170893,731858829,1856220557,1104941965,1028159885,2028527501,     4213730701,281389965,486674829,1232238477,773250445,1659847565,2147     5199373,490475405,1387296141,945347469,1714766221,1950721933,205586     67789,285373325,1336859021,1318010765,631481741,1827408781,10134777     773,739825549,1409105293,1276486541,744622477,216166285,93771149,78     80090253,530293133,1894516621,980446605,338219917,370489741,1479909     9261,1921648013,2098359181,265212301,1119827853,769891725,176554074     a9,214460813,814272397/      data r2d /1820145037,1487248269,218235277,563242893,777440653,1263     1481741,276535693,366739341,1936745869,1094241165,389362061,2247617     241,1003093389,979526541,556714381,137310093,123966861,919337869,77     38592653,104384397,1446849933,913675149,1054996877,125984653,676775     4309,962538381,1385927053,202110861,2108710285,1065927565,177138318     51,332763021,1447687565,1223842701,63881613,517941133,841190797,143     66283789,558389645,757645197,289219981,1703250829,1107423629,105187     75213,1939258765,2024743821,1710983565,1400631181,1496339853,253279     8117,221585805,1803913101,1107946893,683824013,934197645,114237325,     9774079885,1168894861,1701335437,626571149,494738829,1708491661,375     a515533,1193430925/      data r4a /1494108262,1989145426,1552962897,694486778,1754614022,25     11970638,1939753416,1149145743,2023686104,1040210686,1683330424,114     23900382,581089000,293341551,1273074283,260197364,1916786754,878264     3750,253082287,2146393565,272494066,425825733,91072590,206790821,18     437933060,642168769,604837205,1945376344,1849933443,1587706286,6836     562288,1769955586,346238332,251055511,1157232075,1040925359,7394408     692,2007877002,1222081873,202759530,1948470029,2014942097,131050200     70,1652912961,1267494332,963938746,1664474419,1873307198,1529793301     8,1507924495,909609381,741685359,1091759396,392812189,1071533011,10     95548766,996769269,2063692145,1854856650,208609944,1401636755,58899     a605,1364597140,412212905/      data r4b /150549516,1353755158,1122564414,2064237085,2086423914,13     112181247,346330836,630583039,444972475,1587991646,965618785,144897     28269,1198444297,108893502,1901270003,1123044673,154725217,38784221     38,1934149525,1128620729,858627279,1507043503,426565970,2092730831,     4280923487,986857306,191175479,220038288,277186333,1949669045,17699     551057,899167496,1866757140,956408013,2008119210,96771395,165919905     68,1612869550,1218031183,111337293,1165651938,258749796,1753099596,     71704672562,1077801679,1721451514,139284141,528357630,1989300518,71     80151550,681413757,730068156,822220019,1864573946,1152221386,642609     9786,138405954,1149057404,2103698846,2095273026,270963999,160636110     a2,1146066183,397835297/      data r4c /45680022,1194183101,111642895,94366755,1797407667,209651     15221,1481300931,652265045,1354070401,707645090,515950383,147864288     20,1642983694,1238478320,1010345311,494264691,527373412,1405573259,     3982118495,1297417849,117730705,1742495871,1983136315,1000431070,10     450781343,2026272727,1230984446,1688512941,1063422363,1998811297,92     50322742,403753971,9109649,2088320231,354370925,603965117,579278221     6,1983246006,961296177,2146679829,434827931,1353693210,573182476,20     710574344,1517478342,572844831,589901835,1459338914,33770780,143838     82545,258083642,320647151,61390503,1451456060,1687502245,2002543610     9,1385685109,980554348,1982562648,1329962193,221084816,1835442284,4     a06171742,298531967/      data r4d /669940997,1546129994,1248642245,206062863,411560928,5267     144080,1664211095,226482318,13562669,649464267,982394338,278739208,     21505811704,1670976391,1429407408,1748224138,511906636,1953892688,2     359650830,1765738781,2002985015,1335655730,1214548744,371367120,169     41790787,1849958713,211583658,1235895726,1786480126,2005714801,1098     5102272,1644815811,974623474,1803444709,1340350230,1809678106,15870     676780,1309213347,1334826999,1294545857,1729467192,1376050101,83473     78865,1884898754,298580642,70401833,1818902862,1866602282,199874943     80,1983487753,623232475,1042295301,883403328,1772850530,1547059812,     91452810335,403842699,2063645760,394686709,1656142916,1245756901,12     a85160367,229190554,1743839168/      data r5a /1454526097,1497138321,585808529,1270673553,1806902929,44     19666193,1896583825,107858065,1928593041,1318990993,829188753,86183     29505,1819596433,1957629073,1678590609,1385134225,1479913105,218096     3785,149822097,1677742225,909543057,395361425,537850513,1739663505,     4108486289,341939345,695192209,1570898065,1224226449,57830545,62184     57185,1171445905,2109279889,1690518673,317815441,541307025,61616296     61,945036433,1930580625,1827965073,1039842961,2116351121,1165175441     7,736452753,1232836241,909495441,169083537,1561737361,1195142801,16     819436689,1089788561,8851601,926762641,2098691217,1779806865,372762     9769,427695761,199775377,91654801,505987217,1845425809,217656465,32     a0299665,408524945/      data r5b /884985489,4850833,318257809,80375953,1841342097,17088421     129,85529233,1669023889,419528337,1034663057,1769597585,879501457,9     214511505,129797265,1075495569,2006775953,1178807953,1141728401,150     3706833,755880081,1212417681,1922972817,1142715025,1421781137,10153     440689,326046865,1904036497,1856995473,587576977,645917841,28718760     51,2061523089,2076610193,735102097,587135633,2035363985,1187473041,     6593599633,656396945,1778518161,67649169,221410449,494971537,129098     75617,864622225,1766018193,102859409,572766353,1430908561,932455569     8,1627544209,1771344017,1766508177,2015689873,774058641,591751313,1     9871421073,720753809,1837370001,1328955537,1745647249,1342614673,52     a2510993,1835473041/      data r5c /1389186705,1733788817,1124448913,2111303825,802039441,18     194276241,1495700113,8964241,2131689105,1824076945,1636264593,19709     205233,1083168401,1523190929,1546142353,1554675857,1951444625,99161     38193,1225333393,907759761,441550481,229358737,673837713,30156945,8     448453265,1383896209,2039138961,1069351057,1024669329,160263313,102     56269841,1877858449,970198673,853427345,1930197649,308195473,685041     6297,1315904657,455955089,655329425,169197201,1547695249,898509457,     7771776657,1570150033,1548799121,1110377105,657537169,592932497,131     89216273,1091558033,312610961,1532511889,858946705,842052241,188448     91681,93920913,167990417,361859729,1078182033,572126865,1393831057,     a1798464145,41195665/      data r5d / 819646097,241501329,856898193,921006225,836478609,10059     168529,1832129169,1570130065,622624401,1539749009,429189777,1988567     2185,178083473,1842842769,943047313,28833937,1650339473,1915249809,     31226218129,2133381265,744425105,1756970129,1278702225,1859758225,1     4755307665,1368003729,1100499601,1355448465,388019857,748350609,691     5610257,620451985,937528977,2045494417,52034193,1802252433,12563513     677,964467857,1329255057,605882513,1344487057,1800238225,228305553,     71326309521,1201936017,257838225,1044152977,1816049809,828698257,63     82235153,1629313681,2075103377,224773777,775945361,1983787665,21034     970225,1537646225,688968849,2107574929,1901150353,472348305,3713056     a17,2000675473,1468143761/      ks = kstrt - 2      ndvp = (ndv - 1)/nvp + 1      ndp = min0(ndvp,nvrp)      mdp = nvp/min0(nvp,ndv)      if (iflg.eq.0) go to 30      do 20 k = 1, nblok      id = (k + ks)/mdp      do 10 j = 1, ndp      l = ndp*id + j      vran(j,k) = vran0(l)      vran0(l) = 0.0d0   10 continue   20 continue      iflg = 0      return   30 do 60 k = 1, nblok      id = (k + ks)/mdp      idr = k + ks - id*mdp + 1      do 50 j = 1, ndp      l = ndp*id + j      do 40 i = 1, mdp      isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r1(l) - (r1(l)/isc)*isc      r3 = h1l*dble(r1(l)) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r2(l)/isc      isc = r2(l) - i1*isc      r0 = h1l*dble(r2(l)) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r2(l) = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r1(l) = r3 - dble(isc)*bsc      temp = dsqrt(-2.0d0*dlog((dble(r1(l)) + dble(r2(l))*asc)*asc))      isc = 65536      asc = dble(isc)      bsc = asc*asc      i1 = r4(l) - (r4(l)/isc)*isc      r3 = h2l*dble(r4(l)) + asc*h1u*dble(i1)      i1 = r3/bsc      r3 = r3 - dble(i1)*bsc      bsc = 0.5d0*bsc      i1 = r5(l)/isc      isc = r5(l) - i1*isc      r0 = h2l*dble(r5(l)) + asc*h1u*dble(isc)      asc = 1.0d0/bsc      isc = r0*asc      r5(l) = r0 - dble(isc)*bsc      r3 = r3 + (dble(isc) + 2.0d0*h1u*dble(i1))      isc = r3*asc      r4(l) = r3 - dble(isc)*bsc      r0 = 6.28318530717959d0*((dble(r4(l)) + dble(r5(l))*asc)*asc)      if (i.eq.idr) then         vran(j,k) = temp*dsin(r0)         vran0(l) = temp*dcos(r0)      endif   40 continue   50 continue   60 continue      iflg = 1      return      endc-----------------------------------------------------------------------      subroutine PSUM (f,g,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c f(j,k) = sum over k of f(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus      integer idproc, ierr, kstrt, ks, l, kxs, k, kb, lb, msid, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = f(j,kb+kxs)c     elsec        g(j,k) = f(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      f(j,k) = f(j,k) + g(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PISUM (if,ig,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c if(j,k) = sum over k of if(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c if = input and output integer datac ig = scratch integer arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      integer if, ig, nxp, nblok      dimension if(nxp,nblok), ig(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplx, mdouble, lworldc lstat = length of status array      parameter(lstat=10)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mint = default datatype for reals      common /PPARMS/ nproc, lgrp, mreal, mint, mcplx, mdouble, lworldc local data      integer istatus      integer idproc, ierr, kstrt, ks, l, kxs, k, kb, lb, nsid, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        ig(j,k) = if(j,kb+kxs)c     elsec        ig(j,k) = if(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_ISEND(if,nxp,mint,kb+kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb+kxs-1,l+nxp,lgrp,istatus,ierr)      else         call MPI_ISEND(if,nxp,mint,kb-kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb-kxs-1,l+nxp,lgrp,istatus,ierr)      endif      call MPI_WAIT(nsid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      if(j,k) = if(j,k) + ig(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine PNTPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok,ndim)      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok, ndim      complex f, g, s, t      dimension f(ndim,nxv,kypd,kblok), g(ndim,nyv,kxpd,jblok)      dimension s(ndim,kxp,kyp,kblok), t(ndim,kxp,kyp,jblok)      return      endc-----------------------------------------------------------------------      subroutine PNTPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok,ndim)      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok, ndim      complex f, g      dimension f(ndim*nxv*kypd*kblok), g(ndim*nyv*kxpd*jblok)      return      endc-----------------------------------------------------------------------      subroutine P3TPOSE(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,j     1blok,kblok)      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, kxpd, kypd      integer jblok, kblok      complex f, g, s, t      dimension f(3,nxv,kypd,kblok), g(3,nyv,kxpd,jblok)      dimension s(3,kxp,kyp,kblok), t(3,kxp,kyp,jblok)      return      endc-----------------------------------------------------------------------      subroutine P3TPOSEX(f,g,nx,ny,kstrt,nxv,nyv,kxp,kyp,kxpd,kypd,jblo     1k,kblok)      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp      integer kxpd, kypd, jblok, kblok      complex f, g      dimension f(3*nxv*kypd*kblok), g(3*nyv*kxpd*jblok)      return      end