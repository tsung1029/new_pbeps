c * * * periodic 2d electrostatic particle simulation kernel code * * *c this is a simple 2d skeleton particle-in-cell code designed forc exploring new computer architectures.  it contains the critical piecesc needed for depositing charge, advancing particles, and solving thec field.  the code moves only electrons, with periodic electrostaticc forces obtained by solving poisson's equation with fast fourierc transforms.  the only diagnostic is particle and field energy.c portable gcpic kernel code, using algorithm described in:c p. c. liewer and v. k. decyk, j. computational phys. 85, 302 (1989).c written by viktor k. decyk, uclac for mpi distributed memory computers      program beps2kc indx/indy = exponent which determines length in x/y direction,c where nx=2**indx, ny=2**indyc npx/npy = initial number of particles distributed in x/y directionc     parameter( indx =   5, indy =   6, npx =      96, npy =     192)      parameter( indx =   6, indy =   7, npx =     384, npy =     768)c     parameter( indx =   7, indy =   8, npx =    1280, npy =    2560)c npxb/npyb = initial number of particles in beam in x/y directionc     parameter( npxb =  32, npyb =  64)      parameter( npxb = 128, npyb = 256)c     parameter( npxb = 384, npyb = 768)c tend = time at end of simulation, in units of plasma frequencyc dt = time interval between successive calculations      parameter( tend =  65.000, dt = 0.2000000e+00)c vty = thermal velocity of electrons in y directionc vdy = drift velocity of beam electrons y directionc vtdy = thermal velocity of beam electrons in y direction      parameter( vty =   1.000, vdy =   5.000, vtdy =   0.500)c avdy = absolute value of drift velocity of beam electrons y direction      parameter( avdy =   5.000)c indnvp = exponent determining number of real or virtual processorsc indnvp must be <= indyc idps = number of partition boundariesc idimp = dimension of phase space = 4c mshare = (0,1) = (no,yes) architecture is shared memory      parameter( indnvp =   1, idps =    2, idimp =   4, mshare =   0)c np = total number of electrons in simulation      parameter(npxy=npx*npy,npxyb=npxb*npyb,np=npxy+npxyb)      parameter(nx=2**indx,ny=2**indy,nxh=nx/2,nyh=ny/2)      parameter(nxv=nx+2,nyv=ny+2,nxvh=nxv/2,nxvy=nxv*ny)c nloop = number of time steps in simulation      parameter(nloop=tend/dt+.0001)c nvp = number of real or virtual processors, nvp = 2**indnvpc nblok = number of particle partitions      parameter(nvp=2**indnvp,nblok=1+mshare*(nvp-1))c npmax = maximum number of particles in each partitionc nypmx = maximum size of particle partition, including guard cells.      parameter(npmax=(np/nvp)*1.01+7000,nypmx=(ny-1)/nvp+4)      parameter(nxvyp=nxv*nypmx)c kyp = number of complex grids in each field partition in y directionc kxp = number of complex grids in each field partition in x direction      parameter(kyp=(ny-1)/nvp+1,kxp=(nxh-1)/nvp+1)c kblok = number of field partitions in y directionc jblok = number of field partitions in x direction      parameter(kblok=1+mshare*(ny/kyp-1),jblok=1+mshare*(nxh/kxp-1))c nxyh = maximum(nx,ny)/2c nxhy = maximum(nx/2,ny)      parameter(nmx=nx*(ny/nx)+ny*(nx/ny),nxy=nmx/(2-nx/nmx-ny/nmx))      parameter(nxyh=nxy/2,nmxh=nxh*(ny/nxh)+ny*(nxh/ny))      parameter(nxhy=nmxh/(2-nxh/nmxh-ny/nmxh))c nbmax = size of buffer for passing particles between processors      parameter(nbmax=1+(2*(npxy*vty+npxyb*vtdy)+1.4*npxyb*avdy)*dt/ny)c ntmax = size of hole array for particles leaving processors      parameter(ntmax=2*nbmax)c npd = size of scratch buffers for vectorized charge deposition      parameter(npd=128,nine=9)      complex qc, fxc, fyc, qt, fxt, fyt, ffc, bs, br, scr, sct      common /large/ partc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition l      dimension part(idimp,npmax,nblok)c maskp = scratch array for particle addresses      dimension maskp(npmax,nblok)c in real space, q(j,k,l) = charge density at grid point (j,kk)c where kk = k + noff(l) - 1c in real space, fx(j,k,l) = x component of force/charge at grid (j,kk)c in real space, fy(j,k,l) = y component of force/charge at grid (j,kk)c in other words, fx/fy are the convolutions of the electric fieldc over the particle shape, where kk = k + noff(l) - 1      dimension q(nxv,nypmx,nblok)      dimension fx(nxv,nypmx,nblok), fy(nxv,nypmx,nblok)c qc(j,k,l) = complex charge density for fourier mode j-1,kk-1c fxc(j,k,l) = x component of force/charge for fourier mode j-1,kk-1c fyc(j,k,l) = y component of force/charge for fourier mode j-1,kk-1c where kk = k + kyp*(l - 1)      dimension qc(nxvh,kyp,kblok)      dimension fxc(nxvh,kyp,kblok), fyc(nxvh,kyp,kblok)c qt(k,j,l) = complex charge density for fourier mode jj-1,k-1c fxt(k,j,l) = x component of force/charge for fourier mode jj-1,k-1c fyt(k,j,l) = y component of force/charge for fourier mode jj-1,k-1c where jj = j + kxp*(l - 1)      dimension qt(nyv,kxp,jblok)      dimension fxt(nyv,kxp,jblok), fyt(nyv,kxp,jblok)c ffc = form factor array for poisson solver      dimension ffc(nyh,kxp,jblok)c mixup, sct = arrays for fft      dimension mixup(nxhy), sct(nxyh)c bs, br = complex buffer arrays for transpose      dimension bs(kxp,kyp,kblok), br(kxp,kyp,jblok)c edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition l      dimension edges(idps,nblok)c nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.      dimension nyp(nblok), noff(nblok)c npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition l      dimension npp(nblok), nps(nblok)c sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processor      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)c rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processor      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)c ihole = location of holes left in particle arrays      dimension ihole(ntmax,nblok)c jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition l      dimension jsl(idps,nblok), jsr(idps,nblok)c jss(idps,l) = scratch array for particle partition lc scr(j,idps,l) = complex scratch array for particle partition l      dimension jss(idps,nblok), scr(nxvh,idps,nblok)c nn = scratch address array for vectorized charge depositionc amxy = scratch weight array for vectorized charge deposition      dimension nn(npd,nine,nblok), amxy(npd,nine,nblok)  991 format (5h t = ,i7)  992 format (19h * * * q.e.d. * * *)  993 format (34h field, kinetic, total energies = ,3e14.7)c vtx = thermal velocity of electrons in x directionc qme = charge on electron, in units of ec vdx = drift velocity of beam electrons in x directionc vtdx = thermal velocity of beam electrons in x direction      data vtx,qme,vdx,vtdx /1.,-1.,0.,.5/c ax/ay = half-width of particle in x/y direction      data ax,ay /.866667,.866667/c ntpose = (0,1) = (no,yes) input, output data are transposed in pfft2r      data ntpose /1/c initialize for parallel processing      call ppinit(idproc,nvp)      kstrt = idproc + 1      if (kstrt.eq.1) then         open(unit=6,file='output2',form='formatted',status='unknown')      endifc initialize timer      call timera(-1,'total   ',time)c initialize constants      itime = 0      qtme = qme*dt      affp = float(nx*ny)/float(np)      zero = 0.c calculate partition variables      call dcomp2(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c prepare fft tables      isign = 0      call pfft2r(qc,qt,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrt,nxv     1h,nyv,kxp,kyp,jblok,kblok,nxhy,nxyh)c calculate form factors      call ppois2(qt,fxt,fyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp     1,jblok,nyh)c initialize density profile and velocity distributionc background electrons      do 130 l = 1, nblok      nps(l) = 1      npp(l) = 0      nyp3 = nyp(l) + 3      do 120 k = 1, nyp3c     do 120 k = 1, nypmx      do 110 j = 1, nxc     if (k.le.nyp3) then      q(j,k,l) = 0.c     endif  110 continue  120 continue  130 continue      if (npxy.gt.0) call pistr2 (part,edges,npp,nps,vtx,vty,zero,zero,n     1px,npy,nx,ny,idimp,npmax,nblok,idps)c beam electrons      do 140 l = 1, nblok      nps(l) = npp(l) + 1  140 continue      if (npxyb.gt.0) call pistr2 (part,edges,npp,nps,vtdx,vtdy,vdx,vdy,     1npxb,npyb,nx,ny,idimp,npmax,nblok,idps)c deposit charge for initial distribution      call pdost2(part,q,npp,noff,qme,nx,idimp,npmax,nblok,nxv,nypmx)c     call psost2x(part,q,npp,noff,nn,amxy,qme,nx,idimp,npmax,nblok,nxv,c    1nxvyp,npd,nine)c add background ion density      qi0 = -qme/affp      do 240 l = 1, nblok      do 230 k = 1, nyp(l)c     do 230 k = 1, nypmx      do 220 j = 1, nxc     if (k.le.nyp(l)) then      q(j,k+1,l) = q(j,k+1,l) + qi0c     endif  220 continue  230 continue  240 continuecc * * * start main iteration loop * * *c  500 if (nloop.le.itime) go to 2000      if (kstrt.eq.1) write (6,991) itime      isign = -1c copy data from particle to field partition, and add up guard cells      call cppfp2 (q,qc,isign,scr,kstrt,nvp,nx,nxv,nypmx,nblok,nxvh,kyp,     1kblok,idps)c transform charge to fourier space      call pfft2r(qc,qt,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrt,nxv     1h,nyv,kxp,kyp,jblok,kblok,nxhy,nxyh)c calculate force/charge in fourier space      call ppois2(qt,fxt,fyt,isign,ffc,ax,ay,affp,we,nx,ny,kstrt,nyv,kxp     1,jblok,nyh)c transform force/charge to real space      isign = 1      call pfft2r(fxc,fxt,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrt,n     1xvh,nyv,kxp,kyp,jblok,kblok,nxhy,nxyh)      call pfft2r(fyc,fyt,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrt,n     1xvh,nyv,kxp,kyp,jblok,kblok,nxhy,nxyh)c copy data from field to particle partition, and copy to guard cells      call cppfp2 (fx,fxc,isign,scr,kstrt,nvp,nx,nxv,nypmx,nblok,nxvh,ky     1p,kblok,idps)      call cppfp2 (fy,fyc,isign,scr,kstrt,nvp,nx,nxv,nypmx,nblok,nxvh,ky     1p,kblok,idps)c particle push and charge density updatec     call timera(-1,'push    ',time)c initialize charge density to zero      do 1250 l = 1, nblok      nyp3 = nyp(l) + 3      do 1240 k = 1, nyp3c     do 1240 k = 1, nypmx      do 1230 j = 1, nxc     if (k.le.nyp3) then      q(j,k,l) = 0.c     endif 1230 continue 1240 continue 1250 continue      wke = 0.c push particles      call ppush2 (part,fx,fy,npp,noff,qtme,dt,wke,nx,idimp,npmax,nblok,     1nxv,nypmx)c move particles into appropriate spatial regions      call pmove2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,     1jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c     call pxmov2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,jsr,jsl,c    1jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ierr)      if (ierr.ne.0) then         call ppexit         stop      endifc deposit charge      call pdost2(part,q,npp,noff,qme,nx,idimp,npmax,nblok,nxv,nypmx)c     call psost2x(part,q,npp,noff,nn,amxy,qme,nx,idimp,npmax,nblok,nxv,c    1nxvyp,npd,nine)c add background charge      do 1280 l = 1, nblok      do 1270 k = 1, nyp(l)c     do 1270 k = 1, nypmx      do 1260 j = 1, nxc     if (k.le.nyp(l)) then      q(j,k+1,l) = q(j,k+1,l) + qi0c     endif 1260 continue 1270 continue 1280 continuec     call timera(1,'push    ',time)c energy diagnostic      wt = we + wke      if (kstrt.eq.1) write (6,993) we, wke, wt      itime = itime + 1      go to 500 2000 continuecc * * * end main iteration loop * * *c      if (kstrt.eq.1) write (6,992)      call timera(1,'total   ',time)      call ppexit      stop      endc-----------------------------------------------------------------------      subroutine ppinit(idproc,nvp)c this subroutine initializes parallel processingc input: nvp, output: idprocc idproc = processor idc nvp = number of real or virtual processors requested      implicit none      integer idproc, nvpc get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for realsc mint = default datatype for integersc mcplx = default datatype for complex type      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ierror, ndprec      save /pparms/c ndprec = (0,1) = (no,yes) use (normal,autodouble) precision      data ndprec /1/c this segment is used for shared memory computersc     nproc = nvpc     idproc = 0c this segment is used for mpi computers      if (MPI_STATUS_SIZE.gt.lstat) then         write (2,*) ' status size too small, actual/required = ', lstat     1, MPI_STATUS_SIZE         stop      endifc initialize the MPI execution environment      call MPI_INIT(ierror)      if (ierror.ne.0) stop      lgrp = MPI_COMM_WORLDc determine the rank of the calling process in the communicator      call MPI_COMM_RANK(lgrp,idproc,ierror)c determine the size of the group associated with a communicator      call MPI_COMM_SIZE(lgrp,nproc,ierror)c set default datatypes         mint = MPI_INTEGERc single precision      if (ndprec.eq.0) then         mreal = MPI_REAL         mcplx = MPI_COMPLEXc double precision      else         mreal = MPI_DOUBLE_PRECISION         mcplx = MPI_DOUBLE_COMPLEX      endifc requested number of processors not obtained      if (nproc.ne.nvp) then         write (2,*) ' processor number error: nvp, nproc=', nvp, nproc         call ppexit         stop      endif      return      endc-----------------------------------------------------------------------      subroutine ppexitc this subroutine terminates parallel processing      implicit nonec common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplxc lgrp = current communicator      common /pparms/ nproc, lgrp, mreal, mint, mcplx      integer ierrorc synchronize processes      call MPI_BARRIER(lgrp,ierror)c terminate MPI execution environment      call MPI_FINALIZE(ierror)      return      endc-----------------------------------------------------------------------      subroutine dcomp2(edges,nyp,noff,ny,kstrt,nvp,idps,nblok)c this subroutine determines spatial boundaries for particlec decomposition, calculates number of grid points in each spatialc region, and the offset of these grid points from the global addressc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc nyp(l) = number of primary gridpoints in particle partition l.c noff(l) = lowermost global gridpoint in particle partition l.c ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idps = number of partition boundariesc nblok = number of particle partitions.      implicit none      real edges      integer nyp, noff, ny, kstrt, nvp, idps, nblok      dimension edges(idps,nblok)      dimension nyp(nblok), noff(nblok)c local data      integer ks, kb, kr, l      real at1      ks = kstrt - 2      at1 = float(ny)/float(nvp)      do 10 l = 1, nblok      kb = l + ks      edges(1,l) = at1*float(kb)      noff(l) = edges(1,l) + .5      edges(2,l) = at1*float(kb + 1)      kr = edges(2,l) + .5      nyp(l) = kr - noff(l)   10 continue      return      endc-----------------------------------------------------------------------      subroutine pistr2 (part,edges,npp,nps,vtx,vty,vdx,vdy,npx,npy,nx,n     1y,idimp,npmax,nblok,idps)c for 2d code, this subroutine calculates initial particle co-ordinatesc and velocities with uniform density and maxwellian velocity with driftc for distributed data.c part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc nps(l) = starting address of particles in partition lc vtx/vty = thermal velocity of electrons in x/y directionc vdx/vdy = drift velocity of beam electrons in x/y directionc npx/npy = initial number of particles distributed in x/y directionc nx/ny = system length in x/y directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc ranorm = gaussian random number with zero mean and unit variancec with spatial decomposition      double precision ranorm      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok), nps(nblok)      dimension sum2(2), work2(2)      npxy = npx*npy      at1 = float(nx)/float(npx)      at2 = float(ny)/float(npy)      do 30 k = 1, npy      yt = at2*(float(k) - .5)      do 20 j = 1, npxc uniform density profile      xt = at1*(float(j) - .5)c maxwellian velocity distribution      vxt = vtx*ranorm(d)      vyt = vty*ranorm(d)      do 10 l = 1, nblok      if ((yt.ge.edges(1,l)).and.(yt.lt.edges(2,l))) then         npt = npp(l) + 1         part(1,npt,l) = xt         part(2,npt,l) = yt         part(3,npt,l) = vxt         part(4,npt,l) = vyt         npp(l) = npt      endif   10 continue   20 continue   30 continuec add correct drift      sum2(1) = 0.      sum2(2) = 0.      do 50 l = 1, nblok      sum0 = 0.      sum1 = 0.      do 40 j = nps(l), npp(l)c     do 40 j = 1, npmaxc     if ((j.ge.nps(l)).and.(j.le.npp(l))) then      sum0 = sum0 + part(3,j,l)      sum1 = sum1 + part(4,j,l)c     endif   40 continue      sum2(1) = sum2(1) + sum0      sum2(2) = sum2(2) + sum1   50 continue      call psum(sum2,work2,2,1)      at1 = 1./float(npxy)      sum2(1) = at1*sum2(1) - vdx      sum2(2) = at1*sum2(2) - vdy      do 70 l = 1, nblok      do 60 j = nps(l), npp(l)c     do 60 j = 1, npmaxc     if ((j.ge.nps(l)).and.(j.le.npp(l))) then      part(3,j,l) = part(3,j,l) - sum2(1)      part(4,j,l) = part(4,j,l) - sum2(2)c     endif   60 continue   70 continue      return      endc-----------------------------------------------------------------------      subroutine pdost2(part,q,npp,noff,qm,nx,idimp,npmax,nblok,nxv,nypm     1x)c for 2d code, this subroutine calculates particle charge densityc using second-order spline interpolation, periodic boundariesc and distributed data.c charge density is approximated by values at the nearest grid pointsc q(n,m)=qm*(.75-dx**2)*(.75-dy**2)c q(n+1,m)=.5*qm*((.5+dx)**2)*(.75-dy**2)c q(n-1,m)=.5*qm*((.5-dx)**2)*(.75-dy**2)c q(n,m+1)=.5*qm*(.75-dx**2)*(.5+dy)**2c q(n+1,m+1)=.25*qm*((.5+dx)**2)*(.5+dy)**2c q(n-1,m+1)=.25*qm*((.5-dx)**2)*(.5+dy)**2c q(n,m-1)=.5*qm*(.75-dx**2)*(.5-dy)**2c q(n+1,m-1)=.25*qm*((.5+dx)**2)*(.5-dy)**2c q(n-1,m-1)=.25*qm*((.5-dx)**2)*(.5-dy)**2c where n,m = nearest grid points and dx = x-n, dy = y-mc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc q(j,k,l) = charge density at grid point (j,kk),c where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c qm = charge on particle, in units of ec nx = system length in x directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nxv = first dimension of field arrays, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c scalar version with spatial decomposition      dimension part(idimp,npmax,nblok), q(nxv,nypmx,nblok)      dimension npp(nblok), noff(nblok)      do 20 l = 1, nblok      mnoff = noff(l) - 2c find interpolation weights      do 10 j = 1, npp(l)c     do 10 j = 1, npmaxc     if (j.le.npp(l)) then      nn = part(1,j,l) + .5      dxl = part(1,j,l) - float(nn)      nn = nn + 1      if (nn.gt.nx) nn = nn - nx      amx = qm*(.75 - dxl*dxl)      np = nn + 1      if (np.gt.nx) np = np - nx      dxp = .5*qm*(.5 + dxl)**2      nl = nn - 1      if (nl.lt.1) nl = nl + nx      dxl = .5*qm*(.5 - dxl)**2      mm = part(2,j,l) + .5      dyl = part(2,j,l) - float(mm)      mm = mm - mnoff      amy = .75 - dyl*dyl      mp = mm + 1      dyp = .5*(.5 + dyl)**2      ml = mm - 1      dyl = .5*(.5 - dyl)**2c deposit charge      q(nn,mm,l) = q(nn,mm,l) + amx*amy      q(np,mm,l) = q(np,mm,l) + dxp*amy      q(nl,mm,l) = q(nl,mm,l) + dxl*amy      q(nn,mp,l) = q(nn,mp,l) + amx*dyp      q(np,mp,l) = q(np,mp,l) + dxp*dyp      q(nl,mp,l) = q(nl,mp,l) + dxl*dyp      q(nn,ml,l) = q(nn,ml,l) + amx*dyl      q(np,ml,l) = q(np,ml,l) + dxp*dyl      q(nl,ml,l) = q(nl,ml,l) + dxl*dylc     endif   10 continue   20 continue      return      endc-----------------------------------------------------------------------      subroutine psost2x(part,q,npp,noff,nn,amxy,qm,nx,idimp,npmax,nblok     1,nxv,nxvyp,npd,nine)c for 2d code, this subroutine calculates particle charge densityc using second-order spline interpolation, periodic boundaries,c with short vectors over independent weights, and distributed data,c as in j. schwartzmeier and t. hewitt, proc. 12th conf. on numericalc simulation of plasmas, san francisco, ca, 1987.c charge density is approximated by values at the nearest grid pointsc q(n,m)=qm*(.75-dx**2)*(.75-dy**2)c q(n+1,m)=.5*qm*((.5+dx)**2)*(.75-dy**2)c q(n-1,m)=.5*qm*((.5-dx)**2)*(.75-dy**2)c q(n,m+1)=.5*qm*(.75-dx**2)*(.5+dy)**2c q(n+1,m+1)=.25*qm*((.5+dx)**2)*(.5+dy)**2c q(n-1,m+1)=.25*qm*((.5-dx)**2)*(.5+dy)**2c q(n,m-1)=.5*qm*(.75-dx**2)*(.5-dy)**2c q(n+1,m-1)=.25*qm*((.5+dx)**2)*(.5-dy)**2c q(n-1,m-1)=.25*qm*((.5-dx)**2)*(.5-dy)**2c where n,m = nearest grid points and dx = x-n, dy = y-mc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc q(j,k,l) = charge density at grid point (j,kk),c where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c nn = scratch address array for vectorized charge depositionc amxy = scratch weight array for vectorized charge depositionc qm = charge on particle, in units of ec nx = system length in x directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nxv = first dimension of field arrays, must be >= nxc nxvyp = nxv*nypmx, first dimension of charge arrayc npd = size of scratch buffers for vectorized charge depositionc nine = number of independent weightsc version with spatial decomposition and 1d addressing scheme      dimension part(idimp,npmax,nblok), q(nxvyp,nblok)      dimension npp(nblok), noff(nblok)      dimension nn(nine,npd,nblok), amxy(nine,npd,nblok)c find maximum number of blocks      if (npmax.gt.npd) then         ipmax = float(npmax - 1)/float(npd) + 1.      else         ipmax = 1      endifc parallel loop      do 50 l = 1, nblok      mnoff = noff(l) - 1      npb = npd      if (npp(l).gt.npd) then         ipp = float(npp(l) - 1)/float(npd) + 1.      else         ipp = 1      endifc outer loop over blocks of particles      do 40 j = 1, ippc     do 40 j = 1, ipmax      jb = (j - 1)*npd      if (j.ge.ipp) npb = npp(l) - (ipp - 1)*npdc find interpolation weights      do 10 i = 1, npbc     do 10 i = 1, npdc     if ((j.le.ipp).and.(i.le.npb)) then      n = part(1,i+jb,l) + .5      dxl = part(1,i+jb,l) - float(n)      n = n + 1      if (n.gt.nx) n = n - nx      amx = qm*(.75 - dxl*dxl)      np = n + 1      if (np.gt.nx) np = np - nx      dxp = .5*qm*(.5 + dxl)**2      nl = n - 1      if (nl.lt.1) nl = nl + nx      dxl = .5*qm*(.5 - dxl)**2      m = part(2,i+jb,l) + .5      dyl = part(2,i+jb,l) - float(m)      m = nxv*(m - mnoff)      amy = .75 - dyl*dyl      mp = m + nxv      dyp = .5*(.5 + dyl)**2      ml = m - nxv      dyl = .5*(.5 - dyl)**2      nn(1,i,l) = n + m      nn(2,i,l) = np + m      nn(3,i,l) = nl + m      nn(4,i,l) = n + mp      nn(5,i,l) = np + mp      nn(6,i,l) = nl + mp      nn(7,i,l) = n + ml      nn(8,i,l) = np + ml      nn(9,i,l) = nl + ml      amxy(1,i,l) = amx*amy      amxy(2,i,l) = dxp*amy      amxy(3,i,l) = dxl*amy      amxy(4,i,l) = amx*dyp      amxy(5,i,l) = dxp*dyp      amxy(6,i,l) = dxl*dyp      amxy(7,i,l) = amx*dyl      amxy(8,i,l) = dxp*dyl      amxy(9,i,l) = dxl*dylc     endif   10 continuec deposit charge      do 30 i = 1, npbc     do 30 i = 1, npd      do 20 k = 1, 9c     if ((j.le.ipp).and.(i.le.npb)) then      q(nn(k,i,l),l) = q(nn(k,i,l),l) + amxy(k,i,l)c     endif   20 continue   30 continue   40 continue   50 continue      return      endc-----------------------------------------------------------------------      subroutine cppfp2 (f,fc,isign,scr,kstrt,nvp,nx,nxv,nypmx,nblok,nxv     1h,kyp,kblok,idps)c thus subroutine copies data between particle and field partitions,c where the field and particle partitions are assumed to be the same.c in going from particle to field partitions, data from guard cellsc is added to the field partition.  in going from field to particlec partitions, data is copied to the guard cells.c f(j,k,l) = real data for grid j,k in particle partition l.  the numberc grids per partition is uniform and includes three extra guard cells.c fc(j,k,l) = complex data for grid j,k in field partition l.  thec number of grids per partition is uniform and maximum number ofc partitions is ny/2.c if isign = -1, data is copied from particle to field partition.c if isign = 1, data is copied from field to particle partition.c scr(j,idps,k) = scratch array for particle partition kc kstrt = starting data block numberc nvp = number of real or virtual processorsc nx = system length in x directionc nxv = first dimension of f, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c nblok = number of particle partitions, assumed equal to kblok.c nxvh = first dimension of fc, must be >= nx/2c kyp = number of complex grids in each field partition.c kblok = number of field partitions.c idps = number of partition boundaries      implicit none      integer isign      integer kstrt, nvp, nx, nxv, nypmx, nblok, nxvh, kyp, kblok, idps      real f      complex fc, scr      dimension f(nxv,nypmx,nblok), fc(nxvh,kyp,kblok)      dimension scr(nxvh,idps,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer istatus      integer nxh, ks, moff, kr, krr, kl, kll, ngc, j, k, l, ierr, msid      dimension istatus(lstat)      nxh = nx/2      ks = kstrt - 2      moff = nypmx*nvp      if (isign) 10, 100, 110c copy from particle to field partition   10 do 40 l = 1, kblok      do 30 k = 1, kyp      do 20 j = 1, nxh      fc(j,k,l) = cmplx(f(2*j-1,k+1,l),f(2*j,k+1,l))   20 continue   30 continue   40 continuec add guard cells      do 90 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) then         kr = kr - nvp      endif      krr = kr      kl = l + ks      if (kl.lt.1) then         kl = kl + nvp      endif      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) then            krr = krr - nvp         endif         kll = kll - 1         if (kll.lt.1) then            kll = kll + nvp         endif         ngc = 1      endifc first add guard cells from rightc this segment is used for shared memory computersc     do 50 j = 1, nxhc     scr(j,1,l) = cmplx(f(2*j-1,kyp+2,kl),f(2*j,kyp+2,kl))c     scr(j,2,l) = cmplx(f(2*j-1,kyp+3,kll),f(2*j,kyp+3,kll))c  50 continuec this segment is used for mpi computers      call MPI_IRECV(scr,ngc*nxv,mreal,kl-1,moff+1,lgrp,msid,ierr)      call MPI_SEND(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+1,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(scr(1,2,l),ngc*nxv,mreal,kll-1,moff+5,lgrp,msid,     1ierr)         call MPI_SEND(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+5,lgrp,ierr     1)         call MPI_WAIT(msid,istatus,ierr)      endif      do 60 j = 1, nxh      fc(j,1,l) = fc(j,1,l) + scr(j,1,l)      fc(j,ngc,l) = fc(j,ngc,l) + scr(j,2,l)   60 continuec then add guard cells from leftc this segment is used for shared memory computersc     do 70 j = 1, nxhc     scr(j,1,l) = cmplx(f(2*j-1,1,kr),f(2*j,1,kr))c  70 continuec this segment is used for mpi computers      call MPI_IRECV(scr,nxv,mreal,kr-1,moff+2,lgrp,msid,ierr)      call MPI_SEND(f(1,1,l),nxv,mreal,kl-1,moff+2,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      do 80 j = 1, nxh      fc(j,kyp,l) = fc(j,kyp,l) + scr(j,1,l)   80 continue   90 continue  100 returnc copy from field to particle partition  110 do 140 l = 1, kblok      do 130 k = 1, kyp      do 120 j = 1, nxh      f(2*j-1,k+1,l) = real(fc(j,k,l))      f(2*j,k+1,l) = aimag(fc(j,k,l))  120 continue  130 continue  140 continuec copy to guard cells      do 170 l = 1, kblok      kr = l + ks + 2      if (kr.gt.nvp) then         kr = kr - nvp      endif      krr = kr      kl = l + ks      if (kl.lt.1) then         kl = kl + nvp      endif      kll = kl      ngc = 2c special case of only one grid per processor      if (kyp.eq.1) then         krr = krr + 1         if (krr.gt.nvp) then            krr = krr - nvp         endif         kll = kll - 1         if (kll.lt.1) then            kll = kll + nvp         endif         ngc = 1      endifc copy to left guard cellc this segment is used for shared memory computersc     do 150 j = 1, nxhc     f(2*j-1,1,l) = real(fc(j,kyp,kl))c     f(2*j,1,l) = aimag(fc(j,kyp,kl))c 150 continuec this segment is used for mpi computers      call MPI_IRECV(f(1,1,l),nxv,mreal,kl-1,moff+3,lgrp,msid,ierr)      call MPI_SEND(fc(1,kyp,l),nxv,mreal,kr-1,moff+3,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)c copy to right guard cellsc this loop is used for shared memory computersc     do 160 j = 1, nxhc     f(2*j-1,kyp+2,l) = real(fc(j,1,kr))c     f(2*j,kyp+2,l) = aimag(fc(j,1,kr))c     f(2*j-1,kyp+3,l) = real(fc(j,ngc,krr))c     f(2*j,kyp+3,l) = aimag(fc(j,ngc,krr))c 160 continuec this segment is used for mpi computers      call MPI_IRECV(f(1,kyp+2,l),ngc*nxv,mreal,kr-1,moff+4,lgrp,msid,ie     1rr)      call MPI_SEND(fc(1,1,l),ngc*nxv,mreal,kl-1,moff+4,lgrp,ierr)      call MPI_WAIT(msid,istatus,ierr)      if (kyp.eq.1) then         call MPI_IRECV(f(1,kyp+3,l),ngc*nxv,mreal,krr-1,moff+6,lgrp,msi     1d,ierr)         call MPI_SEND(fc(1,1,l),ngc*nxv,mreal,kll-1,moff+6,lgrp,ierr)         call MPI_WAIT(msid,istatus,ierr)      endif  170 continue      return      endc-----------------------------------------------------------------------      subroutine ppois2(q,fx,fy,isign,ffc,ax,ay,affp,wp,nx,ny,kstrt,nyv,     1kxp,jblok,nyhd)c this subroutine solves 2d poisson's equation in fourier space forc force/charge (or convolution of electric field over particle shape)c with periodic boundary conditions, for distributed data.c the equation used is:c fx(kx,ky) = -sqrt(-1)*kx*g(kx,ky)*s(kx,ky),c fy(kx,ky) = -sqrt(-1)*ky*g(kx,ky)*s(kx,ky),c where kx = 2pi*j/nx, ky = 2pi*k/ny, and j,k = fourier mode numbers,c g(kx,ky) = (affp/(kx**2+ky**2))*s(kx,ky),c s(kx,ky) = exp(-((kx*ax)**2+(ky*ay)**2)/2), except forc fx(kx=pi) = fy(kx=pi) = fx(ky=pi) = fy(ky=pi) = 0, andc fx(kx=0,ky=0) = fy(kx=0,ky=0) = 0.c q(k,j,l) = complex charge density for fourier mode (jj-1,k-1)c fx(k,j,l) = x component of complex force/charge,c fy(k,j,l) = y component of complex force/charge,c for fourier mode (jj-1,k-1), where jj = j + kxp*(l - 1)c jblok = number of data blocksc kxp = number of data values per blockc kstrt = starting data block numberc if isign = 0, form factor array is preparedc if isign is not equal to 0, force/charge is calculated.c aimag(ffc(k,j,l)) = finite-size particle shape factor sc real(ffc(k,j,l)) = potential green's function gc for fourier mode (jj-1,k-1), where jj = j + kxp*(l - 1)c ax/ay = half-width of particle in x/y directionc affp = normalization constant = nx*ny/np, where np=number of particlesc electric field energy is also calculated, usingc wp = nx*ny*sum((affp/(kx**2+ky**2))*|q(kx,ky)*s(kx,ky)|**2)c nx/ny = system length in x/y directionc nyv = first dimension of field arrays, must be >= nyc nyhd = first dimension of form factor array, must be >= nyh      complex q, fx, fy, ffc, zero      dimension q(nyv,kxp,jblok)      dimension fx(nyv,kxp,jblok), fy(nyv,kxp,jblok)      dimension ffc(nyhd,kxp,jblok)      nxh = nx/2      nyh = ny/2      ny2 = ny + 2      ks = kstrt - 2      dnx = 6.28318530717959/float(nx)      dny = 6.28318530717959/float(ny)      zero = cmplx(0.,0.)      if (isign.ne.0) go to 40      if (kstrt.gt.nxh) returnc prepare form factor array      do 30 l = 1, jblok      joff = kxp*(l + ks) - 1      do 20 j = 1, kxp      dkx = dnx*float(j + joff)      at1 = dkx*dkx      at2 = (dkx*ax)**2      do 10 k = 1, nyh      dky = dny*float(k - 1)      at3 = dky*dky + at1      at4 = exp(-.5*((dky*ay)**2 + at2))      if (at3.eq.0.) then         ffc(k,j,l) = cmplx(0.,1.)      else         ffc(k,j,l) = cmplx(affp*at4/at3,at4)      endif   10 continue   20 continue   30 continue      returnc calculate force/charge and sum field energy   40 wp = 0.      if (kstrt.gt.nxh) go to 90      do 80 l = 1, jblokc mode numbers 0 < kx < nx/2 and 0 < ky < ny/2      joff = kxp*(l + ks) - 1      do 60 j = 1, kxp      dkx = dnx*float(j + joff)      if ((j+joff).gt.0) then         do 50 k = 2, nyhc        if ((j+joff).gt.0) then         k1 = ny2 - k         at1 = real(ffc(k,j,l))*aimag(ffc(k,j,l))         at2 = dkx*at1         at3 = dny*float(k - 1)*at1         fx(k,j,l) = at2*cmplx(aimag(q(k,j,l)),-real(q(k,j,l)))         fx(k1,j,l) = at2*cmplx(aimag(q(k1,j,l)),-real(q(k1,j,l)))         fy(k,j,l) = at3*cmplx(aimag(q(k,j,l)),-real(q(k,j,l)))         fy(k1,j,l) = at3*cmplx(-aimag(q(k1,j,l)),real(q(k1,j,l)))         wp = wp + at1*(q(k,j,l)*conjg(q(k,j,l)) + q(k1,j,l)*conjg(q(k1,     1j,l)))c        endif   50    continue      endifc mode numbers ky = 0, ny/2      k1 = nyh + 1      at1 = real(ffc(1,j,l))*aimag(ffc(1,j,l))      fx(1,j,l) = dkx*at1*cmplx(aimag(q(1,j,l)),-real(q(1,j,l)))      fx(k1,j,l) = zero      fy(1,j,l) = zero      fy(k1,j,l) = zero      wp = wp + at1*(q(1,j,l)*conjg(q(1,j,l)))   60 continuec mode numbers kx = 0, nx/2      if ((l+ks).eq.0) then         do 70 k = 2, nyhc        if ((l+ks).eq.0) then         k1 = ny2 - k         at1 = real(ffc(k,1,l))*aimag(ffc(k,1,l))         fx(k,1,l) = zero         fx(k1,1,l) = zero         fy(k,1,l) = dny*float(k - 1)*at1*cmplx(aimag(q(k,1,l)),-real(q(     1k,1,l)))         fy(k1,1,l) = zero         wp = wp + at1*(q(k,1,l)*conjg(q(k,1,l)))c        endif   70    continue      endif   80 continue   90 continue      call psum(wp,work1,1,1)      wp = float(nx*ny)*wp      return      endc-----------------------------------------------------------------------      subroutine ppush2 (part,fx,fy,npp,noff,qtm,dt,ek,nx,idimp,npmax,nb     1lok,nxv,nypmx)c for 2d code, this subroutine updates particle co-ordinates andc velocities using leap-frog scheme in time and second-order splinec interpolation in space, with periodic boundary conditions,c for distributed data.  equations used are:c vx(t+dt/2) = vx(t-dt/2) + (q/m)*fx(x(t),y(t))*dt,c vy(t+dt/2) = vy(t-dt/2) + (q/m)*fy(x(t),y(t))*dt,c where q/m is charge/mass, andc x(t+dt) = x(t) + vx(t+dt/2)*dt, y(t+dt) = y(t) + vy(t+dt/2)*dtc fx(x(t),y(t)) and fy(x(t),y(t)) are approximated by interpolation fromc the nearest grid points:c fx(x,y) = (.75-dy**2)*((.75-dx**2)*fx(n,m)+(.5*(.5+dx)**2)*fx(n+1,m)+c (.5*(.5-dx)**2)*fx(n-1,m)) + (.5*(.5+dy)**2)*((.75-dx**2)*fx(n,m+1)+c (.5*(.5+dx)**2)*fx(n+1,m+1)+(.5*(.5-dx)**2)*fx(n-1,m+1)) +c (.5*(.5-dy)**2)*((.75-dx**2)*fx(n,m-1)+(.5*(.5+dx)**2)*fx(n+1,m-1)+c (.5*(.5-dx)**2)*fx(n-1,m-1))c fy(x,y) = (.75-dy**2)*((.75-dx**2)*fy(n,m)+(.5*(.5+dx)**2)*fy(n+1,m)+c (.5*(.5-dx)**2)*fy(n-1,m)) + (.5*(.5+dy)**2)*((.75-dx**2)*fy(n,m+1)+c (.5*(.5+dx)**2)*fy(n+1,m+1)+(.5*(.5-dx)**2)*fy(n-1,m+1)) +c (.5*(.5-dy)**2)*((.75-dx**2)*fy(n,m-1)+(.5*(.5+dx)**2)*fy(n+1,m-1)+c (.5*(.5-dx)**2)*fy(n-1,m-1))c where n,m = nearest grid points and dx = x-n, dy = y-mc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc fx(j,k,l) = x component of force/charge at grid (j,kk)c fy(j,k,l) = y component of force/charge at grid (j,kk)c in other words, fx/fy are the convolutions of the electric fieldc over the particle shape, where kk = k + noff(l) - 1c npp(l) = number of particles in partition lc noff(l) = lowermost global gridpoint in particle partition l.c qtm = particle charge/mass ratio times dtc dt = time interval between successive calculationsc kinetic energy/mass at time t is also calculated, usingc ek = .125*sum((vx(t+dt/2)+vx(t-dt/2))**2+(vy(t+dt/2)+vy(t-dt/2))**2)c nx = system length in x directionc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c nxv = first dimension of field arrays, must be >= nxc nypmx = maximum size of particle partition, including guard cells.c scalar version with spatial decomposition      dimension part(idimp,npmax,nblok)      dimension fx(nxv,nypmx,nblok), fy(nxv,nypmx,nblok)      dimension npp(nblok), noff(nblok)      zero = 0.      anx = float(nx)      sum1 = 0.      do 20 l = 1, nblok      mnoff = noff(l) - 2c find interpolation weights      do 10 j = 1, npp(l)c     do 10 j = 1, npmaxc     if (j.le.npp(l)) then      nn = part(1,j,l) + .5      dx = part(1,j,l) - float(nn)      nn = nn + 1      if (nn.gt.nx) nn = nn - nx      amx = .75 - dx*dx      np = nn + 1      if (np.gt.nx) np = np - nx      dxp = .5*(.5 + dx)**2      nl = nn - 1      if (nl.lt.1) nl = nl + nx      dxl = .5*(.5 - dx)**2      mm = part(2,j,l) + .5      dy = part(2,j,l) - float(mm)      mm = mm - mnoff      amy = .75 - dy*dy      mp = mm + 1      dyp = .5*(.5 + dy)**2      ml = mm - 1      dyl = .5*(.5 - dy)**2c find acceleration      dx = amy*(amx*fx(nn,mm,l) + dxp*fx(np,mm,l) + dxl*fx(nl,mm,l)) + d     1yp*(amx*fx(nn,mp,l) + dxp*fx(np,mp,l) + dxl*fx(nl,mp,l)) + dyl*(am     2x*fx(nn,ml,l)+ dxp*fx(np,ml,l) + dxl*fx(nl,ml,l))      dy = amy*(amx*fy(nn,mm,l) + dxp*fy(np,mm,l) + dxl*fy(nl,mm,l)) + d     1yp*(amx*fy(nn,mp,l) + dxp*fy(np,mp,l) + dxl*fy(nl,mp,l)) + dyl*(am     2x*fy(nn,ml,l)+ dxp*fy(np,ml,l) + dxl*fy(nl,ml,l))c new velocity      dx = part(3,j,l) + qtm*dx      dy = part(4,j,l) + qtm*dyc average kinetic energy      sum1 = sum1 + (dx + part(3,j,l))**2 + (dy + part(4,j,l))**2      part(3,j,l) = dx      part(4,j,l) = dyc new position      dx = part(1,j,l) + dx*dt      dy = part(2,j,l) + dy*dtc periodic boundary conditions      if (dx.lt.zero) dx = dx + anx      if (dx.ge.anx) dx = dx - anx      part(1,j,l) = dx      part(2,j,l) = dyc     endif   10 continue   20 continue      call psum(sum1,work1,1,1)c normalize kinetic energy      ek = ek + .125*sum1      return      endc-----------------------------------------------------------------------      subroutine pmove2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,js     1r,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,ierr)c this subroutine moves particles into appropriate spatial regionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc ierr = (0,1) = (no,yes) error condition exists      implicit none      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, iter, npr, nps, iwork1, kb, ibflg, kl, kr, j, j1, j2      integer l, msid, istatus      real any, yt      dimension istatus(lstat)      any = float(ny)      ks = kstrt - 2      iter = 4c debugging section: count total number of particles before move      npr = 0      do 5 l = 1, nblok      npr = npr + npp(l)    5 continue      call pisum(npr,iwork1,1,1)c buffer outgoing particles   10 do 30 l = 1, nblok      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0      jss(2,l) = 0      do 20 j = 1, npp(l)c     do 20 j = 1, npmaxc     if (j.le.npp(l)) then      yt = part(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            if ((kb+1).eq.nvp) yt = yt - any            sbufr(1,jsr(1,l),l) = part(1,j,l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,j,l)            sbufr(4,jsr(1,l),l) = part(4,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1         endifc particles going down      elseif (yt.lt.edges(1,l)) then         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            if (kb.eq.0) yt = yt + any            sbufl(1,jsl(1,l),l) = part(1,j,l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,j,l)            sbufl(4,jsl(1,l),l) = part(4,j,l)            ihole(jsl(1,l)+jsr(1,l),l) = j         else            jss(2,l) = 1         endif      endifc     endif   20 continue      jss(1,l) = jsl(1,l) + jsr(1,l)   30 continuec check for full buffer condition      ibflg = 0      do 40 l = 1, nblok      ibflg = ibflg + jss(2,l)   40 continue      call pisum(ibflg,iwork1,1,1)c copy particle buffers   50 iter = iter + 4      do 80 l = 1, nblokc get particles from belowc this segment is used for shared memory computersc     kl = l + ksc     if (kl.lt.1) thenc        jsl(2,l) = jsr(1,kl+nvp)c     elsec        jsl(2,l) = jsr(1,kl)c     endifc this segment is used for mpi computers      kr = l + ks + 2      kl = l + ks      if (kl.lt.1) then         call MPI_IRECV(jsl(2,l),1,mint,kl+nvp-1,iter-3,lgrp,msid,ierr)      else         call MPI_IRECV(jsl(2,l),1,mint,kl-1,iter-3,lgrp,msid,ierr)      endif      if (kr.gt.nvp) then         call MPI_SEND(jsr(1,l),1,mint,kr-nvp-1,iter-3,lgrp,ierr)      else         call MPI_SEND(jsr(1,l),1,mint,kr-1,iter-3,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)c this loop is used for shared memory computersc     do 60 j = 1, jsl(2,l)c     do 60 j = 1, nbmaxc     if (j.le.jsl(2,l)) thenc     if (kl.lt.1) thenc        rbufl(1,j,l) = sbufr(1,j,kl+nvp)c        rbufl(2,j,l) = sbufr(2,j,kl+nvp)c        rbufl(3,j,l) = sbufr(3,j,kl+nvp)c        rbufl(4,j,l) = sbufr(4,j,kl+nvp)c     elsec        rbufl(1,j,l) = sbufr(1,j,kl)c        rbufl(2,j,l) = sbufr(2,j,kl)c        rbufl(3,j,l) = sbufr(3,j,kl)c        rbufl(4,j,l) = sbufr(4,j,kl)c     endifc     endifc  60 continuec this segment is used for mpi computers      if (kl.lt.1) then         call MPI_IRECV(rbufl,idimp*jsl(2,l),mreal,kl+nvp-1,iter-2,lgrp,     1msid,ierr)      else         call MPI_IRECV(rbufl,idimp*jsl(2,l),mreal,kl-1,iter-2,lgrp,msid     1,ierr)      endif      if (kr.gt.nvp) then         call MPI_SEND(sbufr,idimp*jsr(1,l),mreal,kr-nvp-1,iter-2,lgrp,i     1err)      else         call MPI_SEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-2,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)c get particles from abovec this segment is used for shared memory computersc     kr = l + ks + 2c     if (kr.gt.nvp) thenc        jsr(2,l) = jsl(1,kr-nvp)c     elsec        jsr(2,l) = jsl(1,kr)c     endifc this segment is used for mpi computers      if (kr.gt.nvp) then         call MPI_IRECV(jsr(2,l),1,mint,kr-nvp-1,iter-1,lgrp,msid,ierr)      else         call MPI_IRECV(jsr(2,l),1,mint,kr-1,iter-1,lgrp,msid,ierr)      endif      if (kl.lt.1) then         call MPI_SEND(jsl(1,l),1,mint,kl+nvp-1,iter-1,lgrp,ierr)      else         call MPI_SEND(jsl(1,l),1,mint,kl-1,iter-1,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)c this loop is used for shared memory computersc     do 70 j = 1, jsr(2,l)c     do 70 j = 1, nbmaxc     if (j.le.jsr(2,l)) thenc     if (kr.gt.nvp) thenc        rbufr(1,j,l) = sbufl(1,j,kr-nvp)c        rbufr(2,j,l) = sbufl(2,j,kr-nvp)c        rbufr(3,j,l) = sbufl(3,j,kr-nvp)c        rbufr(4,j,l) = sbufl(4,j,kr-nvp)c     elsec        rbufr(1,j,l) = sbufl(1,j,kr)c        rbufr(2,j,l) = sbufl(2,j,kr)c        rbufr(3,j,l) = sbufl(3,j,kr)c        rbufr(4,j,l) = sbufl(4,j,kr)c     endifc     endifc  70 continuec this segment is used for mpi computers      if (kr.gt.nvp) then         call MPI_IRECV(rbufr,idimp*jsr(2,l),mreal,kr-nvp-1,iter,lgrp,ms     1id,ierr)      else         call MPI_IRECV(rbufr,idimp*jsr(2,l),mreal,kr-1,iter,lgrp,msid,i     1err)      endif      if (kl.lt.1) then         call MPI_SEND(sbufl,idimp*jsl(1,l),mreal,kl+nvp-1,iter,lgrp,ier     1r)      else         call MPI_SEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   80 continuec check if particles must be passed further      nps = 0      do 110 l = 1, nblokc check if any particles coming from above must go further down      jsl(1,l) = 0      do 90 j = 1, jsr(2,l)c     do 90 j = 1, nbmaxc     if (j.le.jsr(2,l)) then      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = 1c     endif   90 continuec check if any particles coming from below must go further up      jsr(1,l) = 0      do 100 j = 1, jsl(2,l)c     do 100 j = 1, nbmaxc     if (j.le.jsl(2,l)) then      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = 1c     endif  100 continue      nps = nps + (jsl(1,l) + jsr(1,l))  110 continue      call pisum(nps,iwork1,1,1)      if (nps.eq.0) go to 150c remove particles which do not belong here      do 140 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jss(2,l) = 0      do 120 j = 1, jsr(2,l)c     do 120 j = 1, nbmaxc     if (j.le.jsr(2,l)) then      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)      endifc     endif  120 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jsr(1,l) = 0      jss(2,l) = 0      do 130 j = 1, jsl(2,l)c     do 130 j = 1, nbmaxc     if (j.le.jsl(2,l)) then      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufl(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufl(3,j,l)         sbufr(4,jsr(1,l),l) = rbufl(4,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)      endifc     endif  130 continue      jsl(2,l) = jss(2,l)  140 continuec check if move would overflow particle array  150 ierr = 0      do 160 l = 1, nblok      if ((npp(l)+jsl(2,l)+jsr(2,l)-jss(1,l)).gt.npmax) then         jss(2,l) = 1      else         jss(2,l) = 0      endif      ierr = ierr + jss(2,l)  160 continue      call pisum(ierr,iwork1,1,1)      if (ierr.gt.0) then         write (6,*) 'particle overflow error, ierr =', ierr         return      endif      do 200 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 170 j = 1, jss(2,l)c     do 170 j = 1, nbmaxc     if (j.le.jss(2,l)) then      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)c     endif  170 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 180 j = 1, jss(2,l)c     do 180 j = 1, nbmaxc     if (j.le.jss(2,l)) thenc no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)      endifc     endif  180 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 190 j = 1, jsr(2,l)c     do 190 j = 1, ntmaxc     if (j.le.jsr(2,l)) thenc holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)      endifc     endif  190 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  200 continuec check if any particles have to be passed further      if (nps.gt.0) go to 50c check if buffer overflowed and more particles remain to be checked      if (ibflg.gt.0) go to 10c debugging section: count total number of particles after move      nps = 0      do 205 l = 1, nblok      nps = nps + npp(l)  205 continue      call pisum(nps,iwork1,1,1)      if (npr.ne.nps) write (6,*) 'number error, np-old,np-new=',npr,nps      return      endc-----------------------------------------------------------------------      subroutine pxmov2 (part,edges,npp,sbufr,sbufl,rbufr,rbufl,ihole,js     1r,jsl,jss,ny,kstrt,nvp,idimp,npmax,nblok,idps,nbmax,ntmax,maskp,ie     2rr)c this subroutine moves particles into appropriate spatial regionsc part(1,n,l) = position x of particle n in partition lc part(2,n,l) = position y of particle n in partition lc part(3,n,l) = velocity vx of particle n in partition lc part(4,n,l) = velocity vy of particle n in partition lc edges(1,l) = lower boundary of particle partition lc edges(2,l) = upper boundary of particle partition lc npp(l) = number of particles in partition lc sbufl = buffer for particles being sent to lower processorc sbufr = buffer for particles being sent to upper processorc rbufl = buffer for particles being received from lower processorc rbufr = buffer for particles being received from upper processorc ihole = location of holes left in particle arraysc jsl(idps,l) = number of particles going down in particle partition lc jsr(idps,l) = number of particles going up in particle partition lc jss(idps,l) = scratch array for particle partition lc ny = system length in y directionc kstrt = starting data block numberc nvp = number of real or virtual processorsc idimp = size of phase space = 4c npmax = maximum number of particles in each partitionc nblok = number of particle partitions.c idps = number of partition boundariesc nbmax =  size of buffers for passing particles between processorsc ntmax =  size of hole array for particles leaving processorsc maskp = scratch array for particle addressesc ierr = (0,1) = (no,yes) error condition existsc optimized for vector processor      real part, edges, sbufr, sbufl, rbufr, rbufl      integer npp, ihole, jsr, jsl, jss, maskp, ierr      integer ny, kstrt, nvp, idimp, npmax, nblok, idps, nbmax, ntmax      dimension part(idimp,npmax,nblok)      dimension maskp(npmax,nblok)      dimension edges(idps,nblok), npp(nblok)      dimension sbufl(idimp,nbmax,nblok), sbufr(idimp,nbmax,nblok)      dimension rbufl(idimp,nbmax,nblok), rbufr(idimp,nbmax,nblok)      dimension jsl(idps,nblok), jsr(idps,nblok), jss(idps,nblok)      dimension ihole(ntmax,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mint = default datatype for integersc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer istatus      dimension istatus(lstat)      any = float(ny)      ks = kstrt - 2      iter = 4c debugging section: count total number of particles before move      npr = 0      do 5 l = 1, nblok      npr = npr + npp(l)    5 continue      call pisum(npr,iwork1,1,1)c buffer outgoing particles   10 do 60 l = 1, nblok      jss(1,l) = 0      jss(2,l) = 0c find mask function for particles out of bounds      do 20 j = 1, npp(l)c     do 20 j = 1, npmaxc     if (j.le.npp(l)) then      yt = part(2,j,l)      if ((yt.ge.edges(2,l)).or.(yt.lt.edges(1,l))) then         jss(1,l) = jss(1,l) + 1         maskp(j,l) = 1      else         maskp(j,l) = 0      endifc     endif   20 continuec set flag if hole buffer would overflow      if (jss(1,l).gt.ntmax) then         jss(1,l) = ntmax         jss(2,l) = 1      endifc accumulate location of holes      do 30 j = 2, npp(l)c     do 30 j = 2, npmaxc     if (j.le.npp(l)) then      maskp(j,l) = maskp(j,l) + maskp(j-1,l)c     endif   30 continuec store addresses of particles out of bounds      do 40 j = 2, npp(l)c     do 40 j = 2, npmaxc     if (j.le.npp(l)) then      if ((maskp(j,l).gt.maskp(j-1,l)).and.(maskp(j,l).le.ntmax)) then         ihole(maskp(j,l),l) = j      endifc     endif   40 continue      if (maskp(1,l).gt.0) ihole(1,l) = 1      kb = l + ks      jsl(1,l) = 0      jsr(1,l) = 0c load particle buffers      do 50 j = 1, jss(1,l)c     do 50 j = 1, ntmaxc     if (j.le.jss(1,l)) then      yt = part(2,ihole(j,l),l)c particles going to the right      if (yt.ge.edges(2,l)) then         if ((kb+1).eq.nvp) yt = yt - any         if (jsr(1,l).lt.nbmax) then            jsr(1,l) = jsr(1,l) + 1            sbufr(1,jsr(1,l),l) = part(1,ihole(j,l),l)            sbufr(2,jsr(1,l),l) = yt            sbufr(3,jsr(1,l),l) = part(3,ihole(j,l),l)            sbufr(4,jsr(1,l),l) = part(4,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1         endifc particles going to the left      else         if (kb.eq.0) yt = yt + any         if (jsl(1,l).lt.nbmax) then            jsl(1,l) = jsl(1,l) + 1            sbufl(1,jsl(1,l),l) = part(1,ihole(j,l),l)            sbufl(2,jsl(1,l),l) = yt            sbufl(3,jsl(1,l),l) = part(3,ihole(j,l),l)            sbufl(4,jsl(1,l),l) = part(4,ihole(j,l),l)            ihole(jsl(1,l)+jsr(1,l),l) = ihole(j,l)         else            jss(2,l) = 1         endif      endifc     endif   50 continue      jss(1,l) = jsl(1,l) + jsr(1,l)   60 continuec check for full buffer condition      ibflg = 0      do 70 l = 1, nblok      ibflg = ibflg + jss(2,l)   70 continue      call pisum(ibflg,iwork1,1,1)c copy particle buffers   80 iter = iter + 4      do 110 l = 1, nblokc get particles from belowc this segment is used for shared memory computersc     kl = l + ksc     if (kl.lt.1) thenc        jsl(2,l) = jsr(1,kl+nvp)c     elsec        jsl(2,l) = jsr(1,kl)c     endifc this segment is used for mpi computers      kr = l + ks + 2      kl = l + ks      if (kl.lt.1) then         call MPI_IRECV(jsl(2,l),1,mint,kl+nvp-1,iter-3,lgrp,msid,ierr)      else         call MPI_IRECV(jsl(2,l),1,mint,kl-1,iter-3,lgrp,msid,ierr)      endif      if (kr.gt.nvp) then         call MPI_SEND(jsr(1,l),1,mint,kr-nvp-1,iter-3,lgrp,ierr)      else         call MPI_SEND(jsr(1,l),1,mint,kr-1,iter-3,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)c this loop is used for shared memory computersc     do 90 j = 1, jsl(2,l)c     do 90 j = 1, nbmaxc     if (j.le.jsl(2,l)) thenc     if (kl.lt.1) thenc        rbufl(1,j,l) = sbufr(1,j,kl+nvp)c        rbufl(2,j,l) = sbufr(2,j,kl+nvp)c        rbufl(3,j,l) = sbufr(3,j,kl+nvp)c        rbufl(4,j,l) = sbufr(4,j,kl+nvp)c     elsec        rbufl(1,j,l) = sbufr(1,j,kl)c        rbufl(2,j,l) = sbufr(2,j,kl)c        rbufl(3,j,l) = sbufr(3,j,kl)c        rbufl(4,j,l) = sbufr(4,j,kl)c     endifc     endifc  90 continuec this segment is used for mpi computers      if (kl.lt.1) then         call MPI_IRECV(rbufl,idimp*jsl(2,l),mreal,kl+nvp-1,iter-2,lgrp,     1msid,ierr)      else         call MPI_IRECV(rbufl,idimp*jsl(2,l),mreal,kl-1,iter-2,lgrp,msid     1,ierr)      endif      if (kr.gt.nvp) then         call MPI_SEND(sbufr,idimp*jsr(1,l),mreal,kr-nvp-1,iter-2,lgrp,i     1err)      else         call MPI_SEND(sbufr,idimp*jsr(1,l),mreal,kr-1,iter-2,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)c get particles from abovec this segment is used for shared memory computersc     kr = l + ks + 2c     if (kr.gt.nvp) thenc        jsr(2,l) = jsl(1,kr-nvp)c     elsec        jsr(2,l) = jsl(1,kr)c     endifc this segment is used for mpi computers      if (kr.gt.nvp) then         call MPI_IRECV(jsr(2,l),1,mint,kr-nvp-1,iter-1,lgrp,msid,ierr)      else         call MPI_IRECV(jsr(2,l),1,mint,kr-1,iter-1,lgrp,msid,ierr)      endif      if (kl.lt.1) then         call MPI_SEND(jsl(1,l),1,mint,kl+nvp-1,iter-1,lgrp,ierr)      else         call MPI_SEND(jsl(1,l),1,mint,kl-1,iter-1,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)c this loop is used for shared memory computersc     do 100 j = 1, jsr(2,l)c     do 100 j = 1, nbmaxc     if (j.le.jsr(2,l)) thenc     if (kr.gt.nvp) thenc        rbufr(1,j,l) = sbufl(1,j,kr-nvp)c        rbufr(2,j,l) = sbufl(2,j,kr-nvp)c        rbufr(3,j,l) = sbufl(3,j,kr-nvp)c        rbufr(4,j,l) = sbufl(4,j,kr-nvp)c     elsec        rbufr(1,j,l) = sbufl(1,j,kr)c        rbufr(2,j,l) = sbufl(2,j,kr)c        rbufr(3,j,l) = sbufl(3,j,kr)c        rbufr(4,j,l) = sbufl(4,j,kr)c     endifc     endifc 100 continuec this segment is used for mpi computers      if (kr.gt.nvp) then         call MPI_IRECV(rbufr,idimp*jsr(2,l),mreal,kr-nvp-1,iter,lgrp,ms     1id,ierr)      else         call MPI_IRECV(rbufr,idimp*jsr(2,l),mreal,kr-1,iter,lgrp,msid,i     1err)      endif      if (kl.lt.1) then         call MPI_SEND(sbufl,idimp*jsl(1,l),mreal,kl+nvp-1,iter,lgrp,ier     1r)      else         call MPI_SEND(sbufl,idimp*jsl(1,l),mreal,kl-1,iter,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)  110 continuec check if particles must be passed further      nps = 0      do 140 l = 1, nblokc check if any particles coming from above must go further down      jsl(1,l) = 0      do 120 j = 1, jsr(2,l)c     do 120 j = 1, nbmaxc     if (j.le.jsr(2,l)) then      if (rbufr(2,j,l).lt.edges(1,l)) jsl(1,l) = 1c     endif  120 continuec check if any particles coming from below must go further up      jsr(1,l) = 0      do 130 j = 1, jsl(2,l)c     do 130 j = 1, nbmaxc     if (j.le.jsl(2,l)) then      if (rbufl(2,j,l).ge.edges(2,l)) jsr(1,l) = 1c     endif  130 continue      nps = nps + (jsl(1,l) + jsr(1,l))  140 continue      call pisum(nps,iwork1,1,1)      if (nps.eq.0) go to 180c remove particles which do not belong here      do 170 l = 1, nblok      kb = l + ksc first check particles coming from above      jsl(1,l) = 0      jss(2,l) = 0      do 150 j = 1, jsr(2,l)c     do 150 j = 1, nbmaxc     if (j.le.jsr(2,l)) then      yt = rbufr(2,j,l)c particles going down      if (yt.lt.edges(1,l)) then         jsl(1,l) = jsl(1,l) + 1         if (kb.eq.0) yt = yt + any         sbufl(1,jsl(1,l),l) = rbufr(1,j,l)         sbufl(2,jsl(1,l),l) = yt         sbufl(3,jsl(1,l),l) = rbufr(3,j,l)         sbufl(4,jsl(1,l),l) = rbufr(4,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufr(1,jss(2,l),l) = rbufr(1,j,l)         rbufr(2,jss(2,l),l) = yt         rbufr(3,jss(2,l),l) = rbufr(3,j,l)         rbufr(4,jss(2,l),l) = rbufr(4,j,l)      endifc     endif  150 continue      jsr(2,l) = jss(2,l)c next check particles coming from below      jsr(1,l) = 0      jss(2,l) = 0      do 160 j = 1, jsl(2,l)c     do 160 j = 1, nbmaxc     if (j.le.jsl(2,l)) then      yt = rbufl(2,j,l)c particles going up      if (yt.ge.edges(2,l)) then         jsr(1,l) = jsr(1,l) + 1         if ((kb+1).eq.nvp) yt = yt - any         sbufr(1,jsr(1,l),l) = rbufl(1,j,l)         sbufr(2,jsr(1,l),l) = yt         sbufr(3,jsr(1,l),l) = rbufl(3,j,l)         sbufr(4,jsr(1,l),l) = rbufl(4,j,l)c particles staying here      else         jss(2,l) = jss(2,l) + 1         rbufl(1,jss(2,l),l) = rbufl(1,j,l)         rbufl(2,jss(2,l),l) = yt         rbufl(3,jss(2,l),l) = rbufl(3,j,l)         rbufl(4,jss(2,l),l) = rbufl(4,j,l)      endifc     endif  160 continue      jsl(2,l) = jss(2,l)  170 continuec check if move would overflow particle array  180 ierr = 0      do 190 l = 1, nblok      if ((npp(l)+jsl(2,l)+jsr(2,l)-jss(1,l)).gt.npmax) then         jss(2,l) = 1      else         jss(2,l) = 0      endif      ierr = ierr + jss(2,l)  190 continue      call pisum(ierr,iwork1,1,1)      if (ierr.gt.0) then         write (6,*) 'particle overflow error, ierr =', ierr         return      endif      do 230 l = 1, nblokc distribute incoming particles from buffersc distribute particles coming from below into holes      jss(2,l) = min0(jss(1,l),jsl(2,l))      do 200 j = 1, jss(2,l)c     do 200 j = 1, nbmaxc     if (j.le.jss(2,l)) then      part(1,ihole(j,l),l) = rbufl(1,j,l)      part(2,ihole(j,l),l) = rbufl(2,j,l)      part(3,ihole(j,l),l) = rbufl(3,j,l)      part(4,ihole(j,l),l) = rbufl(4,j,l)c     endif  200 continue      if (jss(1,l).gt.jsl(2,l)) then         jss(2,l) = min0(jss(1,l)-jsl(2,l),jsr(2,l))      else         jss(2,l) = jsl(2,l) - jss(1,l)      endif      do 210 j = 1, jss(2,l)c     do 210 j = 1, nbmaxc     if (j.le.jss(2,l)) thenc no more particles coming from belowc distribute particles coming from above into holes      if (jss(1,l).gt.jsl(2,l)) then         part(1,ihole(j+jsl(2,l),l),l) = rbufr(1,j,l)         part(2,ihole(j+jsl(2,l),l),l) = rbufr(2,j,l)         part(3,ihole(j+jsl(2,l),l),l) = rbufr(3,j,l)         part(4,ihole(j+jsl(2,l),l),l) = rbufr(4,j,l)      elsec no more holesc distribute remaining particles from below into bottom         part(1,j+npp(l),l) = rbufl(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufl(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufl(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufl(4,j+jss(1,l),l)      endifc     endif  210 continue      if (jss(1,l).le.jsl(2,l)) then         npp(l) = npp(l) + (jsl(2,l) - jss(1,l))         jss(1,l) = jsl(2,l)      endif      jss(2,l) = jss(1,l) - (jsl(2,l) + jsr(2,l))      if (jss(2,l).gt.0) then         jss(1,l) = (jsl(2,l) + jsr(2,l))         jsr(2,l) = jss(2,l)      else         jss(1,l) = jss(1,l) - jsl(2,l)         jsr(2,l) = -jss(2,l)      endif      do 220 j = 1, jsr(2,l)c     do 220 j = 1, ntmaxc     if (j.le.jsr(2,l)) thenc holes left overc fill up remaining holes in particle array with particles from bottom      if (jss(2,l).gt.0) then         j1 = npp(l) - j + 1         j2 = jss(1,l) + jss(2,l) - j + 1         if (j1.gt.ihole(j2,l)) thenc move particle only if it is below current hole            part(1,ihole(j2,l),l) = part(1,j1,l)            part(2,ihole(j2,l),l) = part(2,j1,l)            part(3,ihole(j2,l),l) = part(3,j1,l)            part(4,ihole(j2,l),l) = part(4,j1,l)         endif      elsec no more holesc distribute remaining particles from above into bottom         part(1,j+npp(l),l) = rbufr(1,j+jss(1,l),l)         part(2,j+npp(l),l) = rbufr(2,j+jss(1,l),l)         part(3,j+npp(l),l) = rbufr(3,j+jss(1,l),l)         part(4,j+npp(l),l) = rbufr(4,j+jss(1,l),l)      endifc     endif  220 continue      if (jss(2,l).gt.0) then         npp(l) = npp(l) - jsr(2,l)      else         npp(l) = npp(l) + jsr(2,l)      endif      jss(1,l) = 0  230 continuec check if any particles have to be passed further      if (nps.gt.0) go to 80c check if buffer overflowed and more particles remain to be checked      if (ibflg.gt.0) go to 10c debugging section: count total number of particles after move      nps = 0      do 235 l = 1, nblok      nps = nps + npp(l)  235 continue      call pisum(nps,iwork1,1,1)      if (npr.ne.nps) write (6,*) 'number error, np-old,np-new=',npr,nps      return      endc-----------------------------------------------------------------------      subroutine pfft2r(f,g,bs,br,isign,ntpose,mixup,sct,indx,indy,kstrt     1,nxvh,nyv,kxp,kyp,jblok,kblok,nxhyd,nxyhd)c this subroutine performs a two dimensional real to complex fastc fourier transform and its inverse, using complex arithmetic,c for data which is distributed in blocksc indx/indy = exponent which determines length in x/y direction,c where nx=2**indx, ny=2**indyc ntpose = (0,1) = (no,yes) input, output data are transposedc if isign = 0, the fft tables are preparedc if isign = -1, an inverse fourier transform is performedc if ntpose = 0, f is the input and output array, g is a scratch arrayc f(n,m,l) = (1/nx*ny)*sum(f(j,k,i)*c       exp(-sqrt(-1)*2pi*n*j/nx)*exp(-sqrt(-1)*2pi*mm*kk/ny))c where mm = m + kyp*(l - 1) and kk = k + kyp*(i - 1)c if ntpose = 1, f is the input and g is the outputc g(m,n,l) = (1/nx*ny)*sum(f(j,k,i)*c       exp(-sqrt(-1)*2pi*nn*j/nx)*exp(-sqrt(-1)*2pi*m*kk/ny))c where nn = n + kxp*(l - 1) and kk = k + kyp*(i - 1)c if isign = 1, a forward fourier transform is performedc if ntpose = 0, f is the input and output array, g is a scratch arrayc f(j,k,i) = sum(f(n,m,l)*exp(sqrt(-1)*2pi*n*j/nx)*c       exp(sqrt(-1)*2pi*mm*kk/ny))c where mm = m + kyp*(l - 1) and kk = k + kyp*(i - 1)c if ntpose = 1, g is the input and f is the outputc f(j,k,i) = sum(g(m,n,l)*exp(sqrt(-1)*2pi*nn*j/nx)*c       exp(sqrt(-1)*2pi*m*kk/ny))c where nn = n + kxp*(l - 1) and kk = k + kyp*(i - 1)c bs, br = scratch arraysc kstrt = starting data block numberc nxvh/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/yc mixup = array of bit reversed addressesc sct = sine/cosine tablec nxhyd = maximum of (nx/2,ny)c nxyhd = one half of maximum of (nx,ny)c the real data is stored in a complex array of length nx/2, nyc with the odd/even x points stored in the real/imaginary parts.c in complex notation, fourier coefficients are stored as follows:c if ntpose = 0,c f(j,k,i) = mode j-1,kk-1, where kk = k + kyp*(i - 1)c 1 <= j <= nx/2 and 1 <= kk <= ny, except forc f(1,k,i) = mode nx/2,kk-1, where ny/2+2 <= kk <= ny, andc imaginary part of f(1,1,1) = real part of mode nx/2,0 andc imaginary part of f(1,1,(ny/2)/kyp+1) = real part of mode nx/2,ny/2c if ntpose = 1,c g(k,j,i) = mode jj-1,k-1, where jj = j + kxp*(i - 1)c 1 <= jj <= nx/2 and 1 <= k <= ny, except forc g(k,1,1) = mode nx/2,k-1, where ny/2+2 <= k <= ny, andc imaginary part of g(1,1,1) = real part of mode nx/2,0 andc imaginary part of g(ny/2+1,1,1) = real part of mode nx/2,ny/2c written by viktor k. decyk, uclac parallel version      implicit none      integer isign, ntpose, indx, indy, kstrt, nxvh, nyv, kxp, kyp      integer jblok, kblok, nxhyd, nxyhd, mixup      complex f, g, bs, br, sct      dimension f(nxvh,kyp,kblok), g(nyv,kxp,jblok)      dimension bs(kxp,kyp,kblok), br(kxp,kyp,jblok)      dimension mixup(nxhyd), sct(nxyhd)c local data      integer indx1, indx1y, nx, nxh, nxhh, nxh2, ny, nyh, ny2, nxy      integer nxhy, ks, j, k, lb, ll, jb, it, nxyh, nrx, nry, l, i, m      integer ns, ns2, km, kmr, k1, k2, j1, j2      real dnxy, arg, ani      complex s, t, t1      indx1 = indx - 1      indx1y = max0(indx1,indy)      nx = 2**indx      nxh = nx/2      nxhh = nx/4      nxh2 = nxh + 2      ny = 2**indy      nyh = ny/2      ny2 = ny + 2      nxy = max0(nx,ny)      nxhy = 2**indx1y      ks = kstrt - 2      if (isign) 50, 10, 300c prepare fft tablesc bit-reverse index table: mixup(j) = 1 + reversed bits of (j - 1)   10 do 30 j = 1, nxhy      lb = j - 1      ll = 0      do 20 k = 1, indx1y      jb = lb/2      it = lb - 2*jb      lb = jb      ll = 2*ll + it   20 continue      mixup(j) = ll + 1   30 continuec sine/cosine table for the angles 2*n*pi/nxy      nxyh = nxy/2      dnxy = 6.28318530717959/float(nxy)      do 40 j = 1, nxyh      arg = dnxy*float(j - 1)      sct(j) = cmplx(cos(arg),-sin(arg))   40 continue      returnc inverse fourier transform   50 if (kstrt.gt.ny) go to 180      nrx = nxhy/nxh      do 80 l = 1, kblokc bit-reverse array elements in x      do 70 j = 1, nxh      j1 = (mixup(j) - 1)/nrx + 1      if (j.ge.j1) go to 70      do 60 k = 1, kypc     if (j.lt.j1) then      t = f(j1,k,l)      f(j1,k,l) = f(j,k,l)      f(j,k,l) = tc     endif   60 continue   70 continue   80 continuec first transform in x      nrx = nxy/nxh      do 130 m = 1, indx1      ns = 2**(m - 1)      ns2 = ns + ns      km = nxhh/ns      kmr = km*nrx      do 120 l = 1, kblok      do 110 k = 1, km      k1 = ns2*(k - 1)      k2 = k1 + ns      do 100 j = 1, ns      j1 = j + k1      j2 = j + k2      s = sct(1+kmr*(j-1))      do 90 i = 1, kyp      t = s*f(j2,i,l)      f(j2,i,l) = f(j1,i,l) - t      f(j1,i,l) = f(j1,i,l) + t   90 continue  100 continue  110 continue  120 continue  130 continuec unscramble coefficients and normalize      kmr = nxy/nx      ani = 1./float(2*nx*ny)      do 170 l = 1, kblok      do 150 j = 2, nxhh      t1 = cmplx(aimag(sct(1+kmr*(j-1))),-real(sct(1+kmr*(j-1))))      do 140 k = 1, kyp      t = conjg(f(nxh2-j,k,l))      s = f(j,k,l) + t      t = (f(j,k,l) - t)*t1      f(j,k,l) = ani*(s + t)      f(nxh2-j,k,l) = ani*conjg(s - t)  140 continue  150 continue      do 160 k = 1, kyp      f(nxhh+1,k,l) = 2.*ani*conjg(f(nxhh+1,k,l))      f(1,k,l) = 2.*ani*cmplx(real(f(1,k,l)) + aimag(f(1,k,l)),real(f(1,     1k,l)) - aimag(f(1,k,l)))  160 continue  170 continuec transpose f array to g  180 call ptpose(f,g,bs,br,nxh,ny,kstrt,nxvh,nyv,kxp,kyp,jblok,kblok)      if (kstrt.gt.nxh) go to 290      nry = nxhy/ny      do 210 l = 1, jblokc bit-reverse array elements in y      do 200 k = 1, ny      k1 = (mixup(k) - 1)/nry + 1      if (k.ge.k1) go to 200      do 190 j = 1, kxpc     if (k.lt.k1) then      t = g(k1,j,l)      g(k1,j,l) = g(k,j,l)      g(k,j,l) = tc     endif  190 continue  200 continue  210 continuec then transform in y      nry = nxy/ny      do 260 m = 1, indy      ns = 2**(m - 1)      ns2 = ns + ns      km = nyh/ns      kmr = km*nry      do 250 l = 1, jblok      do 240 k = 1, km      k1 = ns2*(k - 1)      k2 = k1 + ns      do 230 j = 1, ns      j1 = j + k1      j2 = j + k2      s = sct(1+kmr*(j-1))      do 220 i = 1, kxp      t = s*g(j2,i,l)      g(j2,i,l) = g(j1,i,l) - t      g(j1,i,l) = g(j1,i,l) + t  220 continue  230 continue  240 continue  250 continue  260 continuec unscramble modes kx = 0, nx/2      do 280 l = 1, jblok      if ((l+ks).gt.0) go to 280      do 270 k = 2, nyhc     if ((l+ks).eq.0) then      s = g(ny2-k,1,l)      g(ny2-k,1,l) = .5*cmplx(aimag(g(k,1,l) + s),real(g(k,1,l) - s))      g(k,1,l) = .5*cmplx(real(g(k,1,l) + s),aimag(g(k,1,l) - s))c     endif  270 continue  280 continuec transpose g array to f  290 if (ntpose.eq.0) call ptpose(g,f,br,bs,ny,nxh,kstrt,nyv,nxvh,kyp,k     1xp,kblok,jblok)      returnc forward fourier transformc transpose f array to g  300 if (ntpose.eq.0) call ptpose(f,g,bs,br,nxh,ny,kstrt,nxvh,nyv,kxp,k     1yp,jblok,kblok)      if (kstrt.gt.nxh) go to 410      nry = nxhy/ny      do 350 l = 1, jblokc scramble modes kx = 0, nx/2      if ((l+ks).gt.0) go to 320      do 310 k = 2, nyhc     if ((l+ks).eq.0) then      s = cmplx(aimag(g(ny2-k,1,l)),real(g(ny2-k,1,l)))      g(ny2-k,1,l) = conjg(g(k,1,l) - s)      g(k,1,l) = g(k,1,l) + sc     endif  310 continuec bit-reverse array elements in y  320 do 340 k = 1, ny      k1 = (mixup(k) - 1)/nry + 1      if (k.ge.k1) go to 340      do 330 j = 1, kxpc     if (k.lt.k1) then      t = g(k1,j,l)      g(k1,j,l) = g(k,j,l)      g(k,j,l) = tc     endif  330 continue  340 continue  350 continuec first transform in y      nry = nxy/ny      do 400 m = 1, indy      ns = 2**(m - 1)      ns2 = ns + ns      km = nyh/ns      kmr = km*nry      do 390 l = 1, jblok      do 380 k = 1, km      k1 = ns2*(k - 1)      k2 = k1 + ns      do 370 j = 1, ns      j1 = j + k1      j2 = j + k2      s = conjg(sct(1+kmr*(j-1)))      do 360 i = 1, kxp      t = s*g(j2,i,l)      g(j2,i,l) = g(j1,i,l) - t      g(j1,i,l) = g(j1,i,l) + t  360 continue  370 continue  380 continue  390 continue  400 continuec transpose g array to f  410 call ptpose(g,f,br,bs,ny,nxh,kstrt,nyv,nxvh,kyp,kxp,kblok,jblok)      if (kstrt.gt.ny) return      nrx = nxhy/nxh      kmr = nxy/nx      do 470 l = 1, kblokc scramble coefficients      do 430 j = 2, nxhh      t1 = cmplx(aimag(sct(1+kmr*(j-1))),real(sct(1+kmr*(j-1))))      do 420 k = 1, kyp      t = conjg(f(nxh2-j,k,l))      s = f(j,k,l) + t      t = (f(j,k,l) - t)*t1      f(j,k,l) = s + t      f(nxh2-j,k,l) = conjg(s - t)  420 continue  430 continue      do 440 k = 1, kyp      f(nxhh+1,k,l) = 2.*conjg(f(nxhh+1,k,l))      f(1,k,l) = cmplx(real(f(1,k,l)) + aimag(f(1,k,l)),real(f(1,k,l)) -     1 aimag(f(1,k,l)))  440 continuec bit-reverse array elements in x      do 460 j = 1, nxh      j1 = (mixup(j) - 1)/nrx + 1      if (j.ge.j1) go to 460      do 450 k = 1, kypc     if (j.lt.j1) then      t = f(j1,k,l)      f(j1,k,l) = f(j,k,l)      f(j,k,l) = tc     endif  450 continue  460 continue  470 continuec then transform in x      nrx = nxy/nxh      do 520 m = 1, indx1      ns = 2**(m - 1)      ns2 = ns + ns      km = nxhh/ns      kmr = km*nrx      do 510 l = 1, kblok      do 500 k = 1, km      k1 = ns2*(k - 1)      k2 = k1 + ns      do 490 j = 1, ns      j1 = j + k1      j2 = j + k2      s = conjg(sct(1+kmr*(j-1)))      do 480 i = 1, kyp      t = s*f(j2,i,l)      f(j2,i,l) = f(j1,i,l) - t      f(j1,i,l) = f(j1,i,l) + t  480 continue  490 continue  500 continue  510 continue  520 continue      return      endc-----------------------------------------------------------------------      subroutine ptpose(f,g,s,t,nx,ny,kstrt,nxv,nyv,kxp,kyp,jblok,kblok)c this subroutine performs a transpose of a matrix f, distributed in y,c to a matrix g, distributed in x, that is,c g(k+kyp*(m-1),j,l) = f(j+kxp*(l-1),k,m), wherec 1 <= j <= kxp, 1 <= k <= kyp, 1 <= l <= nx/kxp, 1 <= m <= ny/kypc and where indices l and m can be distributed across processors.c this subroutine sends and receives one message at a time, eitherc synchronously or asynchronously. it uses a minimum of system resourcesc f = complex input arrayc g = complex output arrayc s, t = complex scratch arraysc nx/ny = number of points in x/yc kstrt = starting data block numberc nxv/nyv = first dimension of f/gc kxp/kyp = number of data values per block in x/yc jblok/kblok = number of data blocks in x/yc optimized version      implicit none      integer nx, ny, kstrt, nxv, nyv, kxp, kyp, jblok, kblok      complex f, g, s, t      dimension f(nxv,kyp,kblok), g(nyv,kxp,jblok)      dimension s(kxp,kyp,kblok), t(kxp,kyp,jblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c lgrp = current communicatorc mcplx = default datatype for complex      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer ks, kxb, kyb, jkblok, kxym, mtr, ntr, mntr      integer l, i, joff, koff, k, j      integer ir0, is0, ii, ir, is, ierr, msid, istatus      dimension istatus(lstat)      ks = kstrt - 2      kxb = nx/kxp      kyb = ny/kypc this segment is used for shared memory computersc     if (kstrt.gt.nx) returnc     do 40 l = 1, jblokc     joff = kxp*(l + ks)c     do 30 i = 1, kybc     koff = kyp*(i - 1)c     do 20 k = 1, kypc     do 10 j = 1, kxpc     g(k+koff,j,l) = f(j+joff,k,i)c  10 continuec  20 continuec  30 continuec  40 continuec this segment is used for mpi computers      jkblok = max0(jblok,kblok)      kxym = min0(kxb,kyb)      mtr = kyb/kxym      ntr = kxb/kxym      mntr = max0(mtr,ntr)      do 70 l = 1, jkblok      do 60 i = 1, kxym      ir0 = iand(kxym-1,ieor(l+ks,i-1)) + 1      is0 = ir0      do 50 ii = 1, mntrc post receive      if ((kstrt.le.nx).and.(ii.le.mtr)) then         ir = ir0 + kxym*(ii - 1)         call MPI_IRECV(t(1,1,l),kxp*kyp,mcplx,ir-1,ir+kxym+1,lgrp,msid,     1ierr)      endifc send data      if ((kstrt.le.ny).and.(ii.le.ntr)) then         is = is0 + kxym*(ii - 1)         joff = kxp*(is - 1)         do 20 k = 1, kyp         do 10 j = 1, kxp         s(j,k,l) = f(j+joff,k,l)   10    continue   20    continue         call MPI_SEND(s(1,1,l),kxp*kyp,mcplx,is-1,l+ks+kxym+2,lgrp,ierr     1)      endifc receive data      if ((kstrt.le.nx).and.(ii.le.mtr)) then         koff = kyp*(ir - 1)         call MPI_WAIT(msid,istatus,ierr)         do 40 k = 1, kyp         do 30 j = 1, kxp         g(k+koff,j,l) = t(j,k,l)   30    continue   40    continue      endif   50 continue   60 continue   70 continue      return      endc-----------------------------------------------------------------------      function ranorm(d)c this program calculates a random number y from a gaussian distributionc with zero mean and unit variance, according to the method ofc mueller and box:c    y(k) = (-2*ln(x(k)))**1/2*sin(2*pi*x(k+1))c    y(k+1) = (-2*ln(x(k)))**1/2*cos(2*pi*x(k+1)),c where x is a random number uniformly distributed on (0,1).c written for the ibm by viktor k. decyk, ucla      integer r1,r2,r4,r5      double precision ranorm,h1l,h1u,h2l,r0,r3,asc,bsc,temp      save iflg,r1,r2,r4,r5,h1l,h1u,h2l,r0      data r1,r2,r4,r5 /885098780,1824280461,1396483093,55318673/      data h1l,h1u,h2l /65531.0d0,32767.0d0,65525.0d0/      data iflg,r0 /0,0.0d0/      if (iflg.eq.0) go to 10      ranorm = r0      r0 = 0.0d0      iflg = 0      return   10 isc = 65536      asc = dfloat(isc)      bsc = asc*asc      i1 = r1 - (r1/isc)*isc      r3 = h1l*dfloat(r1) + asc*h1u*dfloat(i1)      i1 = r3/bsc      r3 = r3 - dfloat(i1)*bsc      bsc = 0.5d0*bsc      i1 = r2/isc      isc = r2 - i1*isc      r0 = h1l*dfloat(r2) + asc*h1u*dfloat(isc)      asc = 1.0d0/bsc      isc = r0*asc      r2 = r0 - dfloat(isc)*bsc      r3 = r3 + (dfloat(isc) + 2.0d0*h1u*dfloat(i1))      isc = r3*asc      r1 = r3 - dfloat(isc)*bsc      temp = dsqrt(-2.0d0*dlog((dfloat(r1) + dfloat(r2)*asc)*asc))      isc = 65536      asc = dfloat(isc)      bsc = asc*asc      i1 = r4 - (r4/isc)*isc      r3 = h2l*dfloat(r4) + asc*h1u*dfloat(i1)      i1 = r3/bsc      r3 = r3 - dfloat(i1)*bsc      bsc = 0.5d0*bsc      i1 = r5/isc      isc = r5 - i1*isc      r0 = h2l*dfloat(r5) + asc*h1u*dfloat(isc)      asc = 1.0d0/bsc      isc = r0*asc      r5 = r0 - dfloat(isc)*bsc      r3 = r3 + (dfloat(isc) + 2.0d0*h1u*dfloat(i1))      isc = r3*asc      r4 = r3 - dfloat(isc)*bsc      r0 = 6.28318530717959d0*((dfloat(r4) + dfloat(r5)*asc)*asc)      ranorm = temp*dsin(r0)      r0 = temp*dcos(r0)      iflg = 1      return      endc-----------------------------------------------------------------------      subroutine timera(icntrl,chr,time)c this subroutine performs timingc input: icntrl, chrc icntrl = (-1,0,1) = (initialize,ignore,read) clockc clock should be initialized before it is read!c chr = character variable for labeling timingsc time = elapsed time in secondsc written for mpi      implicit none      integer icntrl      character*8 chr      real timec get definition of MPI constants      include 'mpif.h'c common block for parallel processing      integer nproc, lgrp, mreal, mint, mcplxc lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer idproc, ierr      real nclock, mclock      double precision jclock      save jclock   91 format (1x,a8,1x,'max/min real time = ',e14.7,1x,e14.7,1x,'sec')      data jclock /0.0d0/      if (icntrl.eq.0) return      if (icntrl.eq.1) go to 10c initialize clock      call MPI_BARRIER(lgrp,ierr)      jclock = MPI_WTIME()      returnc read clock and write time difference from last clock initialization   10 nclock = real(MPI_WTIME() - jclock)      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MIN,lgrp,ierr)      mclock = time      call MPI_ALLREDUCE(nclock,time,1,mreal,MPI_MAX,lgrp,ierr)      call MPI_COMM_RANK(lgrp,idproc,ierr)      if (idproc.eq.0) write (6,91) chr, time, mclock      return      endc-----------------------------------------------------------------------      subroutine psum (f,g,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c f(j,k) = sum over k of f(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c f = input and output datac g = scratch arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      real f, g      integer nxp, nblok      dimension f(nxp,nblok), g(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mreal = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer istatus      integer idproc, ierr, kstrt, ks, l, kxs, k, kb, lb, msid, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        g(j,k) = f(j,kb+kxs)c     elsec        g(j,k) = f(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_IRECV(g,nxp,mreal,kb+kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb+kxs-1,l+nxp,lgrp,ierr)      else         call MPI_IRECV(g,nxp,mreal,kb-kxs-1,l+nxp,lgrp,msid,ierr)         call MPI_SEND(f,nxp,mreal,kb-kxs-1,l+nxp,lgrp,ierr)      endif      call MPI_WAIT(msid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      f(j,k) = f(j,k) + g(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      endc-----------------------------------------------------------------------      subroutine pisum (if,ig,nxp,nblok)c this subroutine performs a parallel sum of a vector, that is:c if(j,k) = sum over k of if(j,k)c assumes the number of processors nproc is a power of two.c the algorithm performs partial sums in binary pairs, as follows:c first, adjacent processors exchange vectors and sum them.  next,c processors separated by 2 exchange the new vectors and sum them, thenc those separated by 4, up to processors separated by nproc/2.  at thec end, all processors contain the same summation.c if = input and output integer datac ig = scratch integer arrayc nxp = number of data values in vectorc nblok = number of data blocksc written by viktor k. decyk, ucla      implicit none      integer if, ig, nxp, nblok      dimension if(nxp,nblok), ig(nxp,nblok)c common block for parallel processing      integer nproc, lgrp, lstat, mreal, mint, mcplxc lstat = length of status array      parameter(lstat=8)c nproc = number of real or virtual processors obtainedc lgrp = current communicatorc mint = default datatype for reals      common /pparms/ nproc, lgrp, mreal, mint, mcplxc local data      integer istatus      integer idproc, ierr, kstrt, ks, l, kxs, k, kb, lb, nsid, j      dimension istatus(lstat)c find processor idc this line is used for shared memory computersc     idproc = 0c this line is used for mpi computers      call MPI_COMM_RANK(lgrp,idproc,ierr)      kstrt = idproc + 1      if (kstrt.gt.nproc) return      ks = kstrt - 2      l = 1      kxs = 1c main iteration loop   10 if (kxs.ge.nproc) go to 60c shift data      do 30 k = 1, nblok      kb = k + ks      lb = kb/kxs      kb = kb + 1      lb = lb - 2*(lb/2)c this loop is used for shared memory computersc     do 20 j = 1, nxpc     if (lb.eq.0) thenc        ig(j,k) = if(j,kb+kxs)c     elsec        ig(j,k) = if(j,kb-kxs)c     endifc  20 continuec this segment is used for mpi computers      if (lb.eq.0) then         call MPI_ISEND(if,nxp,mint,kb+kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb+kxs-1,l+nxp,lgrp,istatus,ierr)      else         call MPI_ISEND(if,nxp,mint,kb-kxs-1,l+nxp,lgrp,nsid,ierr)         call MPI_RECV(ig,nxp,mint,kb-kxs-1,l+nxp,lgrp,istatus,ierr)      endif      call MPI_WAIT(nsid,istatus,ierr)   30 continuec perform sum      do 50 k = 1, nblok      do 40 j = 1, nxp      if(j,k) = if(j,k) + ig(j,k)   40 continue   50 continue      l = l + 1      kxs = kxs + kxs      go to 10   60 return      end